{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.7)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.4.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.16.37)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.4)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.37)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.14.12->sagemaker==1.72.0) (1.25.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.37)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import source\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "import sklearn.model_selection\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './sound/beep.wav'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Processing </h2>\n",
    "\n",
    "ROI is calculated using next price because we are trying to predict future ROI using the data from current time period.\n",
    "Referred to https://github.com/NGYB/Stocks/blob/master/StockPricePrediction/StockPricePrediction_v1c_xgboost.ipynb & https://towardsdatascience.com/cryptocurrency-price-prediction-using-lstms-tensorflow-for-hackers-part-iii-264fcdbccd3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_DIR, 'crypto-historical-data.csv'), \n",
    "                   parse_dates=['time'], \n",
    "                   index_col=0, \n",
    "                   keep_default_na=False,\n",
    "                   header=0,\n",
    "                   names=['market_cap', 'name', 'price', 'sym', 'time', 'volume'])\n",
    "\n",
    "data.sort_values(by=['sym', 'time'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rank'] = data.groupby(\"time\")[\"market_cap\"] \\\n",
    "                    .rank(\"dense\", ascending=False) \\\n",
    "                    .astype(int)\n",
    "\n",
    "data['market_share'] = data.groupby('time')[\"market_cap\"] \\\n",
    "                    .apply(lambda x: x/float(x.sum()))\n",
    "\n",
    "data['age'] = data.groupby(['sym'])[\"time\"] \\\n",
    "                    .apply(lambda x: x - min(x)) \\\n",
    "                    .dt.days + 1\n",
    "\n",
    "previous_price = data.groupby(['sym'])['price'].shift(-1)\n",
    "data['roi'] = data['price']/previous_price - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>sym</th>\n",
       "      <th>time</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224231</th>\n",
       "      <td>167911000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.753325</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>674188.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034732</td>\n",
       "      <td>1</td>\n",
       "      <td>0.073270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224232</th>\n",
       "      <td>42637600.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.701897</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-09</td>\n",
       "      <td>532170.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224233</th>\n",
       "      <td>43130000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.708448</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>405283.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.337899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224234</th>\n",
       "      <td>42796500.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>1463100.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.122951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224235</th>\n",
       "      <td>64018400.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>2150620.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224236</th>\n",
       "      <td>73935400.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-13</td>\n",
       "      <td>4068680.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016312</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224237</th>\n",
       "      <td>109594000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>4637030.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>7</td>\n",
       "      <td>0.082840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224238</th>\n",
       "      <td>109160000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.690000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>2554360.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.023926</td>\n",
       "      <td>8</td>\n",
       "      <td>0.076433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224239</th>\n",
       "      <td>102028000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-16</td>\n",
       "      <td>3550790.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.022676</td>\n",
       "      <td>9</td>\n",
       "      <td>0.308333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224240</th>\n",
       "      <td>95819700.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>1942830.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021582</td>\n",
       "      <td>10</td>\n",
       "      <td>0.100917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         market_cap      name     price  sym       time     volume  rank  \\\n",
       "224231  167911000.0  Ethereum  0.753325  ETH 2015-08-08   674188.0     4   \n",
       "224232   42637600.0  Ethereum  0.701897  ETH 2015-08-09   532170.0     4   \n",
       "224233   43130000.0  Ethereum  0.708448  ETH 2015-08-10   405283.0     4   \n",
       "224234   42796500.0  Ethereum  1.070000  ETH 2015-08-11  1463100.0     4   \n",
       "224235   64018400.0  Ethereum  1.220000  ETH 2015-08-12  2150620.0     4   \n",
       "224236   73935400.0  Ethereum  1.830000  ETH 2015-08-13  4068680.0     4   \n",
       "224237  109594000.0  Ethereum  1.830000  ETH 2015-08-14  4637030.0     4   \n",
       "224238  109160000.0  Ethereum  1.690000  ETH 2015-08-15  2554360.0     4   \n",
       "224239  102028000.0  Ethereum  1.570000  ETH 2015-08-16  3550790.0     4   \n",
       "224240   95819700.0  Ethereum  1.200000  ETH 2015-08-17  1942830.0     4   \n",
       "\n",
       "        market_share  age       roi  \n",
       "224231      0.034732    1  0.073270  \n",
       "224232      0.009641    2 -0.009247  \n",
       "224233      0.009581    3 -0.337899  \n",
       "224234      0.009541    4 -0.122951  \n",
       "224235      0.013904    5 -0.333333  \n",
       "224236      0.016312    6  0.000000  \n",
       "224237      0.024201    7  0.082840  \n",
       "224238      0.023926    8  0.076433  \n",
       "224239      0.022676    9  0.308333  \n",
       "224240      0.021582   10  0.100917  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['sym'] == 'ETH'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Selection </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = data.query('volume > 100000 & sym != \"\"')\n",
    "filtered = filtered.query('sym == \"ETH\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.sort_values(by=['sym', 'time'], inplace=True)\n",
    "filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Engineering </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 1\n",
    "TARGET = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = ['market_cap', 'price', 'volume', 'rank', 'market_share', 'age', 'roi']\n",
    "feat_columns = []\n",
    "\n",
    "for p in properties:\n",
    "    for w in range(1, W+1):\n",
    "        col_name = \"{}_lag_{}\".format(p, w)\n",
    "        feat[col_name] = feat.groupby(['sym'])[p].shift(w)\n",
    "        feat_columns.append(col_name)\n",
    "    \n",
    "    feat[p + '_mean'] = feat.groupby(['sym'])[p].shift(1) \\\n",
    "                                .transform(lambda x: x.rolling(w, min_periods=1).mean())\n",
    "    feat[p + '_std'] = feat.groupby(['sym'])[p].shift(1) \\\n",
    "                                .transform(lambda x: x.rolling(w, min_periods=1).std())\n",
    "\n",
    "feat.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>sym</th>\n",
       "      <th>time</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "      <th>...</th>\n",
       "      <th>age_lag_1</th>\n",
       "      <th>age_lag_2</th>\n",
       "      <th>age_lag_3</th>\n",
       "      <th>age_mean</th>\n",
       "      <th>age_std</th>\n",
       "      <th>roi_lag_1</th>\n",
       "      <th>roi_lag_2</th>\n",
       "      <th>roi_lag_3</th>\n",
       "      <th>roi_mean</th>\n",
       "      <th>roi_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42796500.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.07</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>1463100.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.122951</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.337899</td>\n",
       "      <td>-0.009247</td>\n",
       "      <td>0.073270</td>\n",
       "      <td>-0.091292</td>\n",
       "      <td>0.217517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64018400.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.22</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>2150620.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.122951</td>\n",
       "      <td>-0.337899</td>\n",
       "      <td>-0.009247</td>\n",
       "      <td>-0.156699</td>\n",
       "      <td>0.166905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73935400.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.83</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-13</td>\n",
       "      <td>4068680.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016312</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.122951</td>\n",
       "      <td>-0.337899</td>\n",
       "      <td>-0.264728</td>\n",
       "      <td>0.122804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>109594000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.83</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>4637030.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>7</td>\n",
       "      <td>0.082840</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.122951</td>\n",
       "      <td>-0.152095</td>\n",
       "      <td>0.168567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>109160000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.69</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>2554360.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.023926</td>\n",
       "      <td>8</td>\n",
       "      <td>0.076433</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.082840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.083498</td>\n",
       "      <td>0.220293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    market_cap      name  price  sym       time     volume  rank  \\\n",
       "3   42796500.0  Ethereum   1.07  ETH 2015-08-11  1463100.0     4   \n",
       "4   64018400.0  Ethereum   1.22  ETH 2015-08-12  2150620.0     4   \n",
       "5   73935400.0  Ethereum   1.83  ETH 2015-08-13  4068680.0     4   \n",
       "6  109594000.0  Ethereum   1.83  ETH 2015-08-14  4637030.0     4   \n",
       "7  109160000.0  Ethereum   1.69  ETH 2015-08-15  2554360.0     4   \n",
       "\n",
       "   market_share  age       roi  ...  age_lag_1  age_lag_2  age_lag_3  \\\n",
       "3      0.009541    4 -0.122951  ...        3.0        2.0        1.0   \n",
       "4      0.013904    5 -0.333333  ...        4.0        3.0        2.0   \n",
       "5      0.016312    6  0.000000  ...        5.0        4.0        3.0   \n",
       "6      0.024201    7  0.082840  ...        6.0        5.0        4.0   \n",
       "7      0.023926    8  0.076433  ...        7.0        6.0        5.0   \n",
       "\n",
       "   age_mean  age_std  roi_lag_1  roi_lag_2  roi_lag_3  roi_mean   roi_std  \n",
       "3       2.0      1.0  -0.337899  -0.009247   0.073270 -0.091292  0.217517  \n",
       "4       3.0      1.0  -0.122951  -0.337899  -0.009247 -0.156699  0.166905  \n",
       "5       4.0      1.0  -0.333333  -0.122951  -0.337899 -0.264728  0.122804  \n",
       "6       5.0      1.0   0.000000  -0.333333  -0.122951 -0.152095  0.168567  \n",
       "7       6.0      1.0   0.082840   0.000000  -0.333333 -0.083498  0.220293  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Split </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START = min(feat['time'])\n",
    "VAL_START = pd.Timestamp('2017-04-25')\n",
    "TEST_START = pd.Timestamp('2017-10-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feat.query(\"time < @VAL_START\")\n",
    "val = feat.query(\"time >= @VAL_START & time < @TEST_START\")\n",
    "trainval = feat.query(\"time <= @TEST_START\")\n",
    "test = feat.query(\"time >= @TEST_START\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = feat_columns + [TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_train = StandardScaler()\n",
    "train_cols_scaled = scaler_train.fit_transform(train[cols_to_scale])\n",
    "train_scaled = train.copy()\n",
    "train_scaled[cols_to_scale] = train_cols_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['market_cap_lag_1', 'market_cap_lag_2', 'market_cap_lag_3', 'price_lag_1', 'price_lag_2', 'price_lag_3', 'volume_lag_1', 'volume_lag_2', 'volume_lag_3', 'rank_lag_1', 'rank_lag_2', 'rank_lag_3', 'market_share_lag_1', 'market_share_lag_2', 'market_share_lag_3', 'age_lag_1', 'age_lag_2', 'age_lag_3', 'roi_lag_1', 'roi_lag_2', 'roi_lag_3', 'price']\n",
      "[9.44029540e+17 9.25097659e+17 9.06196744e+17 1.17463805e+02\n",
      " 1.15293085e+02 1.13101657e+02 1.90377237e+15 1.90423395e+15\n",
      " 1.90462687e+15 7.82296608e-01 7.85099799e-01 7.87882379e-01\n",
      " 1.71415743e-03 1.70741731e-03 1.69725494e-03 3.23440000e+04\n",
      " 3.23440000e+04 3.23440000e+04 5.28228005e-03 5.28118028e-03\n",
      " 5.29072712e-03 1.19814276e+02]\n",
      "[ 8.87641789e+08  8.80615461e+08  8.73852413e+08  1.04847740e+01\n",
      "  1.04080676e+01  1.03313474e+01  2.24390934e+07  2.23817871e+07\n",
      "  2.23229053e+07  2.56179775e+00  2.56500803e+00  2.56821830e+00\n",
      "  6.39788099e-02  6.37561402e-02  6.35737997e-02  3.14000000e+02\n",
      "  3.13000000e+02  3.12000000e+02 -4.12331908e-03 -4.08875318e-03\n",
      " -3.97313096e-03  1.05639418e+01]\n"
     ]
    }
   ],
   "source": [
    "print(cols_to_scale)\n",
    "print(scaler_train.var_)\n",
    "print(scaler_train.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_trainval = StandardScaler()\n",
    "trainval_cols_scaled = scaler_trainval.fit_transform(trainval[cols_to_scale])\n",
    "trainval_scaled = trainval.copy()\n",
    "trainval_scaled[cols_to_scale] = trainval_cols_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_col(df, base, col):\n",
    "    mean = df[base + '_mean']\n",
    "    std = df[base + '_std']\n",
    "    std = np.where(std == 0, 0.001, std)\n",
    "    return (df[col] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scaled = val.copy()\n",
    "for p in properties:\n",
    "    val_scaled[p] = scale_col(val_scaled, p, p)\n",
    "    for w in range(1, W+1):\n",
    "        col_name = \"{}_lag_{}\".format(p, w)\n",
    "        val_scaled[col_name] = scale_col(val_scaled, p, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = test.copy()\n",
    "for p in properties:\n",
    "    test_scaled[p] = scale_col(test_scaled, p, p)\n",
    "    \n",
    "    for w in range(1, W+1):\n",
    "        col_name = \"{}_lag_{}\".format(p, w)\n",
    "        test_scaled[col_name] = scale_col(test_scaled, p, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>price</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "      <th>market_cap_lag_1</th>\n",
       "      <th>market_cap_lag_2</th>\n",
       "      <th>market_cap_lag_3</th>\n",
       "      <th>...</th>\n",
       "      <th>age_lag_1</th>\n",
       "      <th>age_lag_2</th>\n",
       "      <th>age_lag_3</th>\n",
       "      <th>age_mean</th>\n",
       "      <th>age_std</th>\n",
       "      <th>roi_lag_1</th>\n",
       "      <th>roi_lag_2</th>\n",
       "      <th>roi_lag_3</th>\n",
       "      <th>roi_mean</th>\n",
       "      <th>roi_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>1.690000e+02</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>169.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.0</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>169.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.008263</td>\n",
       "      <td>0.138561</td>\n",
       "      <td>1.001767</td>\n",
       "      <td>-2.511383e+05</td>\n",
       "      <td>0.132466</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.038068</td>\n",
       "      <td>0.049450</td>\n",
       "      <td>-0.014020</td>\n",
       "      <td>-0.035430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.013550</td>\n",
       "      <td>0.003842</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>0.052698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.270688</td>\n",
       "      <td>4.687974</td>\n",
       "      <td>6.175089</td>\n",
       "      <td>3.264875e+06</td>\n",
       "      <td>3.185049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.760861</td>\n",
       "      <td>0.850413</td>\n",
       "      <td>0.726003</td>\n",
       "      <td>0.870460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.930222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.829504</td>\n",
       "      <td>0.811678</td>\n",
       "      <td>0.815299</td>\n",
       "      <td>0.038751</td>\n",
       "      <td>0.033079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-39.450642</td>\n",
       "      <td>-36.711017</td>\n",
       "      <td>-4.481230</td>\n",
       "      <td>-4.244337e+07</td>\n",
       "      <td>-6.144347</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-23.480780</td>\n",
       "      <td>-1.154553</td>\n",
       "      <td>-1.154611</td>\n",
       "      <td>-1.154272</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>808.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.154114</td>\n",
       "      <td>-1.154698</td>\n",
       "      <td>-1.154657</td>\n",
       "      <td>-0.141598</td>\n",
       "      <td>0.003867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.972459</td>\n",
       "      <td>-1.025454</td>\n",
       "      <td>-1.167391</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.592107</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.896394</td>\n",
       "      <td>-0.904223</td>\n",
       "      <td>-0.652847</td>\n",
       "      <td>-0.931094</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>850.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.807305</td>\n",
       "      <td>-0.819199</td>\n",
       "      <td>-0.735484</td>\n",
       "      <td>-0.022587</td>\n",
       "      <td>0.027951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.325622</td>\n",
       "      <td>0.220591</td>\n",
       "      <td>-0.301052</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.405553</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.013335</td>\n",
       "      <td>0.116236</td>\n",
       "      <td>-0.032222</td>\n",
       "      <td>-0.078923</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>892.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>-0.019004</td>\n",
       "      <td>-0.018394</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>0.045372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.365829</td>\n",
       "      <td>1.372667</td>\n",
       "      <td>1.157391</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.305166</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.882467</td>\n",
       "      <td>0.927361</td>\n",
       "      <td>0.641495</td>\n",
       "      <td>0.884976</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>934.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.774539</td>\n",
       "      <td>0.795376</td>\n",
       "      <td>0.791792</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.066495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10.454987</td>\n",
       "      <td>26.733397</td>\n",
       "      <td>52.275250</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>23.728305</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.015980</td>\n",
       "      <td>1.154245</td>\n",
       "      <td>1.154700</td>\n",
       "      <td>1.154471</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>976.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.154691</td>\n",
       "      <td>1.154700</td>\n",
       "      <td>1.154700</td>\n",
       "      <td>0.107638</td>\n",
       "      <td>0.171788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       market_cap       price      volume          rank  market_share    age  \\\n",
       "count  169.000000  169.000000  169.000000  1.690000e+02    169.000000  169.0   \n",
       "mean    -0.008263    0.138561    1.001767 -2.511383e+05      0.132466    2.0   \n",
       "std      4.270688    4.687974    6.175089  3.264875e+06      3.185049    0.0   \n",
       "min    -39.450642  -36.711017   -4.481230 -4.244337e+07     -6.144347    2.0   \n",
       "25%     -0.972459   -1.025454   -1.167391  0.000000e+00     -1.592107    2.0   \n",
       "50%      0.325622    0.220591   -0.301052  0.000000e+00     -0.405553    2.0   \n",
       "75%      1.365829    1.372667    1.157391  0.000000e+00      1.305166    2.0   \n",
       "max     10.454987   26.733397   52.275250  1.000000e+03     23.728305    2.0   \n",
       "\n",
       "              roi  market_cap_lag_1  market_cap_lag_2  market_cap_lag_3  ...  \\\n",
       "count  169.000000        169.000000        169.000000        169.000000  ...   \n",
       "mean    -0.038068          0.049450         -0.014020         -0.035430  ...   \n",
       "std      2.760861          0.850413          0.726003          0.870460  ...   \n",
       "min    -23.480780         -1.154553         -1.154611         -1.154272  ...   \n",
       "25%     -0.896394         -0.904223         -0.652847         -0.931094  ...   \n",
       "50%      0.013335          0.116236         -0.032222         -0.078923  ...   \n",
       "75%      0.882467          0.927361          0.641495          0.884976  ...   \n",
       "max      9.015980          1.154245          1.154700          1.154471  ...   \n",
       "\n",
       "       age_lag_1  age_lag_2  age_lag_3    age_mean  age_std   roi_lag_1  \\\n",
       "count      169.0      169.0      169.0  169.000000    169.0  169.000000   \n",
       "mean         1.0        0.0       -1.0  892.000000      1.0   -0.013550   \n",
       "std          0.0        0.0        0.0   48.930222      0.0    0.829504   \n",
       "min          1.0        0.0       -1.0  808.000000      1.0   -1.154114   \n",
       "25%          1.0        0.0       -1.0  850.000000      1.0   -0.807305   \n",
       "50%          1.0        0.0       -1.0  892.000000      1.0    0.052858   \n",
       "75%          1.0        0.0       -1.0  934.000000      1.0    0.774539   \n",
       "max          1.0        0.0       -1.0  976.000000      1.0    1.154691   \n",
       "\n",
       "        roi_lag_2   roi_lag_3    roi_mean     roi_std  \n",
       "count  169.000000  169.000000  169.000000  169.000000  \n",
       "mean     0.003842    0.009708   -0.000023    0.052698  \n",
       "std      0.811678    0.815299    0.038751    0.033079  \n",
       "min     -1.154698   -1.154657   -0.141598    0.003867  \n",
       "25%     -0.819199   -0.735484   -0.022587    0.027951  \n",
       "50%     -0.019004   -0.018394   -0.001695    0.045372  \n",
       "75%      0.795376    0.791792    0.016428    0.066495  \n",
       "max      1.154700    1.154700    0.107638    0.171788  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>sym</th>\n",
       "      <th>time</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "      <th>...</th>\n",
       "      <th>age_lag_1</th>\n",
       "      <th>age_lag_2</th>\n",
       "      <th>age_lag_3</th>\n",
       "      <th>age_mean</th>\n",
       "      <th>age_std</th>\n",
       "      <th>roi_lag_1</th>\n",
       "      <th>roi_lag_2</th>\n",
       "      <th>roi_lag_3</th>\n",
       "      <th>roi_mean</th>\n",
       "      <th>roi_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>0.644398</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.735762</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>-0.966470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.409184</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.209032</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>808.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.109770</td>\n",
       "      <td>-1.050356</td>\n",
       "      <td>0.940586</td>\n",
       "      <td>-0.002394</td>\n",
       "      <td>0.034038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>0.788165</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.329314</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-10-26</td>\n",
       "      <td>-1.260235</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.380912</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.323134</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>809.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.646778</td>\n",
       "      <td>0.505018</td>\n",
       "      <td>-1.151796</td>\n",
       "      <td>-0.010694</td>\n",
       "      <td>0.023834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>0.325622</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-0.186917</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-10-27</td>\n",
       "      <td>-0.753086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.452637</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.712785</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>810.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.038668</td>\n",
       "      <td>0.956228</td>\n",
       "      <td>0.082441</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.003867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0.405008</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-1.401938</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-10-28</td>\n",
       "      <td>-0.548184</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.295124</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-7.282861</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>811.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.461883</td>\n",
       "      <td>-1.147456</td>\n",
       "      <td>0.685573</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.004208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>-1.153368</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>14.099187</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-10-29</td>\n",
       "      <td>43.840972</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101809</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.040576</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.132097</td>\n",
       "      <td>0.762941</td>\n",
       "      <td>0.369157</td>\n",
       "      <td>-0.009341</td>\n",
       "      <td>0.017198</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     market_cap      name      price  sym       time     volume  rank  \\\n",
       "809    0.644398  Ethereum   0.735762  ETH 2017-10-25  -0.966470   0.0   \n",
       "810    0.788165  Ethereum   0.329314  ETH 2017-10-26  -1.260235   0.0   \n",
       "811    0.325622  Ethereum  -0.186917  ETH 2017-10-27  -0.753086   0.0   \n",
       "812    0.405008  Ethereum  -1.401938  ETH 2017-10-28  -0.548184   0.0   \n",
       "813   -1.153368  Ethereum  14.099187  ETH 2017-10-29  43.840972   0.0   \n",
       "\n",
       "     market_share  age       roi  ...  age_lag_1  age_lag_2  age_lag_3  \\\n",
       "809      5.409184  2.0  0.209032  ...        1.0        0.0       -1.0   \n",
       "810      0.380912  2.0  0.323134  ...        1.0        0.0       -1.0   \n",
       "811     -0.452637  2.0  0.712785  ...        1.0        0.0       -1.0   \n",
       "812     -0.295124  2.0 -7.282861  ...        1.0        0.0       -1.0   \n",
       "813      0.101809  2.0  0.040576  ...        1.0        0.0       -1.0   \n",
       "\n",
       "     age_mean  age_std  roi_lag_1  roi_lag_2  roi_lag_3  roi_mean   roi_std  \n",
       "809     808.0      1.0   0.109770  -1.050356   0.940586 -0.002394  0.034038  \n",
       "810     809.0      1.0   0.646778   0.505018  -1.151796 -0.010694  0.023834  \n",
       "811     810.0      1.0  -1.038668   0.956228   0.082441  0.001024  0.003867  \n",
       "812     811.0      1.0   0.461883  -1.147456   0.685573  0.001836  0.004208  \n",
       "813     812.0      1.0  -1.132097   0.762941   0.369157 -0.009341  0.017198  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Formatting </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_to_json_obj(sym_df, cat):\n",
    "    json_obj = {\"start\": str(sym_df['time'][0]), \n",
    "                \"target\": list(sym_df['target']), \n",
    "                \"cat\":[cat], \n",
    "                \"dynamic_feat\":[list(sym_df[column]) for column in sym_df.columns]}\n",
    "    return json_obj\n",
    "\n",
    "def write_json_dataset(features_df, filename, target, feat_columns): \n",
    "    mapping = {}\n",
    "    symbols = features_df['sym'].unique()\n",
    "    with open(filename, 'wb') as f:\n",
    "        \n",
    "        for idx, sym in enumerate(symbols):\n",
    "            sym_df = features_df[features_df['sym'] == sym]\n",
    "            sym_df = sym_df.drop(columns='sym')\n",
    "            \n",
    "            json_obj = {\"start\": str(sym_df['time'].iloc[0]), \n",
    "                        \"target\": list(sym_df[target]), \n",
    "                        \"cat\":[idx], \n",
    "                        \"dynamic_feat\":[list(sym_df[column]) for column in feat_columns]}\n",
    "            \n",
    "            json_line = json.dumps(json_obj) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "                                   \n",
    "            f.write(json_line)\n",
    "            \n",
    "            mapping[sym] = idx\n",
    "    print('JSON file created at ' + filename)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at ./data/train.json\n"
     ]
    }
   ],
   "source": [
    "mapping = write_json_dataset(trainval_scaled, './data/trainval.json', TARGET, feat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'deepar-new'\n",
    "\n",
    "train_location = session.upload_data(os.path.join(DATA_DIR, 'trainval.json'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'D'\n",
    "context_length = 50\n",
    "prediction_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-01 15:36:15 Starting - Starting the training job...\n",
      "2021-02-01 15:36:17 Starting - Launching requested ML instances......\n",
      "2021-02-01 15:37:20 Starting - Preparing the instances for training......\n",
      "2021-02-01 15:38:23 Downloading - Downloading input data...\n",
      "2021-02-01 15:38:58 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.01', u'num_cells': u'100', u'prediction_length': u'1', u'epochs': u'500', u'time_freq': u'D', u'context_length': u'50', u'num_layers': u'4', u'mini_batch_size': u'128', u'early_stopping_patience': u'100'}\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.01', u'num_layers': u'4', u'epochs': u'500', u'embedding_dimension': u'10', u'num_cells': u'100', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'100'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Using early stopping with patience 100\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] [cardinality=auto] Inferred value of cardinality=[1] from dataset.\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=21 from dataset.\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Training set statistics:\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Real time series\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] number of time series: 1\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] number of observations: 623\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] mean target length: 623\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] min/mean/max target: -0.925373613834/-2.44924383026e-08/3.88874220848\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] mean abs(target): 0.594646031363\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] contains missing values: no\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Small number of time series. Doing 1280 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] nvidia-smi took: 0.0251350402832 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:20 INFO 140079012972352] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 265.7320499420166, \"sum\": 265.7320499420166, \"min\": 265.7320499420166}}, \"EndTime\": 1612193961.240195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193960.973477}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:21 INFO 140079012972352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 597.2709655761719, \"sum\": 597.2709655761719, \"min\": 597.2709655761719}}, \"EndTime\": 1612193961.570901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193961.240289}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:24 INFO 140079012972352] Epoch[0] Batch[0] avg_epoch_loss=0.629186\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=0.629186034203\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:28 INFO 140079012972352] Epoch[0] Batch[5] avg_epoch_loss=0.850769\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=0.850769014563\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:28 INFO 140079012972352] Epoch[0] Batch [5]#011Speed: 168.25 samples/sec#011loss=0.850769\u001b[0m\n",
      "\n",
      "2021-02-01 15:39:17 Training - Training image download completed. Training in progress.\u001b[34m[02/01/2021 15:39:31 INFO 140079012972352] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 500, \"sum\": 500.0, \"min\": 500}, \"update.time\": {\"count\": 1, \"max\": 9754.220962524414, \"sum\": 9754.220962524414, \"min\": 9754.220962524414}}, \"EndTime\": 1612193971.325385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193961.571003}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.017851337 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.02672309214\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:31 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:31 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_62ffcdc6-4087-4368-ab50-628639c4cf7f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 41.925907135009766, \"sum\": 41.925907135009766, \"min\": 41.925907135009766}}, \"EndTime\": 1612193971.368529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193971.325494}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:34 INFO 140079012972352] Epoch[1] Batch[0] avg_epoch_loss=0.156080\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=0.156080394983\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:37 INFO 140079012972352] Epoch[1] Batch[5] avg_epoch_loss=0.046251\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=0.0462507718864\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:37 INFO 140079012972352] Epoch[1] Batch [5]#011Speed: 195.62 samples/sec#011loss=0.046251\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:40 INFO 140079012972352] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8924.077033996582, \"sum\": 8924.077033996582, \"min\": 8924.077033996582}}, \"EndTime\": 1612193980.292759, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193971.368597}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.163274906 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=1, train loss <loss>=-0.150246856268\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:40 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:40 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_84c82c56-8fa5-4dd6-a7e1-c42e159a013f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 40.50016403198242, \"sum\": 40.50016403198242, \"min\": 40.50016403198242}}, \"EndTime\": 1612193980.334008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193980.292843}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:43 INFO 140079012972352] Epoch[2] Batch[0] avg_epoch_loss=-0.909207\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=-0.909207284451\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:46 INFO 140079012972352] Epoch[2] Batch[5] avg_epoch_loss=-0.554394\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=-0.554394292335\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:46 INFO 140079012972352] Epoch[2] Batch [5]#011Speed: 188.42 samples/sec#011loss=-0.554394\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:49 INFO 140079012972352] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9048.176050186157, \"sum\": 9048.176050186157, \"min\": 9048.176050186157}}, \"EndTime\": 1612193989.382395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193980.334122}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:49 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.462820626 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:49 INFO 140079012972352] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=2, train loss <loss>=-0.596341024339\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:49 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:49 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_852f317b-c310-4def-ba66-3537cd53693a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.32193183898926, \"sum\": 55.32193183898926, \"min\": 55.32193183898926}}, \"EndTime\": 1612193989.438431, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193989.382488}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:52 INFO 140079012972352] Epoch[3] Batch[0] avg_epoch_loss=-0.752704\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=-0.752703785896\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:55 INFO 140079012972352] Epoch[3] Batch[5] avg_epoch_loss=-1.011565\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=-1.01156469186\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:55 INFO 140079012972352] Epoch[3] Batch [5]#011Speed: 195.27 samples/sec#011loss=-1.011565\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] Epoch[3] Batch[10] avg_epoch_loss=-1.100034\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=-1.20619663\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] Epoch[3] Batch [10]#011Speed: 192.65 samples/sec#011loss=-1.206197\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9637.743949890137, \"sum\": 9637.743949890137, \"min\": 9637.743949890137}}, \"EndTime\": 1612193999.076306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193989.438494}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.544354149 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=3, train loss <loss>=-1.10003375465\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:39:59 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_7e4e915f-9362-4bf6-baa1-52f6995e4b03-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 35.78996658325195, \"sum\": 35.78996658325195, \"min\": 35.78996658325195}}, \"EndTime\": 1612193999.112891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193999.076412}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:02 INFO 140079012972352] Epoch[4] Batch[0] avg_epoch_loss=0.638079\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=0.638078808784\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:06 INFO 140079012972352] Epoch[4] Batch[5] avg_epoch_loss=-0.216550\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=-0.216549913088\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:06 INFO 140079012972352] Epoch[4] Batch [5]#011Speed: 155.59 samples/sec#011loss=-0.216550\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:08 INFO 140079012972352] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9860.55612564087, \"sum\": 9860.55612564087, \"min\": 9860.55612564087}}, \"EndTime\": 1612194008.97358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612193999.112957}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:08 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=125.953972636 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:08 INFO 140079012972352] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=4, train loss <loss>=-0.387887215614\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:08 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:12 INFO 140079012972352] Epoch[5] Batch[0] avg_epoch_loss=-0.625774\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=-0.62577444315\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:15 INFO 140079012972352] Epoch[5] Batch[5] avg_epoch_loss=-0.857465\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=-0.857464879751\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:15 INFO 140079012972352] Epoch[5] Batch [5]#011Speed: 194.97 samples/sec#011loss=-0.857465\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] Epoch[5] Batch[10] avg_epoch_loss=-1.069723\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=-1.32443351746\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] Epoch[5] Batch [10]#011Speed: 192.87 samples/sec#011loss=-1.324434\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9702.544927597046, \"sum\": 9702.544927597046, \"min\": 9702.544927597046}}, \"EndTime\": 1612194018.676924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194008.973726}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.49889397 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=5, train loss <loss>=-1.06972335144\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:18 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:21 INFO 140079012972352] Epoch[6] Batch[0] avg_epoch_loss=-1.500706\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=-1.50070631504\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:24 INFO 140079012972352] Epoch[6] Batch[5] avg_epoch_loss=-0.932613\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=-0.932613306989\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:24 INFO 140079012972352] Epoch[6] Batch [5]#011Speed: 196.17 samples/sec#011loss=-0.932613\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] Epoch[6] Batch[10] avg_epoch_loss=-1.048254\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=-1.18702213764\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] Epoch[6] Batch [10]#011Speed: 195.24 samples/sec#011loss=-1.187022\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9545.691967010498, \"sum\": 9545.691967010498, \"min\": 9545.691967010498}}, \"EndTime\": 1612194028.22324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194018.677013}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.298835278 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=6, train loss <loss>=-1.04825368456\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:28 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:31 INFO 140079012972352] Epoch[7] Batch[0] avg_epoch_loss=-1.098688\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=-1.09868812561\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:34 INFO 140079012972352] Epoch[7] Batch[5] avg_epoch_loss=-1.216696\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=-1.21669555704\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:34 INFO 140079012972352] Epoch[7] Batch [5]#011Speed: 196.86 samples/sec#011loss=-1.216696\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] Epoch[7] Batch[10] avg_epoch_loss=-1.338225\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=-1.48406002522\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] Epoch[7] Batch [10]#011Speed: 193.91 samples/sec#011loss=-1.484060\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] processed a total of 1324 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9568.274021148682, \"sum\": 9568.274021148682, \"min\": 9568.274021148682}}, \"EndTime\": 1612194037.792524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194028.223376}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.371867162 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=7, train loss <loss>=-1.33822486075\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:37 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_8fb79404-4e71-4eff-96b6-8024a5654b13-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.49494934082031, \"sum\": 33.49494934082031, \"min\": 33.49494934082031}}, \"EndTime\": 1612194037.826741, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194037.792623}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:40 INFO 140079012972352] Epoch[8] Batch[0] avg_epoch_loss=-1.096413\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=-1.09641289711\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:44 INFO 140079012972352] Epoch[8] Batch[5] avg_epoch_loss=-1.280700\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=-1.28070034583\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:44 INFO 140079012972352] Epoch[8] Batch [5]#011Speed: 196.07 samples/sec#011loss=-1.280700\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:46 INFO 140079012972352] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8875.157833099365, \"sum\": 8875.157833099365, \"min\": 8875.157833099365}}, \"EndTime\": 1612194046.702034, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194037.826806}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:46 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.502184524 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:46 INFO 140079012972352] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=8, train loss <loss>=-1.41401773691\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:46 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:46 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_8ca0be1d-7a51-4ff0-9789-3f01178382c6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 35.524845123291016, \"sum\": 35.524845123291016, \"min\": 35.524845123291016}}, \"EndTime\": 1612194046.738376, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194046.702135}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:49 INFO 140079012972352] Epoch[9] Batch[0] avg_epoch_loss=-1.502178\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=-1.50217783451\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:53 INFO 140079012972352] Epoch[9] Batch[5] avg_epoch_loss=-1.775668\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=-1.77566766739\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:53 INFO 140079012972352] Epoch[9] Batch [5]#011Speed: 194.04 samples/sec#011loss=-1.775668\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:55 INFO 140079012972352] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8943.998098373413, \"sum\": 8943.998098373413, \"min\": 8943.998098373413}}, \"EndTime\": 1612194055.682515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194046.738447}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:55 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.091550014 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:55 INFO 140079012972352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=9, train loss <loss>=-1.54095136523\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:55 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:55 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_9e8c4873-b0aa-4203-a013-03a24fc61253-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.25607490539551, \"sum\": 36.25607490539551, \"min\": 36.25607490539551}}, \"EndTime\": 1612194055.719585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194055.682621}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:58 INFO 140079012972352] Epoch[10] Batch[0] avg_epoch_loss=-1.552225\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:40:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=-1.55222451687\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:02 INFO 140079012972352] Epoch[10] Batch[5] avg_epoch_loss=-1.399258\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=-1.39925799767\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:02 INFO 140079012972352] Epoch[10] Batch [5]#011Speed: 188.51 samples/sec#011loss=-1.399258\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] Epoch[10] Batch[10] avg_epoch_loss=-1.517051\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=-1.65840167999\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] Epoch[10] Batch [10]#011Speed: 153.34 samples/sec#011loss=-1.658402\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10598.804950714111, \"sum\": 10598.804950714111, \"min\": 10598.804950714111}}, \"EndTime\": 1612194066.318518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194055.719648}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=120.955482083 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=10, train loss <loss>=-1.51705058054\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:06 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:09 INFO 140079012972352] Epoch[11] Batch[0] avg_epoch_loss=-1.535073\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=-1.53507304192\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:12 INFO 140079012972352] Epoch[11] Batch[5] avg_epoch_loss=-1.686365\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=-1.68636518717\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:12 INFO 140079012972352] Epoch[11] Batch [5]#011Speed: 195.88 samples/sec#011loss=-1.686365\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] Epoch[11] Batch[10] avg_epoch_loss=-1.662001\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=-1.63276375532\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] Epoch[11] Batch [10]#011Speed: 196.31 samples/sec#011loss=-1.632764\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9615.575075149536, \"sum\": 9615.575075149536, \"min\": 9615.575075149536}}, \"EndTime\": 1612194075.934729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194066.318613}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.634450335 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=11, train loss <loss>=-1.66200089997\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:15 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_344ad66f-2860-408c-b48a-fb2d6b7515b7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.83000183105469, \"sum\": 55.83000183105469, \"min\": 55.83000183105469}}, \"EndTime\": 1612194075.991529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194075.934876}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:19 INFO 140079012972352] Epoch[12] Batch[0] avg_epoch_loss=-0.259434\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=-0.259434103966\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:22 INFO 140079012972352] Epoch[12] Batch[5] avg_epoch_loss=-0.663865\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=-0.663865089417\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:22 INFO 140079012972352] Epoch[12] Batch [5]#011Speed: 196.46 samples/sec#011loss=-0.663865\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] Epoch[12] Batch[10] avg_epoch_loss=-0.802434\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=-0.968716478348\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] Epoch[12] Batch [10]#011Speed: 193.88 samples/sec#011loss=-0.968716\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9648.202180862427, \"sum\": 9648.202180862427, \"min\": 9648.202180862427}}, \"EndTime\": 1612194085.639891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194075.9916}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.91400652 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=12, train loss <loss>=-0.802433902567\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:25 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:28 INFO 140079012972352] Epoch[13] Batch[0] avg_epoch_loss=-0.906664\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=-0.906664013863\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:31 INFO 140079012972352] Epoch[13] Batch[5] avg_epoch_loss=-1.282106\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=-1.28210550547\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:31 INFO 140079012972352] Epoch[13] Batch [5]#011Speed: 193.50 samples/sec#011loss=-1.282106\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] Epoch[13] Batch[10] avg_epoch_loss=-1.446929\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=-1.64471790791\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] Epoch[13] Batch [10]#011Speed: 195.16 samples/sec#011loss=-1.644718\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9630.271911621094, \"sum\": 9630.271911621094, \"min\": 9630.271911621094}}, \"EndTime\": 1612194095.271218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194085.640016}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.885430611 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=13, train loss <loss>=-1.44692932476\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:35 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:38 INFO 140079012972352] Epoch[14] Batch[0] avg_epoch_loss=0.419212\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.419212281704\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:41 INFO 140079012972352] Epoch[14] Batch[5] avg_epoch_loss=-0.732504\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=-0.732504134377\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:41 INFO 140079012972352] Epoch[14] Batch [5]#011Speed: 196.75 samples/sec#011loss=-0.732504\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:44 INFO 140079012972352] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8917.014122009277, \"sum\": 8917.014122009277, \"min\": 8917.014122009277}}, \"EndTime\": 1612194104.188923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194095.271298}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:44 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.646427272 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:44 INFO 140079012972352] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=14, train loss <loss>=-0.93944028914\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:44 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:47 INFO 140079012972352] Epoch[15] Batch[0] avg_epoch_loss=-1.275571\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=-1.27557110786\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:50 INFO 140079012972352] Epoch[15] Batch[5] avg_epoch_loss=-1.523918\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=-1.52391821146\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:50 INFO 140079012972352] Epoch[15] Batch [5]#011Speed: 197.43 samples/sec#011loss=-1.523918\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] Epoch[15] Batch[10] avg_epoch_loss=-1.639300\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=-1.77775738239\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] Epoch[15] Batch [10]#011Speed: 194.59 samples/sec#011loss=-1.777757\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9624.242067337036, \"sum\": 9624.242067337036, \"min\": 9624.242067337036}}, \"EndTime\": 1612194113.813869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194104.189018}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.281580688 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=15, train loss <loss>=-1.63929965279\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:53 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:56 INFO 140079012972352] Epoch[16] Batch[0] avg_epoch_loss=-1.008080\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:41:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=-1.00808000565\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:00 INFO 140079012972352] Epoch[16] Batch[5] avg_epoch_loss=-0.915975\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=-0.915975327293\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:00 INFO 140079012972352] Epoch[16] Batch [5]#011Speed: 197.45 samples/sec#011loss=-0.915975\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] Epoch[16] Batch[10] avg_epoch_loss=-1.100493\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=-1.32191333771\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] Epoch[16] Batch [10]#011Speed: 174.47 samples/sec#011loss=-1.321913\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10044.660091400146, \"sum\": 10044.660091400146, \"min\": 10044.660091400146}}, \"EndTime\": 1612194123.859148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194113.813955}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.207526889 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=16, train loss <loss>=-1.10049260475\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:03 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:07 INFO 140079012972352] Epoch[17] Batch[0] avg_epoch_loss=-1.373711\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=-1.37371098995\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:10 INFO 140079012972352] Epoch[17] Batch[5] avg_epoch_loss=-1.517089\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=-1.51708904902\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:10 INFO 140079012972352] Epoch[17] Batch [5]#011Speed: 197.67 samples/sec#011loss=-1.517089\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:13 INFO 140079012972352] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9151.448011398315, \"sum\": 9151.448011398315, \"min\": 9151.448011398315}}, \"EndTime\": 1612194133.011307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194123.859255}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.211055928 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=17, train loss <loss>=-1.63007198572\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:15 INFO 140079012972352] Epoch[18] Batch[0] avg_epoch_loss=-2.122818\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=-2.12281847\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:19 INFO 140079012972352] Epoch[18] Batch[5] avg_epoch_loss=-2.089917\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=-2.08991718292\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:19 INFO 140079012972352] Epoch[18] Batch [5]#011Speed: 194.40 samples/sec#011loss=-2.089917\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] Epoch[18] Batch[10] avg_epoch_loss=-1.998141\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=-1.88800888062\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] Epoch[18] Batch [10]#011Speed: 191.04 samples/sec#011loss=-1.888009\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9616.518020629883, \"sum\": 9616.518020629883, \"min\": 9616.518020629883}}, \"EndTime\": 1612194142.628483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194133.011386}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.350334692 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=18, train loss <loss>=-1.99814068187\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:22 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_21e65db0-a91f-47a3-b913-8e2a8eee275b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 50.6138801574707, \"sum\": 50.6138801574707, \"min\": 50.6138801574707}}, \"EndTime\": 1612194142.679805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194142.62857}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:25 INFO 140079012972352] Epoch[19] Batch[0] avg_epoch_loss=-0.235773\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=-0.235773295164\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:28 INFO 140079012972352] Epoch[19] Batch[5] avg_epoch_loss=-0.757042\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=-0.757042343418\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:28 INFO 140079012972352] Epoch[19] Batch [5]#011Speed: 195.33 samples/sec#011loss=-0.757042\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:31 INFO 140079012972352] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8892.441987991333, \"sum\": 8892.441987991333, \"min\": 8892.441987991333}}, \"EndTime\": 1612194151.572391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194142.679877}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.040858819 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=19, train loss <loss>=-1.06832982004\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:31 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:34 INFO 140079012972352] Epoch[20] Batch[0] avg_epoch_loss=-1.363244\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=-1.36324381828\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:37 INFO 140079012972352] Epoch[20] Batch[5] avg_epoch_loss=-1.576239\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=-1.57623926799\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:37 INFO 140079012972352] Epoch[20] Batch [5]#011Speed: 195.46 samples/sec#011loss=-1.576239\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:40 INFO 140079012972352] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8849.694013595581, \"sum\": 8849.694013595581, \"min\": 8849.694013595581}}, \"EndTime\": 1612194160.422825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194151.572466}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.584481811 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=20, train loss <loss>=-1.64835067987\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:40 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:43 INFO 140079012972352] Epoch[21] Batch[0] avg_epoch_loss=-2.056222\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=-2.05622220039\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:46 INFO 140079012972352] Epoch[21] Batch[5] avg_epoch_loss=-2.051450\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=-2.05144951741\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:46 INFO 140079012972352] Epoch[21] Batch [5]#011Speed: 195.73 samples/sec#011loss=-2.051450\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] Epoch[21] Batch[10] avg_epoch_loss=-2.097760\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=-2.15333185196\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] Epoch[21] Batch [10]#011Speed: 194.11 samples/sec#011loss=-2.153332\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9661.874055862427, \"sum\": 9661.874055862427, \"min\": 9661.874055862427}}, \"EndTime\": 1612194170.085413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194160.422922}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.891577693 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=21, train loss <loss>=-2.09775966948\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:50 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_44bf6a36-32e2-47ec-b260-d5ab5cfa3793-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 40.524959564208984, \"sum\": 40.524959564208984, \"min\": 40.524959564208984}}, \"EndTime\": 1612194170.126589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194170.08551}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:53 INFO 140079012972352] Epoch[22] Batch[0] avg_epoch_loss=-1.203707\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=-1.20370662212\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:56 INFO 140079012972352] Epoch[22] Batch[5] avg_epoch_loss=-1.334010\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=-1.33401007454\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:56 INFO 140079012972352] Epoch[22] Batch [5]#011Speed: 198.39 samples/sec#011loss=-1.334010\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:59 INFO 140079012972352] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8917.460918426514, \"sum\": 8917.460918426514, \"min\": 8917.460918426514}}, \"EndTime\": 1612194179.044246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194170.126711}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:59 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.639017156 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:59 INFO 140079012972352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=22, train loss <loss>=-1.57827023864\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:42:59 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:02 INFO 140079012972352] Epoch[23] Batch[0] avg_epoch_loss=-2.028575\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=-2.02857542038\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:06 INFO 140079012972352] Epoch[23] Batch[5] avg_epoch_loss=-2.003024\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=-2.00302396218\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:06 INFO 140079012972352] Epoch[23] Batch [5]#011Speed: 161.41 samples/sec#011loss=-2.003024\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:08 INFO 140079012972352] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9830.554008483887, \"sum\": 9830.554008483887, \"min\": 9830.554008483887}}, \"EndTime\": 1612194188.875526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194179.044354}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:08 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.2044777 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:08 INFO 140079012972352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=23, train loss <loss>=-2.05288639069\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:08 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:11 INFO 140079012972352] Epoch[24] Batch[0] avg_epoch_loss=-2.009101\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=-2.00910139084\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:15 INFO 140079012972352] Epoch[24] Batch[5] avg_epoch_loss=-2.167449\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=-2.1674489975\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:15 INFO 140079012972352] Epoch[24] Batch [5]#011Speed: 195.06 samples/sec#011loss=-2.167449\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:17 INFO 140079012972352] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8949.250936508179, \"sum\": 8949.250936508179, \"min\": 8949.250936508179}}, \"EndTime\": 1612194197.825443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194188.875619}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:17 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.786314163 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:17 INFO 140079012972352] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=24, train loss <loss>=-2.23119783401\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:17 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:17 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_9eb7747d-e7fe-4fac-bd08-e9e7fb3b19f3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.17382049560547, \"sum\": 36.17382049560547, \"min\": 36.17382049560547}}, \"EndTime\": 1612194197.862404, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194197.825524}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:20 INFO 140079012972352] Epoch[25] Batch[0] avg_epoch_loss=-2.691317\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=-2.69131731987\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:24 INFO 140079012972352] Epoch[25] Batch[5] avg_epoch_loss=-2.345252\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=-2.34525241454\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:24 INFO 140079012972352] Epoch[25] Batch [5]#011Speed: 195.57 samples/sec#011loss=-2.345252\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] Epoch[25] Batch[10] avg_epoch_loss=-2.133177\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=-1.87868549824\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] Epoch[25] Batch [10]#011Speed: 197.61 samples/sec#011loss=-1.878685\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9552.973985671997, \"sum\": 9552.973985671997, \"min\": 9552.973985671997}}, \"EndTime\": 1612194207.415531, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194197.86247}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.092218521 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=25, train loss <loss>=-2.1331765435\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:30 INFO 140079012972352] Epoch[26] Batch[0] avg_epoch_loss=-0.171720\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=-0.171719685197\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:33 INFO 140079012972352] Epoch[26] Batch[5] avg_epoch_loss=-0.607950\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=-0.607950096329\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:33 INFO 140079012972352] Epoch[26] Batch [5]#011Speed: 198.48 samples/sec#011loss=-0.607950\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:36 INFO 140079012972352] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8916.3339138031, \"sum\": 8916.3339138031, \"min\": 8916.3339138031}}, \"EndTime\": 1612194216.332833, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194207.41562}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:36 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.292869301 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:36 INFO 140079012972352] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=26, train loss <loss>=-0.836685243249\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:36 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:39 INFO 140079012972352] Epoch[27] Batch[0] avg_epoch_loss=-1.092069\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=-1.09206926823\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:42 INFO 140079012972352] Epoch[27] Batch[5] avg_epoch_loss=-1.234971\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=-1.23497112592\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:42 INFO 140079012972352] Epoch[27] Batch [5]#011Speed: 197.92 samples/sec#011loss=-1.234971\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] Epoch[27] Batch[10] avg_epoch_loss=-1.353417\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=-1.49555132389\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] Epoch[27] Batch [10]#011Speed: 197.44 samples/sec#011loss=-1.495551\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9556.379079818726, \"sum\": 9556.379079818726, \"min\": 9556.379079818726}}, \"EndTime\": 1612194225.890158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194216.332921}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.300368693 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=27, train loss <loss>=-1.35341667045\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:49 INFO 140079012972352] Epoch[28] Batch[0] avg_epoch_loss=-1.646560\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=-1.64656043053\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:52 INFO 140079012972352] Epoch[28] Batch[5] avg_epoch_loss=-1.799982\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=-1.79998240868\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:52 INFO 140079012972352] Epoch[28] Batch [5]#011Speed: 196.80 samples/sec#011loss=-1.799982\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] Epoch[28] Batch[10] avg_epoch_loss=-1.972880\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=-2.18035812378\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] Epoch[28] Batch [10]#011Speed: 196.34 samples/sec#011loss=-2.180358\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9643.61310005188, \"sum\": 9643.61310005188, \"min\": 9643.61310005188}}, \"EndTime\": 1612194235.534412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194225.89025}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.431367285 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=28, train loss <loss>=-1.972880461\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:55 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:58 INFO 140079012972352] Epoch[29] Batch[0] avg_epoch_loss=-2.221554\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:43:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=-2.22155380249\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:01 INFO 140079012972352] Epoch[29] Batch[5] avg_epoch_loss=-2.232038\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=-2.23203796148\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:01 INFO 140079012972352] Epoch[29] Batch [5]#011Speed: 189.13 samples/sec#011loss=-2.232038\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:05 INFO 140079012972352] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9494.880199432373, \"sum\": 9494.880199432373, \"min\": 9494.880199432373}}, \"EndTime\": 1612194245.030288, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194235.5345}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:05 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.384237693 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:05 INFO 140079012972352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=29, train loss <loss>=-2.2170466423\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:05 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:08 INFO 140079012972352] Epoch[30] Batch[0] avg_epoch_loss=-2.527379\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=-2.52737903595\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:11 INFO 140079012972352] Epoch[30] Batch[5] avg_epoch_loss=-2.209480\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=-2.20948004723\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:11 INFO 140079012972352] Epoch[30] Batch [5]#011Speed: 198.62 samples/sec#011loss=-2.209480\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] Epoch[30] Batch[10] avg_epoch_loss=-2.175659\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=-2.13507325649\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] Epoch[30] Batch [10]#011Speed: 197.59 samples/sec#011loss=-2.135073\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9754.873991012573, \"sum\": 9754.873991012573, \"min\": 9754.873991012573}}, \"EndTime\": 1612194254.786113, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194245.030423}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.673994675 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=30, train loss <loss>=-2.17565877871\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:14 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:17 INFO 140079012972352] Epoch[31] Batch[0] avg_epoch_loss=-2.171642\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=-2.17164158821\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:21 INFO 140079012972352] Epoch[31] Batch[5] avg_epoch_loss=-2.184473\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=-2.18447319667\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:21 INFO 140079012972352] Epoch[31] Batch [5]#011Speed: 196.11 samples/sec#011loss=-2.184473\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:23 INFO 140079012972352] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8873.536825180054, \"sum\": 8873.536825180054, \"min\": 8873.536825180054}}, \"EndTime\": 1612194263.660625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194254.786237}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:23 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.724919802 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:23 INFO 140079012972352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=31, train loss <loss>=-2.12550683022\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:23 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:26 INFO 140079012972352] Epoch[32] Batch[0] avg_epoch_loss=-1.719341\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=-1.71934068203\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:29 INFO 140079012972352] Epoch[32] Batch[5] avg_epoch_loss=-1.983284\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=-1.98328445355\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:29 INFO 140079012972352] Epoch[32] Batch [5]#011Speed: 193.90 samples/sec#011loss=-1.983284\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] Epoch[32] Batch[10] avg_epoch_loss=-2.064531\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=-2.1620257616\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] Epoch[32] Batch [10]#011Speed: 195.92 samples/sec#011loss=-2.162026\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9533.07294845581, \"sum\": 9533.07294845581, \"min\": 9533.07294845581}}, \"EndTime\": 1612194273.194462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194263.660719}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.260731831 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=32, train loss <loss>=-2.06453050267\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:33 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:36 INFO 140079012972352] Epoch[33] Batch[0] avg_epoch_loss=-2.234092\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=-2.23409223557\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:39 INFO 140079012972352] Epoch[33] Batch[5] avg_epoch_loss=-2.146866\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=-2.1468659242\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:39 INFO 140079012972352] Epoch[33] Batch [5]#011Speed: 195.32 samples/sec#011loss=-2.146866\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:42 INFO 140079012972352] processed a total of 1225 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8858.705043792725, \"sum\": 8858.705043792725, \"min\": 8858.705043792725}}, \"EndTime\": 1612194282.053831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194273.194533}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:42 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.279989059 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:42 INFO 140079012972352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=33, train loss <loss>=-2.22851274014\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:42 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:45 INFO 140079012972352] Epoch[34] Batch[0] avg_epoch_loss=-2.150913\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=-2.15091252327\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:48 INFO 140079012972352] Epoch[34] Batch[5] avg_epoch_loss=-2.367730\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=-2.36772974332\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:48 INFO 140079012972352] Epoch[34] Batch [5]#011Speed: 193.86 samples/sec#011loss=-2.367730\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] Epoch[34] Batch[10] avg_epoch_loss=-2.258518\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=-2.1274630785\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] Epoch[34] Batch [10]#011Speed: 196.93 samples/sec#011loss=-2.127463\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9620.004892349243, \"sum\": 9620.004892349243, \"min\": 9620.004892349243}}, \"EndTime\": 1612194291.674669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194282.053924}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.043700638 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=34, train loss <loss>=-2.25851762295\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:51 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_5ee162eb-65ff-4d8d-82f0-395c92d09a88-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.0640754699707, \"sum\": 37.0640754699707, \"min\": 37.0640754699707}}, \"EndTime\": 1612194291.712773, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194291.674757}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:54 INFO 140079012972352] Epoch[35] Batch[0] avg_epoch_loss=-1.583152\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=-1.5831515789\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:57 INFO 140079012972352] Epoch[35] Batch[5] avg_epoch_loss=-1.835104\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=-1.83510384957\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:44:57 INFO 140079012972352] Epoch[35] Batch [5]#011Speed: 197.26 samples/sec#011loss=-1.835104\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] Epoch[35] Batch[10] avg_epoch_loss=-1.827224\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=-1.81776747704\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] Epoch[35] Batch [10]#011Speed: 194.55 samples/sec#011loss=-1.817767\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9546.046018600464, \"sum\": 9546.046018600464, \"min\": 9546.046018600464}}, \"EndTime\": 1612194301.258953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194291.712844}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.503613125 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=35, train loss <loss>=-1.82722368024\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:01 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:04 INFO 140079012972352] Epoch[36] Batch[0] avg_epoch_loss=-1.920355\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=-1.92035460472\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:08 INFO 140079012972352] Epoch[36] Batch[5] avg_epoch_loss=-1.735724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=-1.73572427034\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:08 INFO 140079012972352] Epoch[36] Batch [5]#011Speed: 167.71 samples/sec#011loss=-1.735724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:11 INFO 140079012972352] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9939.340114593506, \"sum\": 9939.340114593506, \"min\": 9939.340114593506}}, \"EndTime\": 1612194311.199055, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194301.259078}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:11 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=127.068804464 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:11 INFO 140079012972352] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=36, train loss <loss>=-1.75641635656\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:11 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:14 INFO 140079012972352] Epoch[37] Batch[0] avg_epoch_loss=-2.076250\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=-2.07625031471\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:17 INFO 140079012972352] Epoch[37] Batch[5] avg_epoch_loss=-2.105705\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=-2.10570514202\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:17 INFO 140079012972352] Epoch[37] Batch [5]#011Speed: 197.62 samples/sec#011loss=-2.105705\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:20 INFO 140079012972352] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8981.301069259644, \"sum\": 8981.301069259644, \"min\": 8981.301069259644}}, \"EndTime\": 1612194320.181068, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194311.199168}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:20 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.290934325 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:20 INFO 140079012972352] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=37, train loss <loss>=-2.14692443609\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:20 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:23 INFO 140079012972352] Epoch[38] Batch[0] avg_epoch_loss=-2.263595\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=-2.26359462738\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:26 INFO 140079012972352] Epoch[38] Batch[5] avg_epoch_loss=-2.606488\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=-2.60648822784\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:26 INFO 140079012972352] Epoch[38] Batch [5]#011Speed: 196.15 samples/sec#011loss=-2.606488\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:29 INFO 140079012972352] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8858.951091766357, \"sum\": 8858.951091766357, \"min\": 8858.951091766357}}, \"EndTime\": 1612194329.040736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194320.181191}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:29 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.355695956 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:29 INFO 140079012972352] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=38, train loss <loss>=-2.5634973526\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:29 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:29 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_68569328-701c-4541-9db3-46433781d179-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 52.475929260253906, \"sum\": 52.475929260253906, \"min\": 52.475929260253906}}, \"EndTime\": 1612194329.093959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194329.040825}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:32 INFO 140079012972352] Epoch[39] Batch[0] avg_epoch_loss=-2.891914\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=-2.89191412926\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:35 INFO 140079012972352] Epoch[39] Batch[5] avg_epoch_loss=-2.701741\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=-2.70174058278\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:35 INFO 140079012972352] Epoch[39] Batch [5]#011Speed: 195.92 samples/sec#011loss=-2.701741\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] Epoch[39] Batch[10] avg_epoch_loss=-2.757833\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=-2.82514333725\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] Epoch[39] Batch [10]#011Speed: 197.76 samples/sec#011loss=-2.825143\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9514.657020568848, \"sum\": 9514.657020568848, \"min\": 9514.657020568848}}, \"EndTime\": 1612194338.608751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194329.094028}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.361877152 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=39, train loss <loss>=-2.7578327439\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:38 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_0685e03d-c84e-4ff9-8e8b-5c57af95fb4e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.521196365356445, \"sum\": 36.521196365356445, \"min\": 36.521196365356445}}, \"EndTime\": 1612194338.646009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194338.608845}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:41 INFO 140079012972352] Epoch[40] Batch[0] avg_epoch_loss=-2.730133\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=-2.73013329506\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:44 INFO 140079012972352] Epoch[40] Batch[5] avg_epoch_loss=-2.046591\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=-2.0465914011\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:44 INFO 140079012972352] Epoch[40] Batch [5]#011Speed: 194.77 samples/sec#011loss=-2.046591\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:47 INFO 140079012972352] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8922.414064407349, \"sum\": 8922.414064407349, \"min\": 8922.414064407349}}, \"EndTime\": 1612194347.568554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194338.646076}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:47 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.87028428 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:47 INFO 140079012972352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=40, train loss <loss>=-2.07021239996\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:47 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:50 INFO 140079012972352] Epoch[41] Batch[0] avg_epoch_loss=-2.219786\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=-2.21978592873\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:53 INFO 140079012972352] Epoch[41] Batch[5] avg_epoch_loss=-2.146213\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=-2.14621317387\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:53 INFO 140079012972352] Epoch[41] Batch [5]#011Speed: 197.97 samples/sec#011loss=-2.146213\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:56 INFO 140079012972352] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8937.327861785889, \"sum\": 8937.327861785889, \"min\": 8937.327861785889}}, \"EndTime\": 1612194356.506804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194347.56865}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:56 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.643953187 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:56 INFO 140079012972352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=41, train loss <loss>=-2.18203423023\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:56 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:59 INFO 140079012972352] Epoch[42] Batch[0] avg_epoch_loss=-2.301394\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:45:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=-2.30139422417\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:03 INFO 140079012972352] Epoch[42] Batch[5] avg_epoch_loss=-2.494351\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=-2.49435118834\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:03 INFO 140079012972352] Epoch[42] Batch [5]#011Speed: 185.92 samples/sec#011loss=-2.494351\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:06 INFO 140079012972352] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9871.864080429077, \"sum\": 9871.864080429077, \"min\": 9871.864080429077}}, \"EndTime\": 1612194366.379318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194356.506895}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:06 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=125.707661683 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:06 INFO 140079012972352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=42, train loss <loss>=-2.53700945377\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:06 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:09 INFO 140079012972352] Epoch[43] Batch[0] avg_epoch_loss=-2.518708\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=-2.51870846748\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:12 INFO 140079012972352] Epoch[43] Batch[5] avg_epoch_loss=-2.465993\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=-2.46599316597\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:12 INFO 140079012972352] Epoch[43] Batch [5]#011Speed: 197.52 samples/sec#011loss=-2.465993\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] Epoch[43] Batch[10] avg_epoch_loss=-2.614859\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=-2.79349808693\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] Epoch[43] Batch [10]#011Speed: 195.64 samples/sec#011loss=-2.793498\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] processed a total of 1333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9631.407022476196, \"sum\": 9631.407022476196, \"min\": 9631.407022476196}}, \"EndTime\": 1612194376.011575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194366.37952}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.399383667 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=43, train loss <loss>=-2.61485903913\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:16 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:19 INFO 140079012972352] Epoch[44] Batch[0] avg_epoch_loss=-2.393156\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=-2.39315605164\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:22 INFO 140079012972352] Epoch[44] Batch[5] avg_epoch_loss=-2.471901\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=-2.47190062205\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:22 INFO 140079012972352] Epoch[44] Batch [5]#011Speed: 196.35 samples/sec#011loss=-2.471901\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:24 INFO 140079012972352] processed a total of 1216 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8969.993829727173, \"sum\": 8969.993829727173, \"min\": 8969.993829727173}}, \"EndTime\": 1612194384.982321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194376.011671}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:24 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.560753355 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:24 INFO 140079012972352] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=44, train loss <loss>=-2.49704940319\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:24 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:28 INFO 140079012972352] Epoch[45] Batch[0] avg_epoch_loss=-2.694738\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=-2.69473814964\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:31 INFO 140079012972352] Epoch[45] Batch[5] avg_epoch_loss=-2.621417\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=-2.62141684691\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:31 INFO 140079012972352] Epoch[45] Batch [5]#011Speed: 195.78 samples/sec#011loss=-2.621417\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] Epoch[45] Batch[10] avg_epoch_loss=-2.725557\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=-2.85052609444\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] Epoch[45] Batch [10]#011Speed: 197.04 samples/sec#011loss=-2.850526\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9539.35194015503, \"sum\": 9539.35194015503, \"min\": 9539.35194015503}}, \"EndTime\": 1612194394.522485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194384.982425}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.702696283 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=45, train loss <loss>=-2.72555741397\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:34 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:37 INFO 140079012972352] Epoch[46] Batch[0] avg_epoch_loss=-0.679190\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=-0.679189741611\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:40 INFO 140079012972352] Epoch[46] Batch[5] avg_epoch_loss=-1.022577\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=-1.02257738511\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:40 INFO 140079012972352] Epoch[46] Batch [5]#011Speed: 196.53 samples/sec#011loss=-1.022577\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:43 INFO 140079012972352] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8897.310972213745, \"sum\": 8897.310972213745, \"min\": 8897.310972213745}}, \"EndTime\": 1612194403.420574, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194394.522591}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.916050451 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=46, train loss <loss>=-1.36908092499\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:46 INFO 140079012972352] Epoch[47] Batch[0] avg_epoch_loss=-1.785208\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=-1.78520798683\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:49 INFO 140079012972352] Epoch[47] Batch[5] avg_epoch_loss=-1.771579\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=-1.77157940467\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:49 INFO 140079012972352] Epoch[47] Batch [5]#011Speed: 196.08 samples/sec#011loss=-1.771579\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] Epoch[47] Batch[10] avg_epoch_loss=-1.866776\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=-1.98101148605\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] Epoch[47] Batch [10]#011Speed: 195.71 samples/sec#011loss=-1.981011\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] processed a total of 1341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9611.001968383789, \"sum\": 9611.001968383789, \"min\": 9611.001968383789}}, \"EndTime\": 1612194413.032656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194403.420675}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.525564303 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=47, train loss <loss>=-1.8667758053\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:53 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:56 INFO 140079012972352] Epoch[48] Batch[0] avg_epoch_loss=-1.880376\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=-1.88037598133\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:59 INFO 140079012972352] Epoch[48] Batch[5] avg_epoch_loss=-2.050724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=-2.05072420835\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:46:59 INFO 140079012972352] Epoch[48] Batch [5]#011Speed: 198.24 samples/sec#011loss=-2.050724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] Epoch[48] Batch[10] avg_epoch_loss=-2.159467\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=-2.28995769024\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] Epoch[48] Batch [10]#011Speed: 185.17 samples/sec#011loss=-2.289958\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] processed a total of 1320 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9686.691045761108, \"sum\": 9686.691045761108, \"min\": 9686.691045761108}}, \"EndTime\": 1612194422.720012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194413.032756}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.267166361 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=48, train loss <loss>=-2.15946670012\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:02 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:06 INFO 140079012972352] Epoch[49] Batch[0] avg_epoch_loss=-1.932790\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=-1.9327903986\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:09 INFO 140079012972352] Epoch[49] Batch[5] avg_epoch_loss=-2.134420\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=-2.13442011674\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:09 INFO 140079012972352] Epoch[49] Batch [5]#011Speed: 197.75 samples/sec#011loss=-2.134420\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:12 INFO 140079012972352] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9345.21198272705, \"sum\": 9345.21198272705, \"min\": 9345.21198272705}}, \"EndTime\": 1612194432.065964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194422.720131}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:12 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.040220962 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:12 INFO 140079012972352] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=49, train loss <loss>=-2.24103579521\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:12 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:15 INFO 140079012972352] Epoch[50] Batch[0] avg_epoch_loss=-2.555058\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=-2.55505776405\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:18 INFO 140079012972352] Epoch[50] Batch[5] avg_epoch_loss=-2.544913\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=-2.54491345088\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:18 INFO 140079012972352] Epoch[50] Batch [5]#011Speed: 196.28 samples/sec#011loss=-2.544913\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:21 INFO 140079012972352] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8960.932970046997, \"sum\": 8960.932970046997, \"min\": 8960.932970046997}}, \"EndTime\": 1612194441.027549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194432.066066}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:21 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.483474749 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:21 INFO 140079012972352] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=50, train loss <loss>=-2.6631695509\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:21 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:24 INFO 140079012972352] Epoch[51] Batch[0] avg_epoch_loss=-2.701215\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=-2.70121526718\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:27 INFO 140079012972352] Epoch[51] Batch[5] avg_epoch_loss=-2.292608\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=-2.2926077048\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:27 INFO 140079012972352] Epoch[51] Batch [5]#011Speed: 185.87 samples/sec#011loss=-2.292608\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] Epoch[51] Batch[10] avg_epoch_loss=-2.322396\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=-2.3581428051\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] Epoch[51] Batch [10]#011Speed: 183.00 samples/sec#011loss=-2.358143\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10032.257795333862, \"sum\": 10032.257795333862, \"min\": 10032.257795333862}}, \"EndTime\": 1612194451.060408, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194441.027648}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=129.679746258 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=51, train loss <loss>=-2.32239638675\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:31 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:34 INFO 140079012972352] Epoch[52] Batch[0] avg_epoch_loss=-2.302525\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=-2.30252528191\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:37 INFO 140079012972352] Epoch[52] Batch[5] avg_epoch_loss=-2.314288\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=-2.31428778172\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:37 INFO 140079012972352] Epoch[52] Batch [5]#011Speed: 197.31 samples/sec#011loss=-2.314288\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] Epoch[52] Batch[10] avg_epoch_loss=-2.441941\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=-2.59512434006\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] Epoch[52] Batch [10]#011Speed: 197.82 samples/sec#011loss=-2.595124\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9560.637950897217, \"sum\": 9560.637950897217, \"min\": 9560.637950897217}}, \"EndTime\": 1612194460.622254, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194451.060511}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.436515458 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=52, train loss <loss>=-2.44194076278\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:40 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:43 INFO 140079012972352] Epoch[53] Batch[0] avg_epoch_loss=-2.465837\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=-2.46583747864\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:46 INFO 140079012972352] Epoch[53] Batch[5] avg_epoch_loss=-2.770058\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=-2.77005799611\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:46 INFO 140079012972352] Epoch[53] Batch [5]#011Speed: 196.54 samples/sec#011loss=-2.770058\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:49 INFO 140079012972352] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8990.605115890503, \"sum\": 8990.605115890503, \"min\": 8990.605115890503}}, \"EndTime\": 1612194469.613587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194460.622351}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:49 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.364399611 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:49 INFO 140079012972352] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=53, train loss <loss>=-2.77036979198\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:49 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:49 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_61393fec-5e29-4bb5-969b-d01f990ef950-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.80298614501953, \"sum\": 33.80298614501953, \"min\": 33.80298614501953}}, \"EndTime\": 1612194469.648196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194469.613688}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:52 INFO 140079012972352] Epoch[54] Batch[0] avg_epoch_loss=-2.011045\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=-2.01104521751\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:55 INFO 140079012972352] Epoch[54] Batch[5] avg_epoch_loss=-2.541893\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=-2.54189324379\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:55 INFO 140079012972352] Epoch[54] Batch [5]#011Speed: 197.04 samples/sec#011loss=-2.541893\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:58 INFO 140079012972352] processed a total of 1225 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8849.720001220703, \"sum\": 8849.720001220703, \"min\": 8849.720001220703}}, \"EndTime\": 1612194478.498091, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194469.648277}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:58 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.400064518 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:58 INFO 140079012972352] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=54, train loss <loss>=-2.67434194088\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:47:58 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:01 INFO 140079012972352] Epoch[55] Batch[0] avg_epoch_loss=-2.357934\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=-2.35793352127\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:05 INFO 140079012972352] Epoch[55] Batch[5] avg_epoch_loss=-2.475563\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=-2.47556253274\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:05 INFO 140079012972352] Epoch[55] Batch [5]#011Speed: 167.25 samples/sec#011loss=-2.475563\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] Epoch[55] Batch[10] avg_epoch_loss=-2.609363\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=-2.76992249489\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] Epoch[55] Batch [10]#011Speed: 182.27 samples/sec#011loss=-2.769922\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10591.381072998047, \"sum\": 10591.381072998047, \"min\": 10591.381072998047}}, \"EndTime\": 1612194489.093468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194478.499449}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=121.606696623 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=55, train loss <loss>=-2.60936251554\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:09 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:12 INFO 140079012972352] Epoch[56] Batch[0] avg_epoch_loss=-2.035873\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=-2.03587293625\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:15 INFO 140079012972352] Epoch[56] Batch[5] avg_epoch_loss=-2.214701\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=-2.21470137437\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:15 INFO 140079012972352] Epoch[56] Batch [5]#011Speed: 198.75 samples/sec#011loss=-2.214701\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:18 INFO 140079012972352] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8992.457866668701, \"sum\": 8992.457866668701, \"min\": 8992.457866668701}}, \"EndTime\": 1612194498.086883, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194489.093567}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:18 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.228273455 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:18 INFO 140079012972352] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=56, train loss <loss>=-2.33321323395\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:18 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:21 INFO 140079012972352] Epoch[57] Batch[0] avg_epoch_loss=-2.497869\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=-2.49786925316\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:24 INFO 140079012972352] Epoch[57] Batch[5] avg_epoch_loss=-2.548100\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=-2.54809995492\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:24 INFO 140079012972352] Epoch[57] Batch [5]#011Speed: 196.04 samples/sec#011loss=-2.548100\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:26 INFO 140079012972352] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8853.295803070068, \"sum\": 8853.295803070068, \"min\": 8853.295803070068}}, \"EndTime\": 1612194506.94092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194498.086967}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:26 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.656574382 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:26 INFO 140079012972352] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=57, train loss <loss>=-2.53782627583\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:26 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:29 INFO 140079012972352] Epoch[58] Batch[0] avg_epoch_loss=-2.668970\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=-2.6689696312\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:33 INFO 140079012972352] Epoch[58] Batch[5] avg_epoch_loss=-2.798436\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=-2.79843600591\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:33 INFO 140079012972352] Epoch[58] Batch [5]#011Speed: 199.38 samples/sec#011loss=-2.798436\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:35 INFO 140079012972352] processed a total of 1230 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8784.59906578064, \"sum\": 8784.59906578064, \"min\": 8784.59906578064}}, \"EndTime\": 1612194515.72626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194506.941012}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:35 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.015622386 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:35 INFO 140079012972352] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=58, train loss <loss>=-2.89056437016\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:35 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:35 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_8f310371-0bf8-46ff-950d-9a50be746545-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 48.95782470703125, \"sum\": 48.95782470703125, \"min\": 48.95782470703125}}, \"EndTime\": 1612194515.775923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194515.726348}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:38 INFO 140079012972352] Epoch[59] Batch[0] avg_epoch_loss=-2.796344\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=-2.79634404182\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:42 INFO 140079012972352] Epoch[59] Batch[5] avg_epoch_loss=-2.726967\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=-2.72696683804\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:42 INFO 140079012972352] Epoch[59] Batch [5]#011Speed: 197.20 samples/sec#011loss=-2.726967\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] Epoch[59] Batch[10] avg_epoch_loss=-2.653911\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=-2.56624474525\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] Epoch[59] Batch [10]#011Speed: 196.89 samples/sec#011loss=-2.566245\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9493.342876434326, \"sum\": 9493.342876434326, \"min\": 9493.342876434326}}, \"EndTime\": 1612194525.269409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194515.775993}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.672360017 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=59, train loss <loss>=-2.65391134132\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:48 INFO 140079012972352] Epoch[60] Batch[0] avg_epoch_loss=-2.766031\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=-2.76603078842\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:51 INFO 140079012972352] Epoch[60] Batch[5] avg_epoch_loss=-2.595198\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=-2.59519803524\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:51 INFO 140079012972352] Epoch[60] Batch [5]#011Speed: 195.51 samples/sec#011loss=-2.595198\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] Epoch[60] Batch[10] avg_epoch_loss=-2.636285\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=-2.68558859825\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] Epoch[60] Batch [10]#011Speed: 198.40 samples/sec#011loss=-2.685589\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] processed a total of 1341 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9528.345823287964, \"sum\": 9528.345823287964, \"min\": 9528.345823287964}}, \"EndTime\": 1612194534.79856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194525.269572}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.734432871 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=60, train loss <loss>=-2.63628465479\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:57 INFO 140079012972352] Epoch[61] Batch[0] avg_epoch_loss=-2.523120\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:48:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=-2.52312016487\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:00 INFO 140079012972352] Epoch[61] Batch[5] avg_epoch_loss=-2.759667\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=-2.75966691971\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:00 INFO 140079012972352] Epoch[61] Batch [5]#011Speed: 197.19 samples/sec#011loss=-2.759667\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:04 INFO 140079012972352] processed a total of 1205 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9204.899072647095, \"sum\": 9204.899072647095, \"min\": 9204.899072647095}}, \"EndTime\": 1612194544.00426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194534.798758}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.906418318 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=61, train loss <loss>=-2.88244814873\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:07 INFO 140079012972352] Epoch[62] Batch[0] avg_epoch_loss=-2.509580\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=-2.50957965851\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:10 INFO 140079012972352] Epoch[62] Batch[5] avg_epoch_loss=-2.534837\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=-2.53483746449\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:10 INFO 140079012972352] Epoch[62] Batch [5]#011Speed: 197.95 samples/sec#011loss=-2.534837\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] Epoch[62] Batch[10] avg_epoch_loss=-2.645788\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=-2.77892951965\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] Epoch[62] Batch [10]#011Speed: 197.50 samples/sec#011loss=-2.778930\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9825.23512840271, \"sum\": 9825.23512840271, \"min\": 9825.23512840271}}, \"EndTime\": 1612194553.830225, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194544.004361}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.292751842 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=62, train loss <loss>=-2.64578839866\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:16 INFO 140079012972352] Epoch[63] Batch[0] avg_epoch_loss=-1.943954\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=-1.94395399094\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:20 INFO 140079012972352] Epoch[63] Batch[5] avg_epoch_loss=-2.187380\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=-2.18738027414\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:20 INFO 140079012972352] Epoch[63] Batch [5]#011Speed: 195.95 samples/sec#011loss=-2.187380\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] Epoch[63] Batch[10] avg_epoch_loss=-2.273080\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=-2.37591872215\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] Epoch[63] Batch [10]#011Speed: 197.52 samples/sec#011loss=-2.375919\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9619.032859802246, \"sum\": 9619.032859802246, \"min\": 9619.032859802246}}, \"EndTime\": 1612194563.449869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194553.830319}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.264975451 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=63, train loss <loss>=-2.27307956869\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:23 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:26 INFO 140079012972352] Epoch[64] Batch[0] avg_epoch_loss=-2.240845\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=-2.24084472656\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:29 INFO 140079012972352] Epoch[64] Batch[5] avg_epoch_loss=-2.432100\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=-2.43209973971\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:29 INFO 140079012972352] Epoch[64] Batch [5]#011Speed: 197.66 samples/sec#011loss=-2.432100\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:32 INFO 140079012972352] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8852.443933486938, \"sum\": 8852.443933486938, \"min\": 8852.443933486938}}, \"EndTime\": 1612194572.303087, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194563.450001}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:32 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.652962313 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:32 INFO 140079012972352] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=64, train loss <loss>=-2.53958489895\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:32 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:35 INFO 140079012972352] Epoch[65] Batch[0] avg_epoch_loss=-2.864814\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=-2.86481428146\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:38 INFO 140079012972352] Epoch[65] Batch[5] avg_epoch_loss=-2.909183\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=-2.90918338299\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:38 INFO 140079012972352] Epoch[65] Batch [5]#011Speed: 197.59 samples/sec#011loss=-2.909183\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] Epoch[65] Batch[10] avg_epoch_loss=-2.892220\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=-2.87186408043\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] Epoch[65] Batch [10]#011Speed: 197.48 samples/sec#011loss=-2.871864\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9585.2689743042, \"sum\": 9585.2689743042, \"min\": 9585.2689743042}}, \"EndTime\": 1612194581.889104, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194572.303219}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.205623747 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=65, train loss <loss>=-2.89222006364\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:41 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_a0841916-31b2-4bf6-beb8-32535d3dc321-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 50.76313018798828, \"sum\": 50.76313018798828, \"min\": 50.76313018798828}}, \"EndTime\": 1612194581.940554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194581.88919}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:44 INFO 140079012972352] Epoch[66] Batch[0] avg_epoch_loss=-1.919826\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=-1.91982591152\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:48 INFO 140079012972352] Epoch[66] Batch[5] avg_epoch_loss=-2.186257\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=-2.18625654777\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:48 INFO 140079012972352] Epoch[66] Batch [5]#011Speed: 197.53 samples/sec#011loss=-2.186257\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] Epoch[66] Batch[10] avg_epoch_loss=-2.411249\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=-2.68123979568\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] Epoch[66] Batch [10]#011Speed: 193.96 samples/sec#011loss=-2.681240\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9579.251050949097, \"sum\": 9579.251050949097, \"min\": 9579.251050949097}}, \"EndTime\": 1612194591.520027, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194581.940685}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.230113813 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=66, train loss <loss>=-2.41124893319\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:51 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:54 INFO 140079012972352] Epoch[67] Batch[0] avg_epoch_loss=-2.585948\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=-2.58594751358\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:57 INFO 140079012972352] Epoch[67] Batch[5] avg_epoch_loss=-2.481194\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=-2.48119429747\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:49:57 INFO 140079012972352] Epoch[67] Batch [5]#011Speed: 196.41 samples/sec#011loss=-2.481194\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] Epoch[67] Batch[10] avg_epoch_loss=-2.480971\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=-2.48070306778\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] Epoch[67] Batch [10]#011Speed: 191.33 samples/sec#011loss=-2.480703\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9681.725978851318, \"sum\": 9681.725978851318, \"min\": 9681.725978851318}}, \"EndTime\": 1612194601.202346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194591.520114}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.82548228 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=67, train loss <loss>=-2.48097101125\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:01 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:04 INFO 140079012972352] Epoch[68] Batch[0] avg_epoch_loss=-2.738577\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=-2.73857688904\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:08 INFO 140079012972352] Epoch[68] Batch[5] avg_epoch_loss=-2.505769\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=-2.50576945146\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:08 INFO 140079012972352] Epoch[68] Batch [5]#011Speed: 169.42 samples/sec#011loss=-2.505769\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] Epoch[68] Batch[10] avg_epoch_loss=-2.620404\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=-2.75796523094\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] Epoch[68] Batch [10]#011Speed: 196.60 samples/sec#011loss=-2.757965\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10310.503005981445, \"sum\": 10310.503005981445, \"min\": 10310.503005981445}}, \"EndTime\": 1612194611.513589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194601.20245}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=127.342612653 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=68, train loss <loss>=-2.62040389668\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:11 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:14 INFO 140079012972352] Epoch[69] Batch[0] avg_epoch_loss=-2.835881\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=-2.83588075638\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:17 INFO 140079012972352] Epoch[69] Batch[5] avg_epoch_loss=-2.645744\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=-2.64574384689\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:17 INFO 140079012972352] Epoch[69] Batch [5]#011Speed: 196.45 samples/sec#011loss=-2.645744\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] Epoch[69] Batch[10] avg_epoch_loss=-2.566531\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=-2.47147498131\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] Epoch[69] Batch [10]#011Speed: 193.24 samples/sec#011loss=-2.471475\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9651.450872421265, \"sum\": 9651.450872421265, \"min\": 9651.450872421265}}, \"EndTime\": 1612194621.165751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194611.513736}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.796384739 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=69, train loss <loss>=-2.56653072617\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:21 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:24 INFO 140079012972352] Epoch[70] Batch[0] avg_epoch_loss=-2.021566\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=-2.02156615257\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:27 INFO 140079012972352] Epoch[70] Batch[5] avg_epoch_loss=-2.111293\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=-2.11129285892\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:27 INFO 140079012972352] Epoch[70] Batch [5]#011Speed: 197.38 samples/sec#011loss=-2.111293\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] Epoch[70] Batch[10] avg_epoch_loss=-2.209145\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=-2.32656822205\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] Epoch[70] Batch [10]#011Speed: 195.12 samples/sec#011loss=-2.326568\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] processed a total of 1334 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9589.247941970825, \"sum\": 9589.247941970825, \"min\": 9589.247941970825}}, \"EndTime\": 1612194630.755767, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194621.165849}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.112066758 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=70, train loss <loss>=-2.2091452967\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:30 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:33 INFO 140079012972352] Epoch[71] Batch[0] avg_epoch_loss=-2.581313\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=-2.58131337166\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:37 INFO 140079012972352] Epoch[71] Batch[5] avg_epoch_loss=-2.575228\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=-2.57522833347\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:37 INFO 140079012972352] Epoch[71] Batch [5]#011Speed: 198.00 samples/sec#011loss=-2.575228\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:39 INFO 140079012972352] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8909.365892410278, \"sum\": 8909.365892410278, \"min\": 8909.365892410278}}, \"EndTime\": 1612194639.66586, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194630.75586}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:39 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.53391768 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:39 INFO 140079012972352] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=71, train loss <loss>=-2.69619166851\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:39 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:42 INFO 140079012972352] Epoch[72] Batch[0] avg_epoch_loss=-3.082941\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=-3.08294129372\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:45 INFO 140079012972352] Epoch[72] Batch[5] avg_epoch_loss=-2.620682\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=-2.62068247795\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:45 INFO 140079012972352] Epoch[72] Batch [5]#011Speed: 193.32 samples/sec#011loss=-2.620682\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:48 INFO 140079012972352] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8920.469045639038, \"sum\": 8920.469045639038, \"min\": 8920.469045639038}}, \"EndTime\": 1612194648.587358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194639.665967}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:48 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.591306518 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:48 INFO 140079012972352] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=72, train loss <loss>=-2.61030170918\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:48 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:51 INFO 140079012972352] Epoch[73] Batch[0] avg_epoch_loss=-2.201476\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=-2.20147633553\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:54 INFO 140079012972352] Epoch[73] Batch[5] avg_epoch_loss=-2.494916\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=-2.49491600196\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:54 INFO 140079012972352] Epoch[73] Batch [5]#011Speed: 196.32 samples/sec#011loss=-2.494916\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] Epoch[73] Batch[10] avg_epoch_loss=-2.568922\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=-2.65773019791\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] Epoch[73] Batch [10]#011Speed: 195.63 samples/sec#011loss=-2.657730\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9593.094825744629, \"sum\": 9593.094825744629, \"min\": 9593.094825744629}}, \"EndTime\": 1612194658.181344, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194648.587448}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.199412918 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=73, train loss <loss>=-2.56892245466\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:50:58 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:01 INFO 140079012972352] Epoch[74] Batch[0] avg_epoch_loss=-2.693943\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=-2.69394278526\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:05 INFO 140079012972352] Epoch[74] Batch[5] avg_epoch_loss=-2.803968\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=-2.80396807194\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:05 INFO 140079012972352] Epoch[74] Batch [5]#011Speed: 165.59 samples/sec#011loss=-2.803968\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] Epoch[74] Batch[10] avg_epoch_loss=-2.936995\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=-3.09662709236\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] Epoch[74] Batch [10]#011Speed: 174.77 samples/sec#011loss=-3.096627\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10626.713037490845, \"sum\": 10626.713037490845, \"min\": 10626.713037490845}}, \"EndTime\": 1612194668.808788, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194658.181442}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=122.143321987 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=74, train loss <loss>=-2.9369948994\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:08 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_539d56d2-6ba4-403d-a40b-ded6c40dd227-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.50403022766113, \"sum\": 36.50403022766113, \"min\": 36.50403022766113}}, \"EndTime\": 1612194668.846147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194668.808887}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:11 INFO 140079012972352] Epoch[75] Batch[0] avg_epoch_loss=-2.374297\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=-2.37429690361\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:15 INFO 140079012972352] Epoch[75] Batch[5] avg_epoch_loss=-2.618318\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=-2.6183180809\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:15 INFO 140079012972352] Epoch[75] Batch [5]#011Speed: 195.41 samples/sec#011loss=-2.618318\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] Epoch[75] Batch[10] avg_epoch_loss=-2.514655\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=-2.39025931358\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] Epoch[75] Batch [10]#011Speed: 196.55 samples/sec#011loss=-2.390259\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] processed a total of 1334 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9583.592891693115, \"sum\": 9583.592891693115, \"min\": 9583.592891693115}}, \"EndTime\": 1612194678.429925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194668.846221}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.192452046 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=75, train loss <loss>=-2.51465500485\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:18 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:21 INFO 140079012972352] Epoch[76] Batch[0] avg_epoch_loss=-2.604574\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=-2.60457396507\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:24 INFO 140079012972352] Epoch[76] Batch[5] avg_epoch_loss=-2.593325\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=-2.59332517783\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:24 INFO 140079012972352] Epoch[76] Batch [5]#011Speed: 196.67 samples/sec#011loss=-2.593325\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:27 INFO 140079012972352] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8935.26005744934, \"sum\": 8935.26005744934, \"min\": 8935.26005744934}}, \"EndTime\": 1612194687.366401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194678.430099}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.005021218 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=76, train loss <loss>=-2.69235675335\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:30 INFO 140079012972352] Epoch[77] Batch[0] avg_epoch_loss=-2.907979\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=-2.90797901154\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:33 INFO 140079012972352] Epoch[77] Batch[5] avg_epoch_loss=-2.967686\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=-2.96768565973\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:33 INFO 140079012972352] Epoch[77] Batch [5]#011Speed: 198.33 samples/sec#011loss=-2.967686\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:36 INFO 140079012972352] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8832.042932510376, \"sum\": 8832.042932510376, \"min\": 8832.042932510376}}, \"EndTime\": 1612194696.199164, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194687.366487}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:36 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.583753915 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:36 INFO 140079012972352] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=77, train loss <loss>=-3.02435288429\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:36 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:36 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_641063b6-c1b1-4170-9ed3-cf34b27a699d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.24802207946777, \"sum\": 55.24802207946777, \"min\": 55.24802207946777}}, \"EndTime\": 1612194696.255225, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194696.199251}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:39 INFO 140079012972352] Epoch[78] Batch[0] avg_epoch_loss=-2.798961\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=-2.79896068573\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:42 INFO 140079012972352] Epoch[78] Batch[5] avg_epoch_loss=-2.496853\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=-2.49685323238\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:42 INFO 140079012972352] Epoch[78] Batch [5]#011Speed: 198.64 samples/sec#011loss=-2.496853\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] Epoch[78] Batch[10] avg_epoch_loss=-2.682874\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=-2.90609812737\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] Epoch[78] Batch [10]#011Speed: 194.22 samples/sec#011loss=-2.906098\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9657.639980316162, \"sum\": 9657.639980316162, \"min\": 9657.639980316162}}, \"EndTime\": 1612194705.913012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194696.255287}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.608894925 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=78, train loss <loss>=-2.68287363919\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:48 INFO 140079012972352] Epoch[79] Batch[0] avg_epoch_loss=-2.413130\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=-2.41312980652\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:52 INFO 140079012972352] Epoch[79] Batch[5] avg_epoch_loss=-2.375514\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=-2.37551430861\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:52 INFO 140079012972352] Epoch[79] Batch [5]#011Speed: 193.62 samples/sec#011loss=-2.375514\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:54 INFO 140079012972352] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8937.880039215088, \"sum\": 8937.880039215088, \"min\": 8937.880039215088}}, \"EndTime\": 1612194714.851902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194705.913115}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.956968871 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=79, train loss <loss>=-2.39933185577\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:57 INFO 140079012972352] Epoch[80] Batch[0] avg_epoch_loss=-2.645288\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:51:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=-2.64528751373\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:01 INFO 140079012972352] Epoch[80] Batch[5] avg_epoch_loss=-2.530316\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=-2.53031647205\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:01 INFO 140079012972352] Epoch[80] Batch [5]#011Speed: 195.87 samples/sec#011loss=-2.530316\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:04 INFO 140079012972352] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9367.539882659912, \"sum\": 9367.539882659912, \"min\": 9367.539882659912}}, \"EndTime\": 1612194724.220227, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194714.851995}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.662202794 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=80, train loss <loss>=-2.61893129349\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:07 INFO 140079012972352] Epoch[81] Batch[0] avg_epoch_loss=-2.734578\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=-2.73457837105\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:10 INFO 140079012972352] Epoch[81] Batch[5] avg_epoch_loss=-2.877136\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=-2.87713575363\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:10 INFO 140079012972352] Epoch[81] Batch [5]#011Speed: 197.37 samples/sec#011loss=-2.877136\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] Epoch[81] Batch[10] avg_epoch_loss=-2.988940\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=-3.12310585976\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] Epoch[81] Batch [10]#011Speed: 198.66 samples/sec#011loss=-3.123106\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9853.34300994873, \"sum\": 9853.34300994873, \"min\": 9853.34300994873}}, \"EndTime\": 1612194734.074261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194724.220309}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.222314502 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=81, train loss <loss>=-2.98894034732\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:14 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:17 INFO 140079012972352] Epoch[82] Batch[0] avg_epoch_loss=-2.251902\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=-2.25190186501\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:20 INFO 140079012972352] Epoch[82] Batch[5] avg_epoch_loss=-1.773181\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=-1.77318091194\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:20 INFO 140079012972352] Epoch[82] Batch [5]#011Speed: 197.05 samples/sec#011loss=-1.773181\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:23 INFO 140079012972352] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8926.666975021362, \"sum\": 8926.666975021362, \"min\": 8926.666975021362}}, \"EndTime\": 1612194743.001569, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194734.07438}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:23 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.492045729 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:23 INFO 140079012972352] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=82, train loss <loss>=-1.9169775188\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:23 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:26 INFO 140079012972352] Epoch[83] Batch[0] avg_epoch_loss=-2.403556\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=-2.40355610847\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:29 INFO 140079012972352] Epoch[83] Batch[5] avg_epoch_loss=-2.330048\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=-2.3300478061\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:29 INFO 140079012972352] Epoch[83] Batch [5]#011Speed: 198.92 samples/sec#011loss=-2.330048\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] Epoch[83] Batch[10] avg_epoch_loss=-2.154755\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=-1.94440461993\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] Epoch[83] Batch [10]#011Speed: 196.09 samples/sec#011loss=-1.944405\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9590.56282043457, \"sum\": 9590.56282043457, \"min\": 9590.56282043457}}, \"EndTime\": 1612194752.592854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194743.001671}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.505426975 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=83, train loss <loss>=-2.15475544875\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:32 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:35 INFO 140079012972352] Epoch[84] Batch[0] avg_epoch_loss=-2.367307\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=-2.36730718613\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:38 INFO 140079012972352] Epoch[84] Batch[5] avg_epoch_loss=-2.416879\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=-2.41687921683\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:38 INFO 140079012972352] Epoch[84] Batch [5]#011Speed: 199.39 samples/sec#011loss=-2.416879\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:41 INFO 140079012972352] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8882.054090499878, \"sum\": 8882.054090499878, \"min\": 8882.054090499878}}, \"EndTime\": 1612194761.47551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194752.592939}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:41 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.869959785 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:41 INFO 140079012972352] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=84, train loss <loss>=-2.5527349472\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:41 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:44 INFO 140079012972352] Epoch[85] Batch[0] avg_epoch_loss=-2.814573\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=-2.81457281113\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:47 INFO 140079012972352] Epoch[85] Batch[5] avg_epoch_loss=-2.964431\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=-2.96443128586\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:47 INFO 140079012972352] Epoch[85] Batch [5]#011Speed: 195.43 samples/sec#011loss=-2.964431\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:50 INFO 140079012972352] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8917.327165603638, \"sum\": 8917.327165603638, \"min\": 8917.327165603638}}, \"EndTime\": 1612194770.393577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194761.475611}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:50 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.836803625 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:50 INFO 140079012972352] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=85, train loss <loss>=-3.03335072994\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:50 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:50 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_07bfdd3c-1c7d-4d0e-9c15-7e51de004380-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.2920036315918, \"sum\": 37.2920036315918, \"min\": 37.2920036315918}}, \"EndTime\": 1612194770.431751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194770.393742}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:53 INFO 140079012972352] Epoch[86] Batch[0] avg_epoch_loss=-3.228792\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=-3.22879219055\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:56 INFO 140079012972352] Epoch[86] Batch[5] avg_epoch_loss=-3.102391\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=-3.10239080588\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:56 INFO 140079012972352] Epoch[86] Batch [5]#011Speed: 196.84 samples/sec#011loss=-3.102391\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:59 INFO 140079012972352] Epoch[86] Batch[10] avg_epoch_loss=-3.177479\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:52:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=-3.26758513451\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:00 INFO 140079012972352] Epoch[86] Batch [10]#011Speed: 196.47 samples/sec#011loss=-3.267585\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:00 INFO 140079012972352] processed a total of 1312 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9568.726062774658, \"sum\": 9568.726062774658, \"min\": 9568.726062774658}}, \"EndTime\": 1612194780.000669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194770.431839}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:00 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.10960363 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:00 INFO 140079012972352] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=86, train loss <loss>=-3.17747913707\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:00 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:00 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_d36cd890-b7b1-4942-a273-ac418cc49ed7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.46687889099121, \"sum\": 42.46687889099121, \"min\": 42.46687889099121}}, \"EndTime\": 1612194780.044481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194780.000847}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:03 INFO 140079012972352] Epoch[87] Batch[0] avg_epoch_loss=-2.639039\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=-2.63903927803\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:07 INFO 140079012972352] Epoch[87] Batch[5] avg_epoch_loss=-3.048546\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=-3.0485462745\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:07 INFO 140079012972352] Epoch[87] Batch [5]#011Speed: 154.00 samples/sec#011loss=-3.048546\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:09 INFO 140079012972352] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9829.062223434448, \"sum\": 9829.062223434448, \"min\": 9829.062223434448}}, \"EndTime\": 1612194789.873708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194780.044564}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:09 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=128.9008267 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:09 INFO 140079012972352] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=87, train loss <loss>=-3.06987807751\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:09 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:12 INFO 140079012972352] Epoch[88] Batch[0] avg_epoch_loss=-3.258688\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=-3.25868797302\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:16 INFO 140079012972352] Epoch[88] Batch[5] avg_epoch_loss=-3.205501\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=-3.20550096035\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:16 INFO 140079012972352] Epoch[88] Batch [5]#011Speed: 195.41 samples/sec#011loss=-3.205501\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] Epoch[88] Batch[10] avg_epoch_loss=-2.970371\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=-2.6882140398\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] Epoch[88] Batch [10]#011Speed: 198.48 samples/sec#011loss=-2.688214\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9551.501035690308, \"sum\": 9551.501035690308, \"min\": 9551.501035690308}}, \"EndTime\": 1612194799.42617, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194789.873859}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.99761792 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=88, train loss <loss>=-2.97037054192\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:19 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:22 INFO 140079012972352] Epoch[89] Batch[0] avg_epoch_loss=-2.346334\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=-2.34633350372\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:25 INFO 140079012972352] Epoch[89] Batch[5] avg_epoch_loss=-2.627380\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=-2.62738017241\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:25 INFO 140079012972352] Epoch[89] Batch [5]#011Speed: 193.78 samples/sec#011loss=-2.627380\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] Epoch[89] Batch[10] avg_epoch_loss=-2.802175\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=-3.01192879677\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] Epoch[89] Batch [10]#011Speed: 197.24 samples/sec#011loss=-3.011929\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9685.052156448364, \"sum\": 9685.052156448364, \"min\": 9685.052156448364}}, \"EndTime\": 1612194809.111804, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194799.426264}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.60591451 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=89, train loss <loss>=-2.80217500166\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:29 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:32 INFO 140079012972352] Epoch[90] Batch[0] avg_epoch_loss=-2.453305\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=-2.45330524445\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:35 INFO 140079012972352] Epoch[90] Batch[5] avg_epoch_loss=-2.704942\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=-2.70494206746\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:35 INFO 140079012972352] Epoch[90] Batch [5]#011Speed: 197.92 samples/sec#011loss=-2.704942\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] Epoch[90] Batch[10] avg_epoch_loss=-2.806671\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=-2.92874593735\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] Epoch[90] Batch [10]#011Speed: 196.91 samples/sec#011loss=-2.928746\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] processed a total of 1373 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9539.32809829712, \"sum\": 9539.32809829712, \"min\": 9539.32809829712}}, \"EndTime\": 1612194818.651833, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194809.111905}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.92827094 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=90, train loss <loss>=-2.80667109923\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:38 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:41 INFO 140079012972352] Epoch[91] Batch[0] avg_epoch_loss=-3.107763\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=-3.10776257515\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:44 INFO 140079012972352] Epoch[91] Batch[5] avg_epoch_loss=-2.998175\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=-2.99817450841\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:44 INFO 140079012972352] Epoch[91] Batch [5]#011Speed: 198.71 samples/sec#011loss=-2.998175\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] Epoch[91] Batch[10] avg_epoch_loss=-3.032144\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=-3.0729072094\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] Epoch[91] Batch [10]#011Speed: 197.46 samples/sec#011loss=-3.072907\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9492.916107177734, \"sum\": 9492.916107177734, \"min\": 9492.916107177734}}, \"EndTime\": 1612194828.145468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194818.651933}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.941135524 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=91, train loss <loss>=-3.03214391795\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:48 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:51 INFO 140079012972352] Epoch[92] Batch[0] avg_epoch_loss=-2.727978\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=-2.7279779911\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:54 INFO 140079012972352] Epoch[92] Batch[5] avg_epoch_loss=-2.633368\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=-2.63336825371\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:54 INFO 140079012972352] Epoch[92] Batch [5]#011Speed: 196.75 samples/sec#011loss=-2.633368\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] Epoch[92] Batch[10] avg_epoch_loss=-2.682492\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=-2.74144115448\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] Epoch[92] Batch [10]#011Speed: 196.75 samples/sec#011loss=-2.741441\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9545.676946640015, \"sum\": 9545.676946640015, \"min\": 9545.676946640015}}, \"EndTime\": 1612194837.691922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194828.145541}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.547187038 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=92, train loss <loss>=-2.68249229951\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:53:57 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:00 INFO 140079012972352] Epoch[93] Batch[0] avg_epoch_loss=-2.808253\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=-2.80825328827\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:04 INFO 140079012972352] Epoch[93] Batch[5] avg_epoch_loss=-2.872907\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=-2.87290692329\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:04 INFO 140079012972352] Epoch[93] Batch [5]#011Speed: 178.22 samples/sec#011loss=-2.872907\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:07 INFO 140079012972352] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9792.330980300903, \"sum\": 9792.330980300903, \"min\": 9792.330980300903}}, \"EndTime\": 1612194847.484893, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194837.692018}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=127.853197464 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=93, train loss <loss>=-2.87633872032\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:07 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:10 INFO 140079012972352] Epoch[94] Batch[0] avg_epoch_loss=-2.715877\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=-2.7158768177\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:13 INFO 140079012972352] Epoch[94] Batch[5] avg_epoch_loss=-2.983785\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=-2.98378451665\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:13 INFO 140079012972352] Epoch[94] Batch [5]#011Speed: 198.84 samples/sec#011loss=-2.983785\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:16 INFO 140079012972352] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8775.739908218384, \"sum\": 8775.739908218384, \"min\": 8775.739908218384}}, \"EndTime\": 1612194856.261697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194847.484993}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:16 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.751937364 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:16 INFO 140079012972352] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=94, train loss <loss>=-2.92988095284\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:16 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:19 INFO 140079012972352] Epoch[95] Batch[0] avg_epoch_loss=-2.787411\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=-2.78741145134\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:22 INFO 140079012972352] Epoch[95] Batch[5] avg_epoch_loss=-3.099199\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=-3.09919945399\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:22 INFO 140079012972352] Epoch[95] Batch [5]#011Speed: 195.40 samples/sec#011loss=-3.099199\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] Epoch[95] Batch[10] avg_epoch_loss=-3.065886\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=-3.02590885162\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] Epoch[95] Batch [10]#011Speed: 197.52 samples/sec#011loss=-3.025909\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] processed a total of 1353 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9510.03885269165, \"sum\": 9510.03885269165, \"min\": 9510.03885269165}}, \"EndTime\": 1612194865.772409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194856.261802}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.268286782 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=95, train loss <loss>=-3.06588554382\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:25 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:28 INFO 140079012972352] Epoch[96] Batch[0] avg_epoch_loss=-2.740201\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=-2.74020051956\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:32 INFO 140079012972352] Epoch[96] Batch[5] avg_epoch_loss=-2.694311\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=-2.69431122144\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:32 INFO 140079012972352] Epoch[96] Batch [5]#011Speed: 195.88 samples/sec#011loss=-2.694311\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:34 INFO 140079012972352] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8883.692026138306, \"sum\": 8883.692026138306, \"min\": 8883.692026138306}}, \"EndTime\": 1612194874.656817, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194865.772524}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:34 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.367163217 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:34 INFO 140079012972352] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=96, train loss <loss>=-2.78228087425\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:34 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:37 INFO 140079012972352] Epoch[97] Batch[0] avg_epoch_loss=-2.865771\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=-2.8657708168\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:40 INFO 140079012972352] Epoch[97] Batch[5] avg_epoch_loss=-2.933843\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=-2.933842659\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:40 INFO 140079012972352] Epoch[97] Batch [5]#011Speed: 198.90 samples/sec#011loss=-2.933843\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:43 INFO 140079012972352] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8920.951128005981, \"sum\": 8920.951128005981, \"min\": 8920.951128005981}}, \"EndTime\": 1612194883.578586, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194874.656921}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.35713412 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=97, train loss <loss>=-2.99710142612\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:46 INFO 140079012972352] Epoch[98] Batch[0] avg_epoch_loss=-3.192464\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=-3.1924636364\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:49 INFO 140079012972352] Epoch[98] Batch[5] avg_epoch_loss=-3.160336\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=-3.16033621629\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:49 INFO 140079012972352] Epoch[98] Batch [5]#011Speed: 196.40 samples/sec#011loss=-3.160336\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] Epoch[98] Batch[10] avg_epoch_loss=-3.058944\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=-2.93727443218\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] Epoch[98] Batch [10]#011Speed: 192.61 samples/sec#011loss=-2.937274\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9630.84888458252, \"sum\": 9630.84888458252, \"min\": 9630.84888458252}}, \"EndTime\": 1612194893.210688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194883.578718}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.317689893 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=98, train loss <loss>=-3.05894449624\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:53 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:56 INFO 140079012972352] Epoch[99] Batch[0] avg_epoch_loss=-2.436408\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=-2.43640804291\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:59 INFO 140079012972352] Epoch[99] Batch[5] avg_epoch_loss=-2.482686\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=-2.48268640041\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:54:59 INFO 140079012972352] Epoch[99] Batch [5]#011Speed: 198.44 samples/sec#011loss=-2.482686\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:02 INFO 140079012972352] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9151.602029800415, \"sum\": 9151.602029800415, \"min\": 9151.602029800415}}, \"EndTime\": 1612194902.363058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194893.210921}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:02 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.367509336 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:02 INFO 140079012972352] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=99, train loss <loss>=-2.42122454643\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:02 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:05 INFO 140079012972352] Epoch[100] Batch[0] avg_epoch_loss=-2.532675\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=-2.53267502785\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:09 INFO 140079012972352] Epoch[100] Batch[5] avg_epoch_loss=-2.583657\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=-2.58365662893\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:09 INFO 140079012972352] Epoch[100] Batch [5]#011Speed: 181.76 samples/sec#011loss=-2.583657\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] Epoch[100] Batch[10] avg_epoch_loss=-2.709569\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=-2.86066360474\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] Epoch[100] Batch [10]#011Speed: 195.38 samples/sec#011loss=-2.860664\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10291.589975357056, \"sum\": 10291.589975357056, \"min\": 10291.589975357056}}, \"EndTime\": 1612194912.65539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194902.36315}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=129.035530505 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=100, train loss <loss>=-2.70956889066\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:12 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:15 INFO 140079012972352] Epoch[101] Batch[0] avg_epoch_loss=-3.016260\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=-3.01625990868\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:18 INFO 140079012972352] Epoch[101] Batch[5] avg_epoch_loss=-3.106625\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=-3.10662484169\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:18 INFO 140079012972352] Epoch[101] Batch [5]#011Speed: 196.72 samples/sec#011loss=-3.106625\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] Epoch[101] Batch[10] avg_epoch_loss=-2.985938\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=-2.84111464024\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] Epoch[101] Batch [10]#011Speed: 193.95 samples/sec#011loss=-2.841115\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9527.821063995361, \"sum\": 9527.821063995361, \"min\": 9527.821063995361}}, \"EndTime\": 1612194922.18393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194912.655492}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.5454345 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=101, train loss <loss>=-2.98593838648\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:22 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:25 INFO 140079012972352] Epoch[102] Batch[0] avg_epoch_loss=-2.382240\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=-2.38224005699\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:28 INFO 140079012972352] Epoch[102] Batch[5] avg_epoch_loss=-2.991724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=-2.99172365665\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:28 INFO 140079012972352] Epoch[102] Batch [5]#011Speed: 197.37 samples/sec#011loss=-2.991724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] Epoch[102] Batch[10] avg_epoch_loss=-2.932666\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=-2.86179618835\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] Epoch[102] Batch [10]#011Speed: 197.89 samples/sec#011loss=-2.861796\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9509.164810180664, \"sum\": 9509.164810180664, \"min\": 9509.164810180664}}, \"EndTime\": 1612194931.693706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194922.184028}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.916566453 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=102, train loss <loss>=-2.93266571652\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:31 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:34 INFO 140079012972352] Epoch[103] Batch[0] avg_epoch_loss=-2.839275\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=-2.83927536011\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:38 INFO 140079012972352] Epoch[103] Batch[5] avg_epoch_loss=-2.660031\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=-2.66003080209\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:38 INFO 140079012972352] Epoch[103] Batch [5]#011Speed: 197.11 samples/sec#011loss=-2.660031\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] Epoch[103] Batch[10] avg_epoch_loss=-2.779731\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=-2.92337126732\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] Epoch[103] Batch [10]#011Speed: 196.63 samples/sec#011loss=-2.923371\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9605.75795173645, \"sum\": 9605.75795173645, \"min\": 9605.75795173645}}, \"EndTime\": 1612194941.300103, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194931.693803}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.500754231 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=103, train loss <loss>=-2.77973101356\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:41 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:44 INFO 140079012972352] Epoch[104] Batch[0] avg_epoch_loss=-2.989433\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=-2.98943305016\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:47 INFO 140079012972352] Epoch[104] Batch[5] avg_epoch_loss=-2.915806\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=-2.91580625375\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:47 INFO 140079012972352] Epoch[104] Batch [5]#011Speed: 195.35 samples/sec#011loss=-2.915806\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] Epoch[104] Batch[10] avg_epoch_loss=-3.015827\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=104, batch=10 train loss <loss>=-3.13585243225\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] Epoch[104] Batch [10]#011Speed: 196.68 samples/sec#011loss=-3.135852\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9616.153955459595, \"sum\": 9616.153955459595, \"min\": 9616.153955459595}}, \"EndTime\": 1612194950.916856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194941.300197}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.578645145 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=104, train loss <loss>=-3.01582724398\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:50 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:53 INFO 140079012972352] Epoch[105] Batch[0] avg_epoch_loss=-2.982942\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=-2.98294186592\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:57 INFO 140079012972352] Epoch[105] Batch[5] avg_epoch_loss=-3.035275\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=-3.03527494272\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:57 INFO 140079012972352] Epoch[105] Batch [5]#011Speed: 196.67 samples/sec#011loss=-3.035275\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:59 INFO 140079012972352] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8921.903848648071, \"sum\": 8921.903848648071, \"min\": 8921.903848648071}}, \"EndTime\": 1612194959.839791, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194950.916954}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:59 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.757552316 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:59 INFO 140079012972352] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=105, train loss <loss>=-2.99365434647\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:55:59 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:02 INFO 140079012972352] Epoch[106] Batch[0] avg_epoch_loss=-2.821031\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=-2.82103061676\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:07 INFO 140079012972352] Epoch[106] Batch[5] avg_epoch_loss=-2.734724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=-2.73472372691\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:07 INFO 140079012972352] Epoch[106] Batch [5]#011Speed: 158.62 samples/sec#011loss=-2.734724\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:09 INFO 140079012972352] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9772.295951843262, \"sum\": 9772.295951843262, \"min\": 9772.295951843262}}, \"EndTime\": 1612194969.612836, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194959.83988}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:09 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=126.478328348 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:09 INFO 140079012972352] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=106, train loss <loss>=-2.64319103956\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:09 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:12 INFO 140079012972352] Epoch[107] Batch[0] avg_epoch_loss=-2.861005\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=-2.86100530624\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:15 INFO 140079012972352] Epoch[107] Batch[5] avg_epoch_loss=-2.806449\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=-2.80644857883\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:15 INFO 140079012972352] Epoch[107] Batch [5]#011Speed: 197.89 samples/sec#011loss=-2.806449\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] Epoch[107] Batch[10] avg_epoch_loss=-2.712641\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=107, batch=10 train loss <loss>=-2.60007221699\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] Epoch[107] Batch [10]#011Speed: 196.45 samples/sec#011loss=-2.600072\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9610.231876373291, \"sum\": 9610.231876373291, \"min\": 9610.231876373291}}, \"EndTime\": 1612194979.22369, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194969.612923}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.392089596 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=107, train loss <loss>=-2.71264114163\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:19 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:22 INFO 140079012972352] Epoch[108] Batch[0] avg_epoch_loss=-3.197780\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=-3.19777965546\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:25 INFO 140079012972352] Epoch[108] Batch[5] avg_epoch_loss=-3.123240\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=-3.12323987484\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:25 INFO 140079012972352] Epoch[108] Batch [5]#011Speed: 195.97 samples/sec#011loss=-3.123240\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] Epoch[108] Batch[10] avg_epoch_loss=-3.121752\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=108, batch=10 train loss <loss>=-3.11996674538\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] Epoch[108] Batch [10]#011Speed: 196.24 samples/sec#011loss=-3.119967\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9518.16987991333, \"sum\": 9518.16987991333, \"min\": 9518.16987991333}}, \"EndTime\": 1612194988.742529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194979.223788}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.154821447 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=108, train loss <loss>=-3.12175208872\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:28 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:31 INFO 140079012972352] Epoch[109] Batch[0] avg_epoch_loss=-2.977710\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=-2.9777097702\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:34 INFO 140079012972352] Epoch[109] Batch[5] avg_epoch_loss=-2.358716\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=-2.35871646802\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:34 INFO 140079012972352] Epoch[109] Batch [5]#011Speed: 196.12 samples/sec#011loss=-2.358716\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:37 INFO 140079012972352] processed a total of 1196 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8820.594072341919, \"sum\": 8820.594072341919, \"min\": 8820.594072341919}}, \"EndTime\": 1612194997.563847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194988.742618}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:37 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.589040181 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:37 INFO 140079012972352] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=109, train loss <loss>=-2.45206595659\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:37 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:40 INFO 140079012972352] Epoch[110] Batch[0] avg_epoch_loss=-2.642219\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=-2.64221906662\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:43 INFO 140079012972352] Epoch[110] Batch[5] avg_epoch_loss=-2.495666\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=-2.49566630522\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:43 INFO 140079012972352] Epoch[110] Batch [5]#011Speed: 196.00 samples/sec#011loss=-2.495666\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] Epoch[110] Batch[10] avg_epoch_loss=-2.477702\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=110, batch=10 train loss <loss>=-2.45614500046\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] Epoch[110] Batch [10]#011Speed: 197.95 samples/sec#011loss=-2.456145\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9544.13890838623, \"sum\": 9544.13890838623, \"min\": 9544.13890838623}}, \"EndTime\": 1612195007.108731, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612194997.56398}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.626491604 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=110, train loss <loss>=-2.47770207578\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:47 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:50 INFO 140079012972352] Epoch[111] Batch[0] avg_epoch_loss=-2.659342\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=-2.65934228897\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:53 INFO 140079012972352] Epoch[111] Batch[5] avg_epoch_loss=-2.839546\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=-2.83954644203\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:53 INFO 140079012972352] Epoch[111] Batch [5]#011Speed: 193.06 samples/sec#011loss=-2.839546\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:56 INFO 140079012972352] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8984.01403427124, \"sum\": 8984.01403427124, \"min\": 8984.01403427124}}, \"EndTime\": 1612195016.093371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195007.108818}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:56 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.02026282 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:56 INFO 140079012972352] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=111, train loss <loss>=-2.96179599762\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:56 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:59 INFO 140079012972352] Epoch[112] Batch[0] avg_epoch_loss=-3.070386\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:56:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=-3.07038593292\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:02 INFO 140079012972352] Epoch[112] Batch[5] avg_epoch_loss=-3.183360\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=-3.18336029847\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:02 INFO 140079012972352] Epoch[112] Batch [5]#011Speed: 188.01 samples/sec#011loss=-3.183360\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:05 INFO 140079012972352] processed a total of 1237 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9565.678834915161, \"sum\": 9565.678834915161, \"min\": 9565.678834915161}}, \"EndTime\": 1612195025.660226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195016.0935}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:05 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=129.314767678 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:05 INFO 140079012972352] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=112, train loss <loss>=-3.1473733902\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:05 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:08 INFO 140079012972352] Epoch[113] Batch[0] avg_epoch_loss=-3.291637\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=-3.29163742065\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:12 INFO 140079012972352] Epoch[113] Batch[5] avg_epoch_loss=-2.774442\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=-2.77444235484\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:12 INFO 140079012972352] Epoch[113] Batch [5]#011Speed: 198.92 samples/sec#011loss=-2.774442\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] Epoch[113] Batch[10] avg_epoch_loss=-2.801366\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=113, batch=10 train loss <loss>=-2.83367452621\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] Epoch[113] Batch [10]#011Speed: 196.35 samples/sec#011loss=-2.833675\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] processed a total of 1335 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9781.800985336304, \"sum\": 9781.800985336304, \"min\": 9781.800985336304}}, \"EndTime\": 1612195035.4455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195025.660312}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.475869389 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=113, train loss <loss>=-2.8013660691\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:15 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:18 INFO 140079012972352] Epoch[114] Batch[0] avg_epoch_loss=-2.933088\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=-2.93308758736\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:21 INFO 140079012972352] Epoch[114] Batch[5] avg_epoch_loss=-2.979384\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=-2.97938406467\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:21 INFO 140079012972352] Epoch[114] Batch [5]#011Speed: 197.59 samples/sec#011loss=-2.979384\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:24 INFO 140079012972352] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8884.382963180542, \"sum\": 8884.382963180542, \"min\": 8884.382963180542}}, \"EndTime\": 1612195044.330594, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195035.4456}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:24 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.594562487 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:24 INFO 140079012972352] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=114, train loss <loss>=-3.0255297184\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:24 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:27 INFO 140079012972352] Epoch[115] Batch[0] avg_epoch_loss=-3.316899\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=-3.31689929962\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:30 INFO 140079012972352] Epoch[115] Batch[5] avg_epoch_loss=-3.330818\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=-3.33081758022\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:30 INFO 140079012972352] Epoch[115] Batch [5]#011Speed: 195.35 samples/sec#011loss=-3.330818\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] Epoch[115] Batch[10] avg_epoch_loss=-3.151503\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=115, batch=10 train loss <loss>=-2.93632507324\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] Epoch[115] Batch [10]#011Speed: 196.33 samples/sec#011loss=-2.936325\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9601.768970489502, \"sum\": 9601.768970489502, \"min\": 9601.768970489502}}, \"EndTime\": 1612195053.933142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195044.330687}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.78396613 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=115, train loss <loss>=-3.15150280432\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:33 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:37 INFO 140079012972352] Epoch[116] Batch[0] avg_epoch_loss=-2.000186\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=-2.00018644333\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:40 INFO 140079012972352] Epoch[116] Batch[5] avg_epoch_loss=-2.461833\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=-2.46183311939\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:40 INFO 140079012972352] Epoch[116] Batch [5]#011Speed: 195.11 samples/sec#011loss=-2.461833\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] Epoch[116] Batch[10] avg_epoch_loss=-2.649132\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=116, batch=10 train loss <loss>=-2.87389082909\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] Epoch[116] Batch [10]#011Speed: 198.25 samples/sec#011loss=-2.873891\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] processed a total of 1362 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9580.941915512085, \"sum\": 9580.941915512085, \"min\": 9580.941915512085}}, \"EndTime\": 1612195063.515216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195053.933293}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.155239018 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=116, train loss <loss>=-2.64913207834\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:46 INFO 140079012972352] Epoch[117] Batch[0] avg_epoch_loss=-2.890731\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=-2.89073085785\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:49 INFO 140079012972352] Epoch[117] Batch[5] avg_epoch_loss=-2.825069\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=-2.8250686725\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:49 INFO 140079012972352] Epoch[117] Batch [5]#011Speed: 197.31 samples/sec#011loss=-2.825069\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] Epoch[117] Batch[10] avg_epoch_loss=-2.788538\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=117, batch=10 train loss <loss>=-2.74470083714\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] Epoch[117] Batch [10]#011Speed: 195.46 samples/sec#011loss=-2.744701\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9615.206956863403, \"sum\": 9615.206956863403, \"min\": 9615.206956863403}}, \"EndTime\": 1612195073.131236, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195063.51531}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.304587632 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=117, train loss <loss>=-2.78853783824\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:53 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:56 INFO 140079012972352] Epoch[118] Batch[0] avg_epoch_loss=-2.934769\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=-2.93476891518\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:59 INFO 140079012972352] Epoch[118] Batch[5] avg_epoch_loss=-3.071473\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=-3.07147304217\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:57:59 INFO 140079012972352] Epoch[118] Batch [5]#011Speed: 197.25 samples/sec#011loss=-3.071473\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] Epoch[118] Batch[10] avg_epoch_loss=-2.950198\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=118, batch=10 train loss <loss>=-2.80466737747\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] Epoch[118] Batch [10]#011Speed: 186.62 samples/sec#011loss=-2.804667\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9779.317140579224, \"sum\": 9779.317140579224, \"min\": 9779.317140579224}}, \"EndTime\": 1612195082.91114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195073.131329}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.011527627 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=118, train loss <loss>=-2.95019774003\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:02 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:06 INFO 140079012972352] Epoch[119] Batch[0] avg_epoch_loss=-2.258059\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=-2.25805926323\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:09 INFO 140079012972352] Epoch[119] Batch[5] avg_epoch_loss=-2.671502\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=-2.67150215308\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:09 INFO 140079012972352] Epoch[119] Batch [5]#011Speed: 191.49 samples/sec#011loss=-2.671502\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:12 INFO 140079012972352] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9482.450008392334, \"sum\": 9482.450008392334, \"min\": 9482.450008392334}}, \"EndTime\": 1612195092.39418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195082.911229}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:12 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.878877895 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:12 INFO 140079012972352] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=119, train loss <loss>=-2.78546168804\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:12 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:15 INFO 140079012972352] Epoch[120] Batch[0] avg_epoch_loss=-2.894964\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=-2.89496421814\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:18 INFO 140079012972352] Epoch[120] Batch[5] avg_epoch_loss=-3.063642\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=-3.06364242236\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:18 INFO 140079012972352] Epoch[120] Batch [5]#011Speed: 197.04 samples/sec#011loss=-3.063642\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] Epoch[120] Batch[10] avg_epoch_loss=-3.096758\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=120, batch=10 train loss <loss>=-3.13649601936\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] Epoch[120] Batch [10]#011Speed: 194.70 samples/sec#011loss=-3.136496\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9564.94688987732, \"sum\": 9564.94688987732, \"min\": 9564.94688987732}}, \"EndTime\": 1612195101.959776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195092.394269}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.747433407 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=120, train loss <loss>=-3.09675769372\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:21 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:25 INFO 140079012972352] Epoch[121] Batch[0] avg_epoch_loss=-2.977038\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=-2.97703814507\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:28 INFO 140079012972352] Epoch[121] Batch[5] avg_epoch_loss=-3.191760\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=-3.19175978502\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:28 INFO 140079012972352] Epoch[121] Batch [5]#011Speed: 199.21 samples/sec#011loss=-3.191760\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:30 INFO 140079012972352] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8970.968008041382, \"sum\": 8970.968008041382, \"min\": 8970.968008041382}}, \"EndTime\": 1612195110.931312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195101.959866}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:30 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.110291778 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:30 INFO 140079012972352] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=121, train loss <loss>=-3.2303160429\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:30 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:30 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_a5248c45-da4c-48dd-b175-e28a67071306-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.305904388427734, \"sum\": 36.305904388427734, \"min\": 36.305904388427734}}, \"EndTime\": 1612195110.968344, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195110.931389}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:33 INFO 140079012972352] Epoch[122] Batch[0] avg_epoch_loss=-3.528452\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=-3.52845168114\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:37 INFO 140079012972352] Epoch[122] Batch[5] avg_epoch_loss=-3.345369\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=-3.34536925952\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:37 INFO 140079012972352] Epoch[122] Batch [5]#011Speed: 197.95 samples/sec#011loss=-3.345369\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:39 INFO 140079012972352] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8790.50898551941, \"sum\": 8790.50898551941, \"min\": 8790.50898551941}}, \"EndTime\": 1612195119.759054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195110.96841}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:39 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.9679745 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:39 INFO 140079012972352] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=122, train loss <loss>=-3.24920761585\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:39 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:39 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_724ce980-05d1-46e4-bee4-c82876b3e33f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.158084869384766, \"sum\": 36.158084869384766, \"min\": 36.158084869384766}}, \"EndTime\": 1612195119.796628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195119.759149}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:42 INFO 140079012972352] Epoch[123] Batch[0] avg_epoch_loss=-3.106971\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=-3.10697054863\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:46 INFO 140079012972352] Epoch[123] Batch[5] avg_epoch_loss=-3.142876\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=-3.14287610849\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:46 INFO 140079012972352] Epoch[123] Batch [5]#011Speed: 198.87 samples/sec#011loss=-3.142876\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] Epoch[123] Batch[10] avg_epoch_loss=-3.282855\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=-3.45082979202\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] Epoch[123] Batch [10]#011Speed: 199.48 samples/sec#011loss=-3.450830\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9438.711881637573, \"sum\": 9438.711881637573, \"min\": 9438.711881637573}}, \"EndTime\": 1612195129.23544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195119.796672}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.669279068 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=123, train loss <loss>=-3.28285505555\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:49 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_adc13370-69ca-493b-956f-72e5b99b6856-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 52.3529052734375, \"sum\": 52.3529052734375, \"min\": 52.3529052734375}}, \"EndTime\": 1612195129.28851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195129.235534}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:52 INFO 140079012972352] Epoch[124] Batch[0] avg_epoch_loss=-2.747207\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=-2.74720668793\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:55 INFO 140079012972352] Epoch[124] Batch[5] avg_epoch_loss=-2.922086\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=-2.92208623886\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:55 INFO 140079012972352] Epoch[124] Batch [5]#011Speed: 196.15 samples/sec#011loss=-2.922086\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:58 INFO 140079012972352] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8873.193979263306, \"sum\": 8873.193979263306, \"min\": 8873.193979263306}}, \"EndTime\": 1612195138.161856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195129.288593}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:58 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.54785489 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:58 INFO 140079012972352] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=124, train loss <loss>=-2.9494540453\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:58:58 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:01 INFO 140079012972352] Epoch[125] Batch[0] avg_epoch_loss=-3.277141\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=-3.27714133263\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:04 INFO 140079012972352] Epoch[125] Batch[5] avg_epoch_loss=-3.326185\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=-3.32618486881\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:04 INFO 140079012972352] Epoch[125] Batch [5]#011Speed: 176.31 samples/sec#011loss=-3.326185\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:07 INFO 140079012972352] processed a total of 1237 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9824.400901794434, \"sum\": 9824.400901794434, \"min\": 9824.400901794434}}, \"EndTime\": 1612195147.986878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195138.161943}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=125.908357784 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=125, train loss <loss>=-3.33149673939\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:07 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:08 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_465ebd37-faea-469b-aecf-53829526e7cd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.5518798828125, \"sum\": 37.5518798828125, \"min\": 37.5518798828125}}, \"EndTime\": 1612195148.025345, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195147.987034}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:11 INFO 140079012972352] Epoch[126] Batch[0] avg_epoch_loss=-3.387817\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=-3.38781690598\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:14 INFO 140079012972352] Epoch[126] Batch[5] avg_epoch_loss=-3.222007\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=-3.22200699647\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:14 INFO 140079012972352] Epoch[126] Batch [5]#011Speed: 198.37 samples/sec#011loss=-3.222007\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] Epoch[126] Batch[10] avg_epoch_loss=-3.142102\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=126, batch=10 train loss <loss>=-3.04621500969\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] Epoch[126] Batch [10]#011Speed: 197.61 samples/sec#011loss=-3.046215\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] processed a total of 1344 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9498.402833938599, \"sum\": 9498.402833938599, \"min\": 9498.402833938599}}, \"EndTime\": 1612195157.523878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195148.025414}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.495384776 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=126, train loss <loss>=-3.14210154793\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:17 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:20 INFO 140079012972352] Epoch[127] Batch[0] avg_epoch_loss=-2.925629\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=-2.92562890053\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:23 INFO 140079012972352] Epoch[127] Batch[5] avg_epoch_loss=-2.479622\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=-2.47962240378\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:23 INFO 140079012972352] Epoch[127] Batch [5]#011Speed: 196.02 samples/sec#011loss=-2.479622\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] Epoch[127] Batch[10] avg_epoch_loss=-2.568832\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=-2.67588400841\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] Epoch[127] Batch [10]#011Speed: 197.51 samples/sec#011loss=-2.675884\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9537.347078323364, \"sum\": 9537.347078323364, \"min\": 9537.347078323364}}, \"EndTime\": 1612195167.061937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195157.523974}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.465469827 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=127, train loss <loss>=-2.56883222407\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:30 INFO 140079012972352] Epoch[128] Batch[0] avg_epoch_loss=-2.237578\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=-2.23757791519\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:33 INFO 140079012972352] Epoch[128] Batch[5] avg_epoch_loss=-2.511809\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=-2.51180867354\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:33 INFO 140079012972352] Epoch[128] Batch [5]#011Speed: 198.73 samples/sec#011loss=-2.511809\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] Epoch[128] Batch[10] avg_epoch_loss=-2.583052\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=-2.66854429245\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] Epoch[128] Batch [10]#011Speed: 196.34 samples/sec#011loss=-2.668544\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] processed a total of 1355 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9566.0400390625, \"sum\": 9566.0400390625, \"min\": 9566.0400390625}}, \"EndTime\": 1612195176.628607, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195167.062032}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.644952922 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=128, train loss <loss>=-2.58305213668\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:36 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:39 INFO 140079012972352] Epoch[129] Batch[0] avg_epoch_loss=-2.879023\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=-2.87902283669\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:42 INFO 140079012972352] Epoch[129] Batch[5] avg_epoch_loss=-2.940134\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=-2.94013381004\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:42 INFO 140079012972352] Epoch[129] Batch [5]#011Speed: 197.24 samples/sec#011loss=-2.940134\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:45 INFO 140079012972352] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8932.346105575562, \"sum\": 8932.346105575562, \"min\": 8932.346105575562}}, \"EndTime\": 1612195185.561534, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195176.628699}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.185126554 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=129, train loss <loss>=-2.98272678852\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:48 INFO 140079012972352] Epoch[130] Batch[0] avg_epoch_loss=-2.924797\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=-2.92479658127\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:51 INFO 140079012972352] Epoch[130] Batch[5] avg_epoch_loss=-3.109926\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=-3.1099263827\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:51 INFO 140079012972352] Epoch[130] Batch [5]#011Speed: 195.90 samples/sec#011loss=-3.109926\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:54 INFO 140079012972352] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8842.061996459961, \"sum\": 8842.061996459961, \"min\": 8842.061996459961}}, \"EndTime\": 1612195194.404548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195185.561615}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.236452148 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=130, train loss <loss>=-3.1519343853\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:57 INFO 140079012972352] Epoch[131] Batch[0] avg_epoch_loss=-3.063890\u001b[0m\n",
      "\u001b[34m[02/01/2021 15:59:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=-3.06389021873\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:00 INFO 140079012972352] Epoch[131] Batch[5] avg_epoch_loss=-3.102132\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=-3.1021323204\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:00 INFO 140079012972352] Epoch[131] Batch [5]#011Speed: 197.49 samples/sec#011loss=-3.102132\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] Epoch[131] Batch[10] avg_epoch_loss=-2.745202\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=-2.31688620234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] Epoch[131] Batch [10]#011Speed: 185.50 samples/sec#011loss=-2.316886\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9683.023929595947, \"sum\": 9683.023929595947, \"min\": 9683.023929595947}}, \"EndTime\": 1612195204.088284, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195194.404649}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.911280544 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=131, train loss <loss>=-2.74520226674\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:07 INFO 140079012972352] Epoch[132] Batch[0] avg_epoch_loss=-2.160142\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=-2.1601421833\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:10 INFO 140079012972352] Epoch[132] Batch[5] avg_epoch_loss=-2.742896\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=-2.74289588133\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:10 INFO 140079012972352] Epoch[132] Batch [5]#011Speed: 197.50 samples/sec#011loss=-2.742896\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] Epoch[132] Batch[10] avg_epoch_loss=-2.928810\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=-3.15190758705\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] Epoch[132] Batch [10]#011Speed: 198.03 samples/sec#011loss=-3.151908\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9975.648880004883, \"sum\": 9975.648880004883, \"min\": 9975.648880004883}}, \"EndTime\": 1612195214.06449, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195204.088368}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=129.011990146 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=132, train loss <loss>=-2.92881029302\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:14 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:17 INFO 140079012972352] Epoch[133] Batch[0] avg_epoch_loss=-2.886504\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=-2.88650417328\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:20 INFO 140079012972352] Epoch[133] Batch[5] avg_epoch_loss=-2.769968\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=-2.7699679931\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:20 INFO 140079012972352] Epoch[133] Batch [5]#011Speed: 198.73 samples/sec#011loss=-2.769968\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:22 INFO 140079012972352] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8837.822914123535, \"sum\": 8837.822914123535, \"min\": 8837.822914123535}}, \"EndTime\": 1612195222.903041, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195214.064619}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:22 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.322415182 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:22 INFO 140079012972352] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=133, train loss <loss>=-2.83247599602\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:22 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:25 INFO 140079012972352] Epoch[134] Batch[0] avg_epoch_loss=-3.167951\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=-3.16795063019\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:29 INFO 140079012972352] Epoch[134] Batch[5] avg_epoch_loss=-3.099653\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=-3.09965252876\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:29 INFO 140079012972352] Epoch[134] Batch [5]#011Speed: 197.77 samples/sec#011loss=-3.099653\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:31 INFO 140079012972352] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8933.541059494019, \"sum\": 8933.541059494019, \"min\": 8933.541059494019}}, \"EndTime\": 1612195231.837465, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195222.903118}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.270601335 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=134, train loss <loss>=-3.10624330044\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:31 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:34 INFO 140079012972352] Epoch[135] Batch[0] avg_epoch_loss=-2.674419\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=-2.67441892624\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:38 INFO 140079012972352] Epoch[135] Batch[5] avg_epoch_loss=-3.044820\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=-3.04481971264\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:38 INFO 140079012972352] Epoch[135] Batch [5]#011Speed: 196.32 samples/sec#011loss=-3.044820\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:40 INFO 140079012972352] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8794.450044631958, \"sum\": 8794.450044631958, \"min\": 8794.450044631958}}, \"EndTime\": 1612195240.632624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195231.837548}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.58793738 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=135, train loss <loss>=-3.08052070141\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:40 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:43 INFO 140079012972352] Epoch[136] Batch[0] avg_epoch_loss=-3.317012\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=-3.31701231003\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:47 INFO 140079012972352] Epoch[136] Batch[5] avg_epoch_loss=-3.344662\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=-3.34466230869\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:47 INFO 140079012972352] Epoch[136] Batch [5]#011Speed: 195.74 samples/sec#011loss=-3.344662\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] Epoch[136] Batch[10] avg_epoch_loss=-3.368462\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=136, batch=10 train loss <loss>=-3.39702153206\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] Epoch[136] Batch [10]#011Speed: 199.11 samples/sec#011loss=-3.397022\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9582.41891860962, \"sum\": 9582.41891860962, \"min\": 9582.41891860962}}, \"EndTime\": 1612195250.215876, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195240.632704}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.993433845 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=136, train loss <loss>=-3.36846195568\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:50 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_b253f59c-d4e9-4655-9ae2-c911e49c3468-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.891916275024414, \"sum\": 33.891916275024414, \"min\": 33.891916275024414}}, \"EndTime\": 1612195250.250526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195250.215968}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:53 INFO 140079012972352] Epoch[137] Batch[0] avg_epoch_loss=-2.758023\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=-2.75802278519\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:56 INFO 140079012972352] Epoch[137] Batch[5] avg_epoch_loss=-2.519133\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=-2.51913305124\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:56 INFO 140079012972352] Epoch[137] Batch [5]#011Speed: 192.40 samples/sec#011loss=-2.519133\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:59 INFO 140079012972352] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8942.460060119629, \"sum\": 8942.460060119629, \"min\": 8942.460060119629}}, \"EndTime\": 1612195259.193119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195250.250593}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:59 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.023411277 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:59 INFO 140079012972352] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=137, train loss <loss>=-2.6295637846\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:00:59 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:02 INFO 140079012972352] Epoch[138] Batch[0] avg_epoch_loss=-2.931138\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=-2.9311375618\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:06 INFO 140079012972352] Epoch[138] Batch[5] avg_epoch_loss=-2.954807\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=-2.95480664571\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:06 INFO 140079012972352] Epoch[138] Batch [5]#011Speed: 173.67 samples/sec#011loss=-2.954807\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:09 INFO 140079012972352] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9954.023122787476, \"sum\": 9954.023122787476, \"min\": 9954.023122787476}}, \"EndTime\": 1612195269.147848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195259.193208}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:09 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=127.58482482 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:09 INFO 140079012972352] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=138, train loss <loss>=-3.02282783985\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:09 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:12 INFO 140079012972352] Epoch[139] Batch[0] avg_epoch_loss=-3.108873\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=-3.10887336731\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:15 INFO 140079012972352] Epoch[139] Batch[5] avg_epoch_loss=-3.101716\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=-3.10171631972\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:15 INFO 140079012972352] Epoch[139] Batch [5]#011Speed: 197.05 samples/sec#011loss=-3.101716\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:18 INFO 140079012972352] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8893.55993270874, \"sum\": 8893.55993270874, \"min\": 8893.55993270874}}, \"EndTime\": 1612195278.042074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195269.147942}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:18 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.336367459 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:18 INFO 140079012972352] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=139, train loss <loss>=-3.20542206764\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:18 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:21 INFO 140079012972352] Epoch[140] Batch[0] avg_epoch_loss=-3.168117\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=-3.16811656952\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:24 INFO 140079012972352] Epoch[140] Batch[5] avg_epoch_loss=-3.128551\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=-3.12855136395\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:24 INFO 140079012972352] Epoch[140] Batch [5]#011Speed: 195.03 samples/sec#011loss=-3.128551\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] Epoch[140] Batch[10] avg_epoch_loss=-3.261823\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=-3.4217499733\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] Epoch[140] Batch [10]#011Speed: 194.07 samples/sec#011loss=-3.421750\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9552.877187728882, \"sum\": 9552.877187728882, \"min\": 9552.877187728882}}, \"EndTime\": 1612195287.595947, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195278.042152}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.512198546 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=140, train loss <loss>=-3.26182345911\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:30 INFO 140079012972352] Epoch[141] Batch[0] avg_epoch_loss=-2.387615\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=-2.38761520386\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:33 INFO 140079012972352] Epoch[141] Batch[5] avg_epoch_loss=-2.809544\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=-2.80954416593\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:33 INFO 140079012972352] Epoch[141] Batch [5]#011Speed: 195.67 samples/sec#011loss=-2.809544\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] Epoch[141] Batch[10] avg_epoch_loss=-3.004864\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=-3.23924803734\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] Epoch[141] Batch [10]#011Speed: 194.64 samples/sec#011loss=-3.239248\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9617.635011672974, \"sum\": 9617.635011672974, \"min\": 9617.635011672974}}, \"EndTime\": 1612195297.214182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195287.596044}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.750220947 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=141, train loss <loss>=-3.00486410748\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:37 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:40 INFO 140079012972352] Epoch[142] Batch[0] avg_epoch_loss=-2.773630\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=-2.77362990379\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:43 INFO 140079012972352] Epoch[142] Batch[5] avg_epoch_loss=-3.016987\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=-3.01698700587\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:43 INFO 140079012972352] Epoch[142] Batch [5]#011Speed: 193.24 samples/sec#011loss=-3.016987\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] Epoch[142] Batch[10] avg_epoch_loss=-3.226158\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=142, batch=10 train loss <loss>=-3.47716302872\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] Epoch[142] Batch [10]#011Speed: 195.86 samples/sec#011loss=-3.477163\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9633.800029754639, \"sum\": 9633.800029754639, \"min\": 9633.800029754639}}, \"EndTime\": 1612195306.848576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195297.214277}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.96747418 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=142, train loss <loss>=-3.22615792535\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:46 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:49 INFO 140079012972352] Epoch[143] Batch[0] avg_epoch_loss=-1.550012\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=-1.55001187325\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:53 INFO 140079012972352] Epoch[143] Batch[5] avg_epoch_loss=-2.136183\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=-2.13618282477\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:53 INFO 140079012972352] Epoch[143] Batch [5]#011Speed: 195.85 samples/sec#011loss=-2.136183\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] Epoch[143] Batch[10] avg_epoch_loss=-2.355297\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=-2.61823372841\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] Epoch[143] Batch [10]#011Speed: 194.47 samples/sec#011loss=-2.618234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9581.236839294434, \"sum\": 9581.236839294434, \"min\": 9581.236839294434}}, \"EndTime\": 1612195316.43058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195306.848668}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.262421468 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=143, train loss <loss>=-2.35529687188\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:56 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:59 INFO 140079012972352] Epoch[144] Batch[0] avg_epoch_loss=-2.714173\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:01:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=-2.71417307854\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:02 INFO 140079012972352] Epoch[144] Batch[5] avg_epoch_loss=-2.439363\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=-2.43936312199\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:02 INFO 140079012972352] Epoch[144] Batch [5]#011Speed: 186.50 samples/sec#011loss=-2.439363\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:05 INFO 140079012972352] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9475.226879119873, \"sum\": 9475.226879119873, \"min\": 9475.226879119873}}, \"EndTime\": 1612195325.906545, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195316.430673}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:05 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.971149708 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:05 INFO 140079012972352] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=144, train loss <loss>=-2.49581649303\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:05 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:09 INFO 140079012972352] Epoch[145] Batch[0] avg_epoch_loss=-2.566780\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=145, batch=0 train loss <loss>=-2.56677961349\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:12 INFO 140079012972352] Epoch[145] Batch[5] avg_epoch_loss=-2.758295\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=145, batch=5 train loss <loss>=-2.75829486052\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:12 INFO 140079012972352] Epoch[145] Batch [5]#011Speed: 197.75 samples/sec#011loss=-2.758295\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:15 INFO 140079012972352] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9169.156074523926, \"sum\": 9169.156074523926, \"min\": 9169.156074523926}}, \"EndTime\": 1612195335.076434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195325.906641}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:15 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.887732834 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:15 INFO 140079012972352] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=145, train loss <loss>=-2.91993482113\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:15 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:18 INFO 140079012972352] Epoch[146] Batch[0] avg_epoch_loss=-3.341511\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=146, batch=0 train loss <loss>=-3.34151053429\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:21 INFO 140079012972352] Epoch[146] Batch[5] avg_epoch_loss=-3.386686\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=146, batch=5 train loss <loss>=-3.38668612639\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:21 INFO 140079012972352] Epoch[146] Batch [5]#011Speed: 198.01 samples/sec#011loss=-3.386686\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] Epoch[146] Batch[10] avg_epoch_loss=-3.267126\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=146, batch=10 train loss <loss>=-3.12365374565\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] Epoch[146] Batch [10]#011Speed: 191.42 samples/sec#011loss=-3.123654\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] processed a total of 1350 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9645.726919174194, \"sum\": 9645.726919174194, \"min\": 9645.726919174194}}, \"EndTime\": 1612195344.722837, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195335.076564}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.956358354 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=146, train loss <loss>=-3.26712595333\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:24 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:27 INFO 140079012972352] Epoch[147] Batch[0] avg_epoch_loss=-2.932015\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=147, batch=0 train loss <loss>=-2.93201541901\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:31 INFO 140079012972352] Epoch[147] Batch[5] avg_epoch_loss=-2.640454\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=147, batch=5 train loss <loss>=-2.64045413335\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:31 INFO 140079012972352] Epoch[147] Batch [5]#011Speed: 196.07 samples/sec#011loss=-2.640454\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:33 INFO 140079012972352] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8950.788021087646, \"sum\": 8950.788021087646, \"min\": 8950.788021087646}}, \"EndTime\": 1612195353.674367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195344.72293}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:33 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.884839748 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:33 INFO 140079012972352] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=147, train loss <loss>=-2.75330393314\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:33 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:36 INFO 140079012972352] Epoch[148] Batch[0] avg_epoch_loss=-2.982996\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=148, batch=0 train loss <loss>=-2.9829955101\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:39 INFO 140079012972352] Epoch[148] Batch[5] avg_epoch_loss=-2.961428\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=148, batch=5 train loss <loss>=-2.96142804623\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:39 INFO 140079012972352] Epoch[148] Batch [5]#011Speed: 196.30 samples/sec#011loss=-2.961428\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] Epoch[148] Batch[10] avg_epoch_loss=-3.035030\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=148, batch=10 train loss <loss>=-3.12335300446\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] Epoch[148] Batch [10]#011Speed: 195.19 samples/sec#011loss=-3.123353\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] processed a total of 1349 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9594.836950302124, \"sum\": 9594.836950302124, \"min\": 9594.836950302124}}, \"EndTime\": 1612195363.270027, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195353.674457}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.594146325 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=148, train loss <loss>=-3.03503029997\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:46 INFO 140079012972352] Epoch[149] Batch[0] avg_epoch_loss=-3.194174\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=149, batch=0 train loss <loss>=-3.19417405128\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:49 INFO 140079012972352] Epoch[149] Batch[5] avg_epoch_loss=-3.185373\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=149, batch=5 train loss <loss>=-3.18537251155\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:49 INFO 140079012972352] Epoch[149] Batch [5]#011Speed: 196.37 samples/sec#011loss=-3.185373\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:52 INFO 140079012972352] processed a total of 1245 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8825.963020324707, \"sum\": 8825.963020324707, \"min\": 8825.963020324707}}, \"EndTime\": 1612195372.096926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195363.270119}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:52 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.058903065 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:52 INFO 140079012972352] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=149, train loss <loss>=-3.23239729404\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:52 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:55 INFO 140079012972352] Epoch[150] Batch[0] avg_epoch_loss=-3.216135\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=150, batch=0 train loss <loss>=-3.21613454819\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:58 INFO 140079012972352] Epoch[150] Batch[5] avg_epoch_loss=-3.320482\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=150, batch=5 train loss <loss>=-3.32048173745\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:02:58 INFO 140079012972352] Epoch[150] Batch [5]#011Speed: 195.94 samples/sec#011loss=-3.320482\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] Epoch[150] Batch[10] avg_epoch_loss=-3.341145\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=150, batch=10 train loss <loss>=-3.36594152451\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] Epoch[150] Batch [10]#011Speed: 183.78 samples/sec#011loss=-3.365942\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9791.944026947021, \"sum\": 9791.944026947021, \"min\": 9791.944026947021}}, \"EndTime\": 1612195381.889834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195372.097019}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.926174798 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=150, train loss <loss>=-3.34114527702\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:01 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:05 INFO 140079012972352] Epoch[151] Batch[0] avg_epoch_loss=-3.280308\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=151, batch=0 train loss <loss>=-3.28030753136\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:08 INFO 140079012972352] Epoch[151] Batch[5] avg_epoch_loss=-3.062063\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=151, batch=5 train loss <loss>=-3.06206293901\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:08 INFO 140079012972352] Epoch[151] Batch [5]#011Speed: 195.39 samples/sec#011loss=-3.062063\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] Epoch[151] Batch[10] avg_epoch_loss=-2.757136\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=151, batch=10 train loss <loss>=-2.39122412205\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] Epoch[151] Batch [10]#011Speed: 198.04 samples/sec#011loss=-2.391224\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9872.497081756592, \"sum\": 9872.497081756592, \"min\": 9872.497081756592}}, \"EndTime\": 1612195391.762969, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195381.889926}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.170057848 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=151, train loss <loss>=-2.75713620403\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:11 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:14 INFO 140079012972352] Epoch[152] Batch[0] avg_epoch_loss=-2.459421\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=152, batch=0 train loss <loss>=-2.459420681\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:18 INFO 140079012972352] Epoch[152] Batch[5] avg_epoch_loss=-2.542146\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=152, batch=5 train loss <loss>=-2.54214564959\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:18 INFO 140079012972352] Epoch[152] Batch [5]#011Speed: 191.50 samples/sec#011loss=-2.542146\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:20 INFO 140079012972352] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8948.101043701172, \"sum\": 8948.101043701172, \"min\": 8948.101043701172}}, \"EndTime\": 1612195400.711679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195391.763105}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:20 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.591345758 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:20 INFO 140079012972352] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=152, train loss <loss>=-2.56900930405\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:20 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:23 INFO 140079012972352] Epoch[153] Batch[0] avg_epoch_loss=-2.850685\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=153, batch=0 train loss <loss>=-2.85068535805\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:26 INFO 140079012972352] Epoch[153] Batch[5] avg_epoch_loss=-2.774024\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=153, batch=5 train loss <loss>=-2.77402444681\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:26 INFO 140079012972352] Epoch[153] Batch [5]#011Speed: 195.93 samples/sec#011loss=-2.774024\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:29 INFO 140079012972352] processed a total of 1266 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8969.42400932312, \"sum\": 8969.42400932312, \"min\": 8969.42400932312}}, \"EndTime\": 1612195409.682071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195400.711789}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:29 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.143876069 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:29 INFO 140079012972352] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=153, train loss <loss>=-2.79924120903\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:29 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:32 INFO 140079012972352] Epoch[154] Batch[0] avg_epoch_loss=-3.155123\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=154, batch=0 train loss <loss>=-3.15512299538\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:35 INFO 140079012972352] Epoch[154] Batch[5] avg_epoch_loss=-2.859422\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=154, batch=5 train loss <loss>=-2.85942248503\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:35 INFO 140079012972352] Epoch[154] Batch [5]#011Speed: 197.93 samples/sec#011loss=-2.859422\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] Epoch[154] Batch[10] avg_epoch_loss=-2.983226\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=154, batch=10 train loss <loss>=-3.13179068565\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] Epoch[154] Batch [10]#011Speed: 198.10 samples/sec#011loss=-3.131791\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] processed a total of 1315 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9542.824983596802, \"sum\": 9542.824983596802, \"min\": 9542.824983596802}}, \"EndTime\": 1612195419.225602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195409.682172}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.797682879 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=154, train loss <loss>=-2.98322621259\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:39 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:42 INFO 140079012972352] Epoch[155] Batch[0] avg_epoch_loss=-2.951811\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=155, batch=0 train loss <loss>=-2.95181059837\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:45 INFO 140079012972352] Epoch[155] Batch[5] avg_epoch_loss=-2.876158\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=155, batch=5 train loss <loss>=-2.87615776062\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:45 INFO 140079012972352] Epoch[155] Batch [5]#011Speed: 198.46 samples/sec#011loss=-2.876158\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] Epoch[155] Batch[10] avg_epoch_loss=-2.879711\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=155, batch=10 train loss <loss>=-2.88397445679\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] Epoch[155] Batch [10]#011Speed: 197.86 samples/sec#011loss=-2.883974\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9444.189071655273, \"sum\": 9444.189071655273, \"min\": 9444.189071655273}}, \"EndTime\": 1612195428.670477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195419.225704}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.965719779 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=155, train loss <loss>=-2.87971080433\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:48 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:51 INFO 140079012972352] Epoch[156] Batch[0] avg_epoch_loss=-2.412140\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=156, batch=0 train loss <loss>=-2.41214036942\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:55 INFO 140079012972352] Epoch[156] Batch[5] avg_epoch_loss=-2.780033\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=156, batch=5 train loss <loss>=-2.78003342946\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:55 INFO 140079012972352] Epoch[156] Batch [5]#011Speed: 182.47 samples/sec#011loss=-2.780033\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] Epoch[156] Batch[10] avg_epoch_loss=-2.689847\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=156, batch=10 train loss <loss>=-2.58162285089\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] Epoch[156] Batch [10]#011Speed: 193.51 samples/sec#011loss=-2.581623\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] processed a total of 1309 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9846.30799293518, \"sum\": 9846.30799293518, \"min\": 9846.30799293518}}, \"EndTime\": 1612195438.517567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195428.670621}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.941451071 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=156, train loss <loss>=-2.68984680284\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:03:58 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:01 INFO 140079012972352] Epoch[157] Batch[0] avg_epoch_loss=-3.003596\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=157, batch=0 train loss <loss>=-3.00359630585\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:05 INFO 140079012972352] Epoch[157] Batch[5] avg_epoch_loss=-2.973604\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=157, batch=5 train loss <loss>=-2.97360424201\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:05 INFO 140079012972352] Epoch[157] Batch [5]#011Speed: 178.98 samples/sec#011loss=-2.973604\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:08 INFO 140079012972352] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9901.807069778442, \"sum\": 9901.807069778442, \"min\": 9901.807069778442}}, \"EndTime\": 1612195448.420058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195438.51765}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:08 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=126.035627558 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:08 INFO 140079012972352] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=157, train loss <loss>=-3.01178486347\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:08 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:11 INFO 140079012972352] Epoch[158] Batch[0] avg_epoch_loss=-2.572289\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=158, batch=0 train loss <loss>=-2.5722887516\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:14 INFO 140079012972352] Epoch[158] Batch[5] avg_epoch_loss=-2.806842\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=158, batch=5 train loss <loss>=-2.80684204896\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:14 INFO 140079012972352] Epoch[158] Batch [5]#011Speed: 198.88 samples/sec#011loss=-2.806842\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] Epoch[158] Batch[10] avg_epoch_loss=-3.052788\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=158, batch=10 train loss <loss>=-3.34792256355\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] Epoch[158] Batch [10]#011Speed: 196.30 samples/sec#011loss=-3.347923\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] processed a total of 1297 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9523.50902557373, \"sum\": 9523.50902557373, \"min\": 9523.50902557373}}, \"EndTime\": 1612195457.944201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195448.420167}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.187452593 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=158, train loss <loss>=-3.05278773741\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:17 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:20 INFO 140079012972352] Epoch[159] Batch[0] avg_epoch_loss=-2.438985\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=159, batch=0 train loss <loss>=-2.43898487091\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:24 INFO 140079012972352] Epoch[159] Batch[5] avg_epoch_loss=-2.486929\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=159, batch=5 train loss <loss>=-2.48692893982\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:24 INFO 140079012972352] Epoch[159] Batch [5]#011Speed: 199.38 samples/sec#011loss=-2.486929\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] Epoch[159] Batch[10] avg_epoch_loss=-2.674227\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=159, batch=10 train loss <loss>=-2.89898576736\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] Epoch[159] Batch [10]#011Speed: 195.31 samples/sec#011loss=-2.898986\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9473.07801246643, \"sum\": 9473.07801246643, \"min\": 9473.07801246643}}, \"EndTime\": 1612195467.417931, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195457.944285}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.968074803 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=159, train loss <loss>=-2.67422749779\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:30 INFO 140079012972352] Epoch[160] Batch[0] avg_epoch_loss=-2.147374\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=160, batch=0 train loss <loss>=-2.14737415314\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:33 INFO 140079012972352] Epoch[160] Batch[5] avg_epoch_loss=-2.429059\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=160, batch=5 train loss <loss>=-2.4290591081\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:33 INFO 140079012972352] Epoch[160] Batch [5]#011Speed: 197.87 samples/sec#011loss=-2.429059\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:36 INFO 140079012972352] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8865.674018859863, \"sum\": 8865.674018859863, \"min\": 8865.674018859863}}, \"EndTime\": 1612195476.284179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195467.418016}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:36 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.314419296 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:36 INFO 140079012972352] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=160, train loss <loss>=-2.5320754528\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:36 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:39 INFO 140079012972352] Epoch[161] Batch[0] avg_epoch_loss=-2.693327\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=161, batch=0 train loss <loss>=-2.69332718849\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:42 INFO 140079012972352] Epoch[161] Batch[5] avg_epoch_loss=-2.737967\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=161, batch=5 train loss <loss>=-2.73796673616\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:42 INFO 140079012972352] Epoch[161] Batch [5]#011Speed: 197.84 samples/sec#011loss=-2.737967\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:45 INFO 140079012972352] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8941.349029541016, \"sum\": 8941.349029541016, \"min\": 8941.349029541016}}, \"EndTime\": 1612195485.226261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195476.284266}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.257772249 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=161, train loss <loss>=-2.7705627203\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:48 INFO 140079012972352] Epoch[162] Batch[0] avg_epoch_loss=-3.159880\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=162, batch=0 train loss <loss>=-3.15987992287\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:51 INFO 140079012972352] Epoch[162] Batch[5] avg_epoch_loss=-3.234789\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=162, batch=5 train loss <loss>=-3.23478937149\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:51 INFO 140079012972352] Epoch[162] Batch [5]#011Speed: 196.97 samples/sec#011loss=-3.234789\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] Epoch[162] Batch[10] avg_epoch_loss=-3.323625\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=162, batch=10 train loss <loss>=-3.43022694588\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] Epoch[162] Batch [10]#011Speed: 195.14 samples/sec#011loss=-3.430227\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9604.437828063965, \"sum\": 9604.437828063965, \"min\": 9604.437828063965}}, \"EndTime\": 1612195494.831873, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195485.226382}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.477639675 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=162, train loss <loss>=-3.32362463258\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:57 INFO 140079012972352] Epoch[163] Batch[0] avg_epoch_loss=-0.444421\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:04:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=163, batch=0 train loss <loss>=-0.44442114234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:01 INFO 140079012972352] Epoch[163] Batch[5] avg_epoch_loss=-1.171447\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=163, batch=5 train loss <loss>=-1.17144672076\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:01 INFO 140079012972352] Epoch[163] Batch [5]#011Speed: 195.13 samples/sec#011loss=-1.171447\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] Epoch[163] Batch[10] avg_epoch_loss=-1.724078\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=163, batch=10 train loss <loss>=-2.38723526001\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] Epoch[163] Batch [10]#011Speed: 186.10 samples/sec#011loss=-2.387235\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9811.975002288818, \"sum\": 9811.975002288818, \"min\": 9811.975002288818}}, \"EndTime\": 1612195504.644696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195494.831988}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.877828054 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=163, train loss <loss>=-1.72407787496\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:07 INFO 140079012972352] Epoch[164] Batch[0] avg_epoch_loss=-2.232490\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=164, batch=0 train loss <loss>=-2.23248958588\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:11 INFO 140079012972352] Epoch[164] Batch[5] avg_epoch_loss=-2.359141\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=164, batch=5 train loss <loss>=-2.35914063454\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:11 INFO 140079012972352] Epoch[164] Batch [5]#011Speed: 173.93 samples/sec#011loss=-2.359141\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:14 INFO 140079012972352] processed a total of 1220 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9457.80086517334, \"sum\": 9457.80086517334, \"min\": 9457.80086517334}}, \"EndTime\": 1612195514.103157, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195504.644788}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:14 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=128.991929148 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:14 INFO 140079012972352] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=164, train loss <loss>=-2.43913078308\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:14 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:17 INFO 140079012972352] Epoch[165] Batch[0] avg_epoch_loss=-2.413303\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=165, batch=0 train loss <loss>=-2.41330265999\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:20 INFO 140079012972352] Epoch[165] Batch[5] avg_epoch_loss=-2.553408\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=165, batch=5 train loss <loss>=-2.55340810617\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:20 INFO 140079012972352] Epoch[165] Batch [5]#011Speed: 196.21 samples/sec#011loss=-2.553408\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] Epoch[165] Batch[10] avg_epoch_loss=-2.817444\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=165, batch=10 train loss <loss>=-3.1342871666\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] Epoch[165] Batch [10]#011Speed: 196.77 samples/sec#011loss=-3.134287\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9623.498916625977, \"sum\": 9623.498916625977, \"min\": 9623.498916625977}}, \"EndTime\": 1612195523.727384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195514.103263}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.3567724 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=165, train loss <loss>=-2.81744404273\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:23 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:26 INFO 140079012972352] Epoch[166] Batch[0] avg_epoch_loss=-3.129236\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=166, batch=0 train loss <loss>=-3.12923645973\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:30 INFO 140079012972352] Epoch[166] Batch[5] avg_epoch_loss=-3.196798\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=166, batch=5 train loss <loss>=-3.19679792722\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:30 INFO 140079012972352] Epoch[166] Batch [5]#011Speed: 196.99 samples/sec#011loss=-3.196798\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:32 INFO 140079012972352] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8932.80291557312, \"sum\": 8932.80291557312, \"min\": 8932.80291557312}}, \"EndTime\": 1612195532.660779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195523.727473}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:32 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.482316307 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:32 INFO 140079012972352] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=166, train loss <loss>=-3.11762127876\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:32 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:35 INFO 140079012972352] Epoch[167] Batch[0] avg_epoch_loss=-2.823920\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=167, batch=0 train loss <loss>=-2.82392001152\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:39 INFO 140079012972352] Epoch[167] Batch[5] avg_epoch_loss=-2.988911\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=167, batch=5 train loss <loss>=-2.98891063531\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:39 INFO 140079012972352] Epoch[167] Batch [5]#011Speed: 188.92 samples/sec#011loss=-2.988911\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:41 INFO 140079012972352] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9072.994947433472, \"sum\": 9072.994947433472, \"min\": 9072.994947433472}}, \"EndTime\": 1612195541.734489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195532.660963}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:41 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.785899961 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:41 INFO 140079012972352] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=167, train loss <loss>=-3.0765696764\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:41 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:44 INFO 140079012972352] Epoch[168] Batch[0] avg_epoch_loss=-3.229751\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=168, batch=0 train loss <loss>=-3.22975063324\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:48 INFO 140079012972352] Epoch[168] Batch[5] avg_epoch_loss=-3.242894\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=168, batch=5 train loss <loss>=-3.24289425214\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:48 INFO 140079012972352] Epoch[168] Batch [5]#011Speed: 191.18 samples/sec#011loss=-3.242894\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:50 INFO 140079012972352] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8972.951889038086, \"sum\": 8972.951889038086, \"min\": 8972.951889038086}}, \"EndTime\": 1612195550.708172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195541.734562}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:50 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.751171861 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:50 INFO 140079012972352] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=168, train loss <loss>=-3.28000826836\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:50 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:53 INFO 140079012972352] Epoch[169] Batch[0] avg_epoch_loss=-3.161814\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=169, batch=0 train loss <loss>=-3.16181445122\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:57 INFO 140079012972352] Epoch[169] Batch[5] avg_epoch_loss=-3.259212\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=169, batch=5 train loss <loss>=-3.25921209653\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:05:57 INFO 140079012972352] Epoch[169] Batch [5]#011Speed: 193.38 samples/sec#011loss=-3.259212\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] Epoch[169] Batch[10] avg_epoch_loss=-3.309984\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=169, batch=10 train loss <loss>=-3.37091059685\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] Epoch[169] Batch [10]#011Speed: 197.73 samples/sec#011loss=-3.370911\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9677.052021026611, \"sum\": 9677.052021026611, \"min\": 9677.052021026611}}, \"EndTime\": 1612195560.385848, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195550.708268}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.36980398 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=169, train loss <loss>=-3.30998414213\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:00 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:03 INFO 140079012972352] Epoch[170] Batch[0] avg_epoch_loss=-1.746954\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=170, batch=0 train loss <loss>=-1.74695396423\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:07 INFO 140079012972352] Epoch[170] Batch[5] avg_epoch_loss=-2.141013\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=170, batch=5 train loss <loss>=-2.14101284742\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:07 INFO 140079012972352] Epoch[170] Batch [5]#011Speed: 167.84 samples/sec#011loss=-2.141013\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] Epoch[170] Batch[10] avg_epoch_loss=-2.329293\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=170, batch=10 train loss <loss>=-2.55522837639\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] Epoch[170] Batch [10]#011Speed: 196.27 samples/sec#011loss=-2.555228\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] processed a total of 1325 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10443.877935409546, \"sum\": 10443.877935409546, \"min\": 10443.877935409546}}, \"EndTime\": 1612195570.830335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195560.385948}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=126.867002189 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=170, train loss <loss>=-2.32929263332\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:10 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:13 INFO 140079012972352] Epoch[171] Batch[0] avg_epoch_loss=-2.160610\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=171, batch=0 train loss <loss>=-2.16060996056\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:17 INFO 140079012972352] Epoch[171] Batch[5] avg_epoch_loss=-2.370509\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=171, batch=5 train loss <loss>=-2.37050863107\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:17 INFO 140079012972352] Epoch[171] Batch [5]#011Speed: 197.10 samples/sec#011loss=-2.370509\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:19 INFO 140079012972352] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8925.362825393677, \"sum\": 8925.362825393677, \"min\": 8925.362825393677}}, \"EndTime\": 1612195579.756273, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195570.830423}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:19 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.961015782 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:19 INFO 140079012972352] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=171, train loss <loss>=-2.46077101231\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:19 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:22 INFO 140079012972352] Epoch[172] Batch[0] avg_epoch_loss=-2.641481\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=172, batch=0 train loss <loss>=-2.6414809227\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:26 INFO 140079012972352] Epoch[172] Batch[5] avg_epoch_loss=-2.730192\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=172, batch=5 train loss <loss>=-2.73019190629\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:26 INFO 140079012972352] Epoch[172] Batch [5]#011Speed: 193.29 samples/sec#011loss=-2.730192\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] Epoch[172] Batch[10] avg_epoch_loss=-2.805537\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=172, batch=10 train loss <loss>=-2.89595122337\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] Epoch[172] Batch [10]#011Speed: 198.09 samples/sec#011loss=-2.895951\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9548.166990280151, \"sum\": 9548.166990280151, \"min\": 9548.166990280151}}, \"EndTime\": 1612195589.305192, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195579.756373}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.349003085 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=172, train loss <loss>=-2.80553705042\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:29 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:32 INFO 140079012972352] Epoch[173] Batch[0] avg_epoch_loss=-3.152928\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=173, batch=0 train loss <loss>=-3.15292811394\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:35 INFO 140079012972352] Epoch[173] Batch[5] avg_epoch_loss=-2.984256\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=173, batch=5 train loss <loss>=-2.98425559203\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:35 INFO 140079012972352] Epoch[173] Batch [5]#011Speed: 198.17 samples/sec#011loss=-2.984256\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] Epoch[173] Batch[10] avg_epoch_loss=-3.119717\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=173, batch=10 train loss <loss>=-3.28226985931\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] Epoch[173] Batch [10]#011Speed: 196.48 samples/sec#011loss=-3.282270\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9505.773067474365, \"sum\": 9505.773067474365, \"min\": 9505.773067474365}}, \"EndTime\": 1612195598.811739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195589.305295}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.284217035 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=173, train loss <loss>=-3.11971662261\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:38 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:41 INFO 140079012972352] Epoch[174] Batch[0] avg_epoch_loss=-2.894168\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=174, batch=0 train loss <loss>=-2.89416766167\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:45 INFO 140079012972352] Epoch[174] Batch[5] avg_epoch_loss=-2.805215\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=174, batch=5 train loss <loss>=-2.80521512032\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:45 INFO 140079012972352] Epoch[174] Batch [5]#011Speed: 199.88 samples/sec#011loss=-2.805215\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] Epoch[174] Batch[10] avg_epoch_loss=-2.727055\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=174, batch=10 train loss <loss>=-2.6332627058\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] Epoch[174] Batch [10]#011Speed: 195.09 samples/sec#011loss=-2.633263\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9504.942178726196, \"sum\": 9504.942178726196, \"min\": 9504.942178726196}}, \"EndTime\": 1612195608.317388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195598.811836}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.401365284 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=174, train loss <loss>=-2.7270549319\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:48 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:51 INFO 140079012972352] Epoch[175] Batch[0] avg_epoch_loss=-2.137017\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=175, batch=0 train loss <loss>=-2.13701748848\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:54 INFO 140079012972352] Epoch[175] Batch[5] avg_epoch_loss=-2.349163\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=175, batch=5 train loss <loss>=-2.34916313489\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:54 INFO 140079012972352] Epoch[175] Batch [5]#011Speed: 198.11 samples/sec#011loss=-2.349163\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:57 INFO 140079012972352] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8861.936092376709, \"sum\": 8861.936092376709, \"min\": 8861.936092376709}}, \"EndTime\": 1612195617.180068, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195608.317474}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:57 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.614387136 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:57 INFO 140079012972352] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=175, train loss <loss>=-2.49274365902\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:06:57 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:00 INFO 140079012972352] Epoch[176] Batch[0] avg_epoch_loss=-2.527457\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=176, batch=0 train loss <loss>=-2.52745747566\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:03 INFO 140079012972352] Epoch[176] Batch[5] avg_epoch_loss=-2.723260\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=176, batch=5 train loss <loss>=-2.72326024373\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:03 INFO 140079012972352] Epoch[176] Batch [5]#011Speed: 184.50 samples/sec#011loss=-2.723260\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] Epoch[176] Batch[10] avg_epoch_loss=-2.809143\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=176, batch=10 train loss <loss>=-2.91220254898\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] Epoch[176] Batch [10]#011Speed: 155.74 samples/sec#011loss=-2.912203\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10556.129932403564, \"sum\": 10556.129932403564, \"min\": 10556.129932403564}}, \"EndTime\": 1612195627.737151, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195617.18018}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=122.296997974 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=176, train loss <loss>=-2.80914310976\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:07 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:10 INFO 140079012972352] Epoch[177] Batch[0] avg_epoch_loss=-2.879010\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=177, batch=0 train loss <loss>=-2.87900972366\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:14 INFO 140079012972352] Epoch[177] Batch[5] avg_epoch_loss=-2.959616\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=177, batch=5 train loss <loss>=-2.95961550872\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:14 INFO 140079012972352] Epoch[177] Batch [5]#011Speed: 194.87 samples/sec#011loss=-2.959616\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] Epoch[177] Batch[10] avg_epoch_loss=-3.106741\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=177, batch=10 train loss <loss>=-3.2832918644\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] Epoch[177] Batch [10]#011Speed: 195.40 samples/sec#011loss=-3.283292\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9582.29398727417, \"sum\": 9582.29398727417, \"min\": 9582.29398727417}}, \"EndTime\": 1612195637.320136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195627.73724}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.021565332 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=177, train loss <loss>=-3.10674112493\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:17 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:20 INFO 140079012972352] Epoch[178] Batch[0] avg_epoch_loss=-2.486328\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=178, batch=0 train loss <loss>=-2.48632788658\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:23 INFO 140079012972352] Epoch[178] Batch[5] avg_epoch_loss=-2.560725\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=178, batch=5 train loss <loss>=-2.56072545052\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:23 INFO 140079012972352] Epoch[178] Batch [5]#011Speed: 196.91 samples/sec#011loss=-2.560725\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] Epoch[178] Batch[10] avg_epoch_loss=-2.753520\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=178, batch=10 train loss <loss>=-2.98487300873\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] Epoch[178] Batch [10]#011Speed: 192.02 samples/sec#011loss=-2.984873\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9636.003017425537, \"sum\": 9636.003017425537, \"min\": 9636.003017425537}}, \"EndTime\": 1612195646.95672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195637.320231}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.569209019 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=178, train loss <loss>=-2.75351979516\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:26 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:30 INFO 140079012972352] Epoch[179] Batch[0] avg_epoch_loss=-2.782684\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=179, batch=0 train loss <loss>=-2.78268361092\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:33 INFO 140079012972352] Epoch[179] Batch[5] avg_epoch_loss=-2.894713\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=179, batch=5 train loss <loss>=-2.89471320311\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:33 INFO 140079012972352] Epoch[179] Batch [5]#011Speed: 196.79 samples/sec#011loss=-2.894713\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] Epoch[179] Batch[10] avg_epoch_loss=-3.065655\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=179, batch=10 train loss <loss>=-3.27078471184\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] Epoch[179] Batch [10]#011Speed: 196.93 samples/sec#011loss=-3.270785\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9576.833009719849, \"sum\": 9576.833009719849, \"min\": 9576.833009719849}}, \"EndTime\": 1612195656.53415, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195646.956814}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.070959977 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=179, train loss <loss>=-3.06565479799\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:36 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:39 INFO 140079012972352] Epoch[180] Batch[0] avg_epoch_loss=-2.902394\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=180, batch=0 train loss <loss>=-2.90239429474\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:42 INFO 140079012972352] Epoch[180] Batch[5] avg_epoch_loss=-3.055145\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=180, batch=5 train loss <loss>=-3.05514546235\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:42 INFO 140079012972352] Epoch[180] Batch [5]#011Speed: 197.71 samples/sec#011loss=-3.055145\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:45 INFO 140079012972352] processed a total of 1274 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8877.86602973938, \"sum\": 8877.86602973938, \"min\": 8877.86602973938}}, \"EndTime\": 1612195665.41266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195656.534295}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.500807395 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=180, train loss <loss>=-3.13635909557\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:48 INFO 140079012972352] Epoch[181] Batch[0] avg_epoch_loss=-3.310238\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=181, batch=0 train loss <loss>=-3.31023788452\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:51 INFO 140079012972352] Epoch[181] Batch[5] avg_epoch_loss=-3.174770\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=181, batch=5 train loss <loss>=-3.17476983865\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:51 INFO 140079012972352] Epoch[181] Batch [5]#011Speed: 196.99 samples/sec#011loss=-3.174770\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:54 INFO 140079012972352] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8876.972913742065, \"sum\": 8876.972913742065, \"min\": 8876.972913742065}}, \"EndTime\": 1612195674.290402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195665.412752}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.064674438 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=181, train loss <loss>=-3.04801559448\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:57 INFO 140079012972352] Epoch[182] Batch[0] avg_epoch_loss=-3.004682\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:07:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=182, batch=0 train loss <loss>=-3.00468230247\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:00 INFO 140079012972352] Epoch[182] Batch[5] avg_epoch_loss=-3.218021\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=182, batch=5 train loss <loss>=-3.21802135309\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:00 INFO 140079012972352] Epoch[182] Batch [5]#011Speed: 197.42 samples/sec#011loss=-3.218021\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] Epoch[182] Batch[10] avg_epoch_loss=-3.184266\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=182, batch=10 train loss <loss>=-3.14375991821\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] Epoch[182] Batch [10]#011Speed: 173.15 samples/sec#011loss=-3.143760\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10095.460891723633, \"sum\": 10095.460891723633, \"min\": 10095.460891723633}}, \"EndTime\": 1612195684.386606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195674.29049}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=128.967088356 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=182, train loss <loss>=-3.18426615542\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:07 INFO 140079012972352] Epoch[183] Batch[0] avg_epoch_loss=-2.320841\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=183, batch=0 train loss <loss>=-2.32084107399\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:10 INFO 140079012972352] Epoch[183] Batch[5] avg_epoch_loss=-2.809171\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=183, batch=5 train loss <loss>=-2.80917104085\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:10 INFO 140079012972352] Epoch[183] Batch [5]#011Speed: 198.23 samples/sec#011loss=-2.809171\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:13 INFO 140079012972352] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9203.521013259888, \"sum\": 9203.521013259888, \"min\": 9203.521013259888}}, \"EndTime\": 1612195693.59074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195684.386704}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.489397806 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=183, train loss <loss>=-2.93614778519\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:16 INFO 140079012972352] Epoch[184] Batch[0] avg_epoch_loss=-3.140280\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=184, batch=0 train loss <loss>=-3.14028024673\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:19 INFO 140079012972352] Epoch[184] Batch[5] avg_epoch_loss=-3.127272\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=184, batch=5 train loss <loss>=-3.127272288\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:19 INFO 140079012972352] Epoch[184] Batch [5]#011Speed: 194.46 samples/sec#011loss=-3.127272\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:22 INFO 140079012972352] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9052.780866622925, \"sum\": 9052.780866622925, \"min\": 9052.780866622925}}, \"EndTime\": 1612195702.644253, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195693.590843}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:22 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.62942669 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:22 INFO 140079012972352] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=184, train loss <loss>=-3.11340665817\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:22 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:25 INFO 140079012972352] Epoch[185] Batch[0] avg_epoch_loss=-3.211780\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=185, batch=0 train loss <loss>=-3.21177959442\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:29 INFO 140079012972352] Epoch[185] Batch[5] avg_epoch_loss=-3.349742\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=185, batch=5 train loss <loss>=-3.3497420152\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:29 INFO 140079012972352] Epoch[185] Batch [5]#011Speed: 192.48 samples/sec#011loss=-3.349742\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:31 INFO 140079012972352] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8968.115091323853, \"sum\": 8968.115091323853, \"min\": 8968.115091323853}}, \"EndTime\": 1612195711.61318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195702.644345}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.27929611 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=185, train loss <loss>=-3.33476068974\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:31 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:34 INFO 140079012972352] Epoch[186] Batch[0] avg_epoch_loss=-3.245759\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=186, batch=0 train loss <loss>=-3.2457587719\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:37 INFO 140079012972352] Epoch[186] Batch[5] avg_epoch_loss=-3.209816\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=186, batch=5 train loss <loss>=-3.20981574059\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:37 INFO 140079012972352] Epoch[186] Batch [5]#011Speed: 195.46 samples/sec#011loss=-3.209816\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:40 INFO 140079012972352] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8868.896007537842, \"sum\": 8868.896007537842, \"min\": 8868.896007537842}}, \"EndTime\": 1612195720.482892, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195711.613298}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=144.2090312 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=186, train loss <loss>=-3.21016087532\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:40 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:43 INFO 140079012972352] Epoch[187] Batch[0] avg_epoch_loss=-3.447523\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=187, batch=0 train loss <loss>=-3.44752311707\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:46 INFO 140079012972352] Epoch[187] Batch[5] avg_epoch_loss=-3.463665\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=187, batch=5 train loss <loss>=-3.46366548538\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:46 INFO 140079012972352] Epoch[187] Batch [5]#011Speed: 199.21 samples/sec#011loss=-3.463665\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] Epoch[187] Batch[10] avg_epoch_loss=-3.350413\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=187, batch=10 train loss <loss>=-3.21451096535\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] Epoch[187] Batch [10]#011Speed: 197.21 samples/sec#011loss=-3.214511\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9447.547912597656, \"sum\": 9447.547912597656, \"min\": 9447.547912597656}}, \"EndTime\": 1612195729.931218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195720.483019}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.599595826 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=187, train loss <loss>=-3.35041343082\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:49 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:52 INFO 140079012972352] Epoch[188] Batch[0] avg_epoch_loss=-3.153999\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=188, batch=0 train loss <loss>=-3.15399885178\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:56 INFO 140079012972352] Epoch[188] Batch[5] avg_epoch_loss=-2.739743\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=188, batch=5 train loss <loss>=-2.73974311352\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:56 INFO 140079012972352] Epoch[188] Batch [5]#011Speed: 199.77 samples/sec#011loss=-2.739743\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:58 INFO 140079012972352] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8784.244060516357, \"sum\": 8784.244060516357, \"min\": 8784.244060516357}}, \"EndTime\": 1612195738.716178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195729.931325}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:58 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.590007545 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:58 INFO 140079012972352] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=188, train loss <loss>=-2.69619512558\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:08:58 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:01 INFO 140079012972352] Epoch[189] Batch[0] avg_epoch_loss=-2.700077\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=189, batch=0 train loss <loss>=-2.70007658005\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:05 INFO 140079012972352] Epoch[189] Batch[5] avg_epoch_loss=-2.926961\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=189, batch=5 train loss <loss>=-2.92696134249\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:05 INFO 140079012972352] Epoch[189] Batch [5]#011Speed: 159.26 samples/sec#011loss=-2.926961\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] Epoch[189] Batch[10] avg_epoch_loss=-2.781592\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=189, batch=10 train loss <loss>=-2.60714979172\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] Epoch[189] Batch [10]#011Speed: 197.77 samples/sec#011loss=-2.607150\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10475.770950317383, \"sum\": 10475.770950317383, \"min\": 10475.770950317383}}, \"EndTime\": 1612195749.193266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195738.716295}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=122.471436231 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=189, train loss <loss>=-2.78159245578\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:09 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:12 INFO 140079012972352] Epoch[190] Batch[0] avg_epoch_loss=-1.756841\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=190, batch=0 train loss <loss>=-1.75684142113\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:15 INFO 140079012972352] Epoch[190] Batch[5] avg_epoch_loss=-2.485806\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=190, batch=5 train loss <loss>=-2.4858058691\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:15 INFO 140079012972352] Epoch[190] Batch [5]#011Speed: 199.14 samples/sec#011loss=-2.485806\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:18 INFO 140079012972352] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8844.101905822754, \"sum\": 8844.101905822754, \"min\": 8844.101905822754}}, \"EndTime\": 1612195758.037953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195749.193364}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:18 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.709170805 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:18 INFO 140079012972352] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=190, train loss <loss>=-2.65471725464\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:18 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:21 INFO 140079012972352] Epoch[191] Batch[0] avg_epoch_loss=-3.007298\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=191, batch=0 train loss <loss>=-3.00729751587\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:24 INFO 140079012972352] Epoch[191] Batch[5] avg_epoch_loss=-2.976467\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=191, batch=5 train loss <loss>=-2.9764674902\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:24 INFO 140079012972352] Epoch[191] Batch [5]#011Speed: 198.60 samples/sec#011loss=-2.976467\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] Epoch[191] Batch[10] avg_epoch_loss=-3.056003\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=191, batch=10 train loss <loss>=-3.15144658089\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] Epoch[191] Batch [10]#011Speed: 195.53 samples/sec#011loss=-3.151447\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9568.873167037964, \"sum\": 9568.873167037964, \"min\": 9568.873167037964}}, \"EndTime\": 1612195767.607492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195758.038056}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.70566641 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=191, train loss <loss>=-3.05600344051\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:30 INFO 140079012972352] Epoch[192] Batch[0] avg_epoch_loss=-2.478160\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=192, batch=0 train loss <loss>=-2.47815966606\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:33 INFO 140079012972352] Epoch[192] Batch[5] avg_epoch_loss=-2.597169\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=192, batch=5 train loss <loss>=-2.59716864427\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:33 INFO 140079012972352] Epoch[192] Batch [5]#011Speed: 199.02 samples/sec#011loss=-2.597169\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:36 INFO 140079012972352] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8824.17607307434, \"sum\": 8824.17607307434, \"min\": 8824.17607307434}}, \"EndTime\": 1612195776.432242, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195767.607587}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:36 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.013723851 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:36 INFO 140079012972352] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=192, train loss <loss>=-2.74983739853\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:36 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:39 INFO 140079012972352] Epoch[193] Batch[0] avg_epoch_loss=-3.084839\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=193, batch=0 train loss <loss>=-3.08483910561\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:42 INFO 140079012972352] Epoch[193] Batch[5] avg_epoch_loss=-3.056671\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=193, batch=5 train loss <loss>=-3.05667130152\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:42 INFO 140079012972352] Epoch[193] Batch [5]#011Speed: 198.06 samples/sec#011loss=-3.056671\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] Epoch[193] Batch[10] avg_epoch_loss=-3.202429\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=193, batch=10 train loss <loss>=-3.37733831406\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] Epoch[193] Batch [10]#011Speed: 195.39 samples/sec#011loss=-3.377338\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9543.24984550476, \"sum\": 9543.24984550476, \"min\": 9543.24984550476}}, \"EndTime\": 1612195785.976288, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195776.432347}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.525215903 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=193, train loss <loss>=-3.20242903449\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:49 INFO 140079012972352] Epoch[194] Batch[0] avg_epoch_loss=-3.041122\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=194, batch=0 train loss <loss>=-3.04112195969\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:52 INFO 140079012972352] Epoch[194] Batch[5] avg_epoch_loss=-2.995740\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=194, batch=5 train loss <loss>=-2.99574005604\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:52 INFO 140079012972352] Epoch[194] Batch [5]#011Speed: 199.24 samples/sec#011loss=-2.995740\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:54 INFO 140079012972352] processed a total of 1164 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8867.216110229492, \"sum\": 8867.216110229492, \"min\": 8867.216110229492}}, \"EndTime\": 1612195794.84419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195785.976382}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.267191579 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=194, train loss <loss>=-2.71853953004\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:57 INFO 140079012972352] Epoch[195] Batch[0] avg_epoch_loss=-2.790451\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:09:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=195, batch=0 train loss <loss>=-2.79045128822\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:01 INFO 140079012972352] Epoch[195] Batch[5] avg_epoch_loss=-2.978798\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=195, batch=5 train loss <loss>=-2.97879779339\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:01 INFO 140079012972352] Epoch[195] Batch [5]#011Speed: 197.74 samples/sec#011loss=-2.978798\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:04 INFO 140079012972352] processed a total of 1208 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9184.848070144653, \"sum\": 9184.848070144653, \"min\": 9184.848070144653}}, \"EndTime\": 1612195804.029938, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195794.844333}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.518122564 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=195, train loss <loss>=-3.08424196243\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:07 INFO 140079012972352] Epoch[196] Batch[0] avg_epoch_loss=-3.054545\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=196, batch=0 train loss <loss>=-3.05454516411\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:10 INFO 140079012972352] Epoch[196] Batch[5] avg_epoch_loss=-3.203820\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=196, batch=5 train loss <loss>=-3.20382002989\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:10 INFO 140079012972352] Epoch[196] Batch [5]#011Speed: 196.57 samples/sec#011loss=-3.203820\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:13 INFO 140079012972352] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9316.335916519165, \"sum\": 9316.335916519165, \"min\": 9316.335916519165}}, \"EndTime\": 1612195813.348197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195804.030089}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.56172407 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=196, train loss <loss>=-3.19671015739\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:16 INFO 140079012972352] Epoch[197] Batch[0] avg_epoch_loss=-3.283785\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=197, batch=0 train loss <loss>=-3.28378534317\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:19 INFO 140079012972352] Epoch[197] Batch[5] avg_epoch_loss=-3.278498\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=197, batch=5 train loss <loss>=-3.27849833171\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:19 INFO 140079012972352] Epoch[197] Batch [5]#011Speed: 195.78 samples/sec#011loss=-3.278498\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:22 INFO 140079012972352] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8861.053943634033, \"sum\": 8861.053943634033, \"min\": 8861.053943634033}}, \"EndTime\": 1612195822.209888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195813.34829}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:22 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.659952992 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:22 INFO 140079012972352] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=197, train loss <loss>=-3.34934914112\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:22 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:25 INFO 140079012972352] Epoch[198] Batch[0] avg_epoch_loss=-3.310590\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=198, batch=0 train loss <loss>=-3.31058979034\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:28 INFO 140079012972352] Epoch[198] Batch[5] avg_epoch_loss=-3.431939\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=198, batch=5 train loss <loss>=-3.43193876743\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:28 INFO 140079012972352] Epoch[198] Batch [5]#011Speed: 194.31 samples/sec#011loss=-3.431939\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] Epoch[198] Batch[10] avg_epoch_loss=-3.477553\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=198, batch=10 train loss <loss>=-3.53229107857\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] Epoch[198] Batch [10]#011Speed: 195.31 samples/sec#011loss=-3.532291\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9571.035146713257, \"sum\": 9571.035146713257, \"min\": 9571.035146713257}}, \"EndTime\": 1612195831.781667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195822.209991}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.779856542 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=198, train loss <loss>=-3.47755345431\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:31 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_d24f5e22-7f5c-4226-9ef1-91658b2adb20-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.025047302246094, \"sum\": 36.025047302246094, \"min\": 36.025047302246094}}, \"EndTime\": 1612195831.818707, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195831.781747}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:34 INFO 140079012972352] Epoch[199] Batch[0] avg_epoch_loss=-2.031826\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=199, batch=0 train loss <loss>=-2.03182649612\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:38 INFO 140079012972352] Epoch[199] Batch[5] avg_epoch_loss=-1.888031\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=199, batch=5 train loss <loss>=-1.88803054889\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:38 INFO 140079012972352] Epoch[199] Batch [5]#011Speed: 198.91 samples/sec#011loss=-1.888031\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:40 INFO 140079012972352] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8831.568002700806, \"sum\": 8831.568002700806, \"min\": 8831.568002700806}}, \"EndTime\": 1612195840.650699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195831.819062}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.082658909 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=199, train loss <loss>=-2.08473087549\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:40 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:43 INFO 140079012972352] Epoch[200] Batch[0] avg_epoch_loss=-2.638505\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=200, batch=0 train loss <loss>=-2.63850545883\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:46 INFO 140079012972352] Epoch[200] Batch[5] avg_epoch_loss=-2.560059\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=200, batch=5 train loss <loss>=-2.56005859375\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:46 INFO 140079012972352] Epoch[200] Batch [5]#011Speed: 198.46 samples/sec#011loss=-2.560059\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] Epoch[200] Batch[10] avg_epoch_loss=-2.555601\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=200, batch=10 train loss <loss>=-2.55025296211\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] Epoch[200] Batch [10]#011Speed: 196.89 samples/sec#011loss=-2.550253\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9567.132234573364, \"sum\": 9567.132234573364, \"min\": 9567.132234573364}}, \"EndTime\": 1612195850.218725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195840.65079}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.597416392 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=200, train loss <loss>=-2.55560148846\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:50 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:53 INFO 140079012972352] Epoch[201] Batch[0] avg_epoch_loss=-2.753137\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=201, batch=0 train loss <loss>=-2.75313687325\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:56 INFO 140079012972352] Epoch[201] Batch[5] avg_epoch_loss=-2.766823\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=201, batch=5 train loss <loss>=-2.76682253679\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:56 INFO 140079012972352] Epoch[201] Batch [5]#011Speed: 196.61 samples/sec#011loss=-2.766823\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:59 INFO 140079012972352] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8885.137796401978, \"sum\": 8885.137796401978, \"min\": 8885.137796401978}}, \"EndTime\": 1612195859.104577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195850.218826}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:59 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.357677268 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:59 INFO 140079012972352] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=201, train loss <loss>=-2.81206030846\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:10:59 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:02 INFO 140079012972352] Epoch[202] Batch[0] avg_epoch_loss=-3.124592\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=202, batch=0 train loss <loss>=-3.12459230423\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:06 INFO 140079012972352] Epoch[202] Batch[5] avg_epoch_loss=-2.642060\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=202, batch=5 train loss <loss>=-2.64205956459\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:06 INFO 140079012972352] Epoch[202] Batch [5]#011Speed: 156.15 samples/sec#011loss=-2.642060\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:09 INFO 140079012972352] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9937.856912612915, \"sum\": 9937.856912612915, \"min\": 9937.856912612915}}, \"EndTime\": 1612195869.043568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195859.104657}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:09 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=126.886658414 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:09 INFO 140079012972352] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=202, train loss <loss>=-2.77398455143\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:09 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:12 INFO 140079012972352] Epoch[203] Batch[0] avg_epoch_loss=-2.918931\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=203, batch=0 train loss <loss>=-2.91893053055\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:15 INFO 140079012972352] Epoch[203] Batch[5] avg_epoch_loss=-3.085119\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=203, batch=5 train loss <loss>=-3.08511881034\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:15 INFO 140079012972352] Epoch[203] Batch [5]#011Speed: 196.53 samples/sec#011loss=-3.085119\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:18 INFO 140079012972352] processed a total of 1228 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9191.58387184143, \"sum\": 9191.58387184143, \"min\": 9191.58387184143}}, \"EndTime\": 1612195878.236219, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195869.043664}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:18 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.59849939 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:18 INFO 140079012972352] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=203, train loss <loss>=-3.11170423031\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:18 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:21 INFO 140079012972352] Epoch[204] Batch[0] avg_epoch_loss=-3.038690\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=204, batch=0 train loss <loss>=-3.03868961334\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:24 INFO 140079012972352] Epoch[204] Batch[5] avg_epoch_loss=-3.154766\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=204, batch=5 train loss <loss>=-3.15476560593\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:24 INFO 140079012972352] Epoch[204] Batch [5]#011Speed: 196.88 samples/sec#011loss=-3.154766\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] Epoch[204] Batch[10] avg_epoch_loss=-3.127513\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=204, batch=10 train loss <loss>=-3.09481024742\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] Epoch[204] Batch [10]#011Speed: 193.56 samples/sec#011loss=-3.094810\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9636.423110961914, \"sum\": 9636.423110961914, \"min\": 9636.423110961914}}, \"EndTime\": 1612195887.873384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195878.236312}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.525540751 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=204, train loss <loss>=-3.12751317024\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:30 INFO 140079012972352] Epoch[205] Batch[0] avg_epoch_loss=-2.270780\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=205, batch=0 train loss <loss>=-2.2707798481\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:34 INFO 140079012972352] Epoch[205] Batch[5] avg_epoch_loss=-1.752149\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=205, batch=5 train loss <loss>=-1.75214852889\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:34 INFO 140079012972352] Epoch[205] Batch [5]#011Speed: 199.19 samples/sec#011loss=-1.752149\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] Epoch[205] Batch[10] avg_epoch_loss=-1.948936\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=205, batch=10 train loss <loss>=-2.18508045673\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] Epoch[205] Batch [10]#011Speed: 196.48 samples/sec#011loss=-2.185080\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9575.51097869873, \"sum\": 9575.51097869873, \"min\": 9575.51097869873}}, \"EndTime\": 1612195897.449479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195887.873477}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.820880066 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=205, train loss <loss>=-1.94893576882\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:37 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:40 INFO 140079012972352] Epoch[206] Batch[0] avg_epoch_loss=-2.421667\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=206, batch=0 train loss <loss>=-2.42166733742\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:43 INFO 140079012972352] Epoch[206] Batch[5] avg_epoch_loss=-2.364612\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=206, batch=5 train loss <loss>=-2.36461218198\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:43 INFO 140079012972352] Epoch[206] Batch [5]#011Speed: 197.96 samples/sec#011loss=-2.364612\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:46 INFO 140079012972352] processed a total of 1198 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8919.75712776184, \"sum\": 8919.75712776184, \"min\": 8919.75712776184}}, \"EndTime\": 1612195906.370165, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195897.449569}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:46 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.306245457 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:46 INFO 140079012972352] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=206, train loss <loss>=-2.46072328091\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:46 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:49 INFO 140079012972352] Epoch[207] Batch[0] avg_epoch_loss=-2.478965\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=207, batch=0 train loss <loss>=-2.47896528244\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:52 INFO 140079012972352] Epoch[207] Batch[5] avg_epoch_loss=-2.628225\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=207, batch=5 train loss <loss>=-2.62822461128\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:52 INFO 140079012972352] Epoch[207] Batch [5]#011Speed: 195.04 samples/sec#011loss=-2.628225\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:55 INFO 140079012972352] processed a total of 1222 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8925.997972488403, \"sum\": 8925.997972488403, \"min\": 8925.997972488403}}, \"EndTime\": 1612195915.297003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195906.370271}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:55 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.901250631 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:55 INFO 140079012972352] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=207, train loss <loss>=-2.66079354286\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:55 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:58 INFO 140079012972352] Epoch[208] Batch[0] avg_epoch_loss=-2.855382\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:11:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=208, batch=0 train loss <loss>=-2.85538244247\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:01 INFO 140079012972352] Epoch[208] Batch[5] avg_epoch_loss=-2.895704\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=208, batch=5 train loss <loss>=-2.89570355415\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:01 INFO 140079012972352] Epoch[208] Batch [5]#011Speed: 184.47 samples/sec#011loss=-2.895704\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] Epoch[208] Batch[10] avg_epoch_loss=-2.977589\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=208, batch=10 train loss <loss>=-3.07585110664\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] Epoch[208] Batch [10]#011Speed: 167.81 samples/sec#011loss=-3.075851\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10348.085880279541, \"sum\": 10348.085880279541, \"min\": 10348.085880279541}}, \"EndTime\": 1612195925.645704, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195915.297102}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=125.721912355 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=208, train loss <loss>=-2.97758880529\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:05 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:08 INFO 140079012972352] Epoch[209] Batch[0] avg_epoch_loss=-1.851715\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=209, batch=0 train loss <loss>=-1.8517152071\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:12 INFO 140079012972352] Epoch[209] Batch[5] avg_epoch_loss=-2.252237\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=209, batch=5 train loss <loss>=-2.25223731995\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:12 INFO 140079012972352] Epoch[209] Batch [5]#011Speed: 194.31 samples/sec#011loss=-2.252237\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:14 INFO 140079012972352] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9085.448026657104, \"sum\": 9085.448026657104, \"min\": 9085.448026657104}}, \"EndTime\": 1612195934.734234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195925.645807}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:14 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.882273113 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:14 INFO 140079012972352] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=209, train loss <loss>=-2.45564517975\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:14 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:17 INFO 140079012972352] Epoch[210] Batch[0] avg_epoch_loss=-2.809561\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=210, batch=0 train loss <loss>=-2.80956101418\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:21 INFO 140079012972352] Epoch[210] Batch[5] avg_epoch_loss=-2.821031\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=210, batch=5 train loss <loss>=-2.82103113333\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:21 INFO 140079012972352] Epoch[210] Batch [5]#011Speed: 196.67 samples/sec#011loss=-2.821031\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] Epoch[210] Batch[10] avg_epoch_loss=-2.923202\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=210, batch=10 train loss <loss>=-3.04580621719\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] Epoch[210] Batch [10]#011Speed: 197.43 samples/sec#011loss=-3.045806\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9571.628093719482, \"sum\": 9571.628093719482, \"min\": 9571.628093719482}}, \"EndTime\": 1612195944.306671, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195934.734335}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.875984654 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=210, train loss <loss>=-2.923201626\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:24 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:27 INFO 140079012972352] Epoch[211] Batch[0] avg_epoch_loss=-3.053001\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=211, batch=0 train loss <loss>=-3.05300116539\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:30 INFO 140079012972352] Epoch[211] Batch[5] avg_epoch_loss=-3.074501\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=211, batch=5 train loss <loss>=-3.07450079918\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:30 INFO 140079012972352] Epoch[211] Batch [5]#011Speed: 194.90 samples/sec#011loss=-3.074501\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] Epoch[211] Batch[10] avg_epoch_loss=-3.168228\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=211, batch=10 train loss <loss>=-3.28070135117\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] Epoch[211] Batch [10]#011Speed: 196.95 samples/sec#011loss=-3.280701\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9618.757963180542, \"sum\": 9618.757963180542, \"min\": 9618.757963180542}}, \"EndTime\": 1612195953.926085, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195944.306757}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.0215643 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=211, train loss <loss>=-3.16822832281\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:33 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:36 INFO 140079012972352] Epoch[212] Batch[0] avg_epoch_loss=-2.989707\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=212, batch=0 train loss <loss>=-2.98970723152\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:40 INFO 140079012972352] Epoch[212] Batch[5] avg_epoch_loss=-3.152919\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=212, batch=5 train loss <loss>=-3.15291885535\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:40 INFO 140079012972352] Epoch[212] Batch [5]#011Speed: 197.49 samples/sec#011loss=-3.152919\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] Epoch[212] Batch[10] avg_epoch_loss=-3.116045\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=212, batch=10 train loss <loss>=-3.07179532051\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] Epoch[212] Batch [10]#011Speed: 196.26 samples/sec#011loss=-3.071795\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9575.906991958618, \"sum\": 9575.906991958618, \"min\": 9575.906991958618}}, \"EndTime\": 1612195963.502842, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195953.926181}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.990500123 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=212, train loss <loss>=-3.11604452133\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:46 INFO 140079012972352] Epoch[213] Batch[0] avg_epoch_loss=-2.817078\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=213, batch=0 train loss <loss>=-2.81707811356\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:49 INFO 140079012972352] Epoch[213] Batch[5] avg_epoch_loss=-2.971038\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=213, batch=5 train loss <loss>=-2.97103846073\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:49 INFO 140079012972352] Epoch[213] Batch [5]#011Speed: 198.77 samples/sec#011loss=-2.971038\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:52 INFO 140079012972352] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8960.062980651855, \"sum\": 8960.062980651855, \"min\": 8960.062980651855}}, \"EndTime\": 1612195972.463799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195963.503052}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:52 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.064041726 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:52 INFO 140079012972352] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=213, train loss <loss>=-3.04424991608\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:52 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:55 INFO 140079012972352] Epoch[214] Batch[0] avg_epoch_loss=-3.103872\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=214, batch=0 train loss <loss>=-3.10387182236\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:58 INFO 140079012972352] Epoch[214] Batch[5] avg_epoch_loss=-3.087666\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=214, batch=5 train loss <loss>=-3.08766619364\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:12:58 INFO 140079012972352] Epoch[214] Batch [5]#011Speed: 193.11 samples/sec#011loss=-3.087666\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:01 INFO 140079012972352] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8986.186027526855, \"sum\": 8986.186027526855, \"min\": 8986.186027526855}}, \"EndTime\": 1612195981.450668, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195972.463875}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:01 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.432393469 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:01 INFO 140079012972352] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=214, train loss <loss>=-3.13194389343\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:01 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:04 INFO 140079012972352] Epoch[215] Batch[0] avg_epoch_loss=-3.111033\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=215, batch=0 train loss <loss>=-3.11103272438\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:08 INFO 140079012972352] Epoch[215] Batch[5] avg_epoch_loss=-3.369108\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=215, batch=5 train loss <loss>=-3.36910792192\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:08 INFO 140079012972352] Epoch[215] Batch [5]#011Speed: 179.71 samples/sec#011loss=-3.369108\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] Epoch[215] Batch[10] avg_epoch_loss=-3.392031\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=215, batch=10 train loss <loss>=-3.41953883171\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] Epoch[215] Batch [10]#011Speed: 195.80 samples/sec#011loss=-3.419539\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10347.412109375, \"sum\": 10347.412109375, \"min\": 10347.412109375}}, \"EndTime\": 1612195991.798774, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195981.45077}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=127.180026043 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=215, train loss <loss>=-3.39203106273\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:11 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:14 INFO 140079012972352] Epoch[216] Batch[0] avg_epoch_loss=-2.990590\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=216, batch=0 train loss <loss>=-2.99059009552\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:18 INFO 140079012972352] Epoch[216] Batch[5] avg_epoch_loss=-2.895783\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=216, batch=5 train loss <loss>=-2.89578262965\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:18 INFO 140079012972352] Epoch[216] Batch [5]#011Speed: 196.05 samples/sec#011loss=-2.895783\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:20 INFO 140079012972352] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8876.068830490112, \"sum\": 8876.068830490112, \"min\": 8876.068830490112}}, \"EndTime\": 1612196000.675488, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612195991.798857}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:20 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.486888832 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:20 INFO 140079012972352] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=216, train loss <loss>=-2.92401235104\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:20 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:23 INFO 140079012972352] Epoch[217] Batch[0] avg_epoch_loss=-2.728353\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=217, batch=0 train loss <loss>=-2.72835326195\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:27 INFO 140079012972352] Epoch[217] Batch[5] avg_epoch_loss=-3.022752\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=217, batch=5 train loss <loss>=-3.02275180817\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:27 INFO 140079012972352] Epoch[217] Batch [5]#011Speed: 195.66 samples/sec#011loss=-3.022752\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] Epoch[217] Batch[10] avg_epoch_loss=-3.173634\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=217, batch=10 train loss <loss>=-3.35469274521\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] Epoch[217] Batch [10]#011Speed: 194.20 samples/sec#011loss=-3.354693\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9633.877992630005, \"sum\": 9633.877992630005, \"min\": 9633.877992630005}}, \"EndTime\": 1612196010.310063, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196000.675648}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.042222529 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=217, train loss <loss>=-3.17363405228\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:30 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:33 INFO 140079012972352] Epoch[218] Batch[0] avg_epoch_loss=-3.214025\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=218, batch=0 train loss <loss>=-3.21402478218\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:36 INFO 140079012972352] Epoch[218] Batch[5] avg_epoch_loss=-3.261854\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=218, batch=5 train loss <loss>=-3.26185361544\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:36 INFO 140079012972352] Epoch[218] Batch [5]#011Speed: 197.48 samples/sec#011loss=-3.261854\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:39 INFO 140079012972352] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8889.965057373047, \"sum\": 8889.965057373047, \"min\": 8889.965057373047}}, \"EndTime\": 1612196019.200748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196010.310161}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:39 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.168027275 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:39 INFO 140079012972352] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=218, train loss <loss>=-3.28600900173\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:39 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:42 INFO 140079012972352] Epoch[219] Batch[0] avg_epoch_loss=-3.365245\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=219, batch=0 train loss <loss>=-3.36524510384\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:45 INFO 140079012972352] Epoch[219] Batch[5] avg_epoch_loss=-3.417064\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=219, batch=5 train loss <loss>=-3.41706438859\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:45 INFO 140079012972352] Epoch[219] Batch [5]#011Speed: 198.41 samples/sec#011loss=-3.417064\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:48 INFO 140079012972352] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8917.321920394897, \"sum\": 8917.321920394897, \"min\": 8917.321920394897}}, \"EndTime\": 1612196028.118725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196019.200852}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:48 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.314398391 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:48 INFO 140079012972352] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=219, train loss <loss>=-3.39078986645\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:48 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:51 INFO 140079012972352] Epoch[220] Batch[0] avg_epoch_loss=-3.357403\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=220, batch=0 train loss <loss>=-3.3574025631\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:54 INFO 140079012972352] Epoch[220] Batch[5] avg_epoch_loss=-3.376094\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=220, batch=5 train loss <loss>=-3.37609430154\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:54 INFO 140079012972352] Epoch[220] Batch [5]#011Speed: 198.21 samples/sec#011loss=-3.376094\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] Epoch[220] Batch[10] avg_epoch_loss=-3.472986\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=220, batch=10 train loss <loss>=-3.58925628662\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] Epoch[220] Batch [10]#011Speed: 197.46 samples/sec#011loss=-3.589256\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] processed a total of 1344 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9569.170951843262, \"sum\": 9569.170951843262, \"min\": 9569.170951843262}}, \"EndTime\": 1612196037.688571, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196028.118817}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.448956635 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=220, train loss <loss>=-3.47298611294\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:13:57 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:00 INFO 140079012972352] Epoch[221] Batch[0] avg_epoch_loss=-3.212983\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=221, batch=0 train loss <loss>=-3.21298289299\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:04 INFO 140079012972352] Epoch[221] Batch[5] avg_epoch_loss=-3.364163\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=221, batch=5 train loss <loss>=-3.3641628027\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:04 INFO 140079012972352] Epoch[221] Batch [5]#011Speed: 188.51 samples/sec#011loss=-3.364163\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] Epoch[221] Batch[10] avg_epoch_loss=-3.245593\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=221, batch=10 train loss <loss>=-3.10330986977\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] Epoch[221] Batch [10]#011Speed: 198.03 samples/sec#011loss=-3.103310\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] processed a total of 1356 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9672.677040100098, \"sum\": 9672.677040100098, \"min\": 9672.677040100098}}, \"EndTime\": 1612196047.36191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196037.688673}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.186410476 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=221, train loss <loss>=-3.24559328773\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:07 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:10 INFO 140079012972352] Epoch[222] Batch[0] avg_epoch_loss=-3.356709\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=222, batch=0 train loss <loss>=-3.35670924187\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:14 INFO 140079012972352] Epoch[222] Batch[5] avg_epoch_loss=-3.536961\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=222, batch=5 train loss <loss>=-3.53696068128\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:14 INFO 140079012972352] Epoch[222] Batch [5]#011Speed: 198.80 samples/sec#011loss=-3.536961\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:16 INFO 140079012972352] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9399.519920349121, \"sum\": 9399.519920349121, \"min\": 9399.519920349121}}, \"EndTime\": 1612196056.762054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196047.362026}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:16 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.834545016 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:16 INFO 140079012972352] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=222, train loss <loss>=-3.49963994026\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:16 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:16 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_239ea598-6049-400e-b25a-a73e41ae070c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 54.64005470275879, \"sum\": 54.64005470275879, \"min\": 54.64005470275879}}, \"EndTime\": 1612196056.817406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196056.762156}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:19 INFO 140079012972352] Epoch[223] Batch[0] avg_epoch_loss=-3.467254\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=223, batch=0 train loss <loss>=-3.467253685\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:23 INFO 140079012972352] Epoch[223] Batch[5] avg_epoch_loss=-3.437566\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=223, batch=5 train loss <loss>=-3.43756604195\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:23 INFO 140079012972352] Epoch[223] Batch [5]#011Speed: 199.19 samples/sec#011loss=-3.437566\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] Epoch[223] Batch[10] avg_epoch_loss=-3.571494\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=223, batch=10 train loss <loss>=-3.73220691681\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] Epoch[223] Batch [10]#011Speed: 197.22 samples/sec#011loss=-3.732207\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9473.572969436646, \"sum\": 9473.572969436646, \"min\": 9473.572969436646}}, \"EndTime\": 1612196066.291142, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196056.817494}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.955215386 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=223, train loss <loss>=-3.57149371234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:26 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/state_c8f8e405-02b8-47b6-b8de-8dd80d13036a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.39099884033203, \"sum\": 33.39099884033203, \"min\": 33.39099884033203}}, \"EndTime\": 1612196066.325237, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196066.291233}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:29 INFO 140079012972352] Epoch[224] Batch[0] avg_epoch_loss=-1.178907\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=224, batch=0 train loss <loss>=-1.1789072752\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:32 INFO 140079012972352] Epoch[224] Batch[5] avg_epoch_loss=-1.622057\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=224, batch=5 train loss <loss>=-1.62205727895\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:32 INFO 140079012972352] Epoch[224] Batch [5]#011Speed: 198.05 samples/sec#011loss=-1.622057\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] Epoch[224] Batch[10] avg_epoch_loss=-1.923186\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=224, batch=10 train loss <loss>=-2.28454091549\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] Epoch[224] Batch [10]#011Speed: 197.62 samples/sec#011loss=-2.284541\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] processed a total of 1324 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9499.238967895508, \"sum\": 9499.238967895508, \"min\": 9499.238967895508}}, \"EndTime\": 1612196075.824606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196066.325303}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.377840959 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=224, train loss <loss>=-1.92318620465\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:35 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:38 INFO 140079012972352] Epoch[225] Batch[0] avg_epoch_loss=-2.275996\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=225, batch=0 train loss <loss>=-2.27599596977\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:42 INFO 140079012972352] Epoch[225] Batch[5] avg_epoch_loss=-2.350623\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=225, batch=5 train loss <loss>=-2.35062344869\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:42 INFO 140079012972352] Epoch[225] Batch [5]#011Speed: 198.97 samples/sec#011loss=-2.350623\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] Epoch[225] Batch[10] avg_epoch_loss=-2.398702\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=225, batch=10 train loss <loss>=-2.45639624596\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] Epoch[225] Batch [10]#011Speed: 197.82 samples/sec#011loss=-2.456396\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9518.823146820068, \"sum\": 9518.823146820068, \"min\": 9518.823146820068}}, \"EndTime\": 1612196085.343988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196075.824687}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.518948588 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=225, train loss <loss>=-2.3987019929\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:48 INFO 140079012972352] Epoch[226] Batch[0] avg_epoch_loss=-2.590251\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=226, batch=0 train loss <loss>=-2.59025073051\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:51 INFO 140079012972352] Epoch[226] Batch[5] avg_epoch_loss=-2.718428\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=226, batch=5 train loss <loss>=-2.71842813492\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:51 INFO 140079012972352] Epoch[226] Batch [5]#011Speed: 198.38 samples/sec#011loss=-2.718428\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:54 INFO 140079012972352] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8863.565921783447, \"sum\": 8863.565921783447, \"min\": 8863.565921783447}}, \"EndTime\": 1612196094.208318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196085.344087}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.265681109 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=226, train loss <loss>=-2.85918924809\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:57 INFO 140079012972352] Epoch[227] Batch[0] avg_epoch_loss=-3.330656\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:14:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=227, batch=0 train loss <loss>=-3.3306555748\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:00 INFO 140079012972352] Epoch[227] Batch[5] avg_epoch_loss=-3.265855\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=227, batch=5 train loss <loss>=-3.26585507393\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:00 INFO 140079012972352] Epoch[227] Batch [5]#011Speed: 194.13 samples/sec#011loss=-3.265855\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] Epoch[227] Batch[10] avg_epoch_loss=-3.338186\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=227, batch=10 train loss <loss>=-3.42498402596\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] Epoch[227] Batch [10]#011Speed: 163.67 samples/sec#011loss=-3.424984\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] processed a total of 1298 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10367.357969284058, \"sum\": 10367.357969284058, \"min\": 10367.357969284058}}, \"EndTime\": 1612196104.576362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196094.208407}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=125.199082144 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=227, train loss <loss>=-3.33818641576\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:07 INFO 140079012972352] Epoch[228] Batch[0] avg_epoch_loss=-2.799805\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=228, batch=0 train loss <loss>=-2.79980516434\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:11 INFO 140079012972352] Epoch[228] Batch[5] avg_epoch_loss=-2.615395\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=228, batch=5 train loss <loss>=-2.61539538701\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:11 INFO 140079012972352] Epoch[228] Batch [5]#011Speed: 198.82 samples/sec#011loss=-2.615395\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:13 INFO 140079012972352] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9070.868968963623, \"sum\": 9070.868968963623, \"min\": 9070.868968963623}}, \"EndTime\": 1612196113.647861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196104.576448}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.888651764 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=228, train loss <loss>=-2.64261238575\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:16 INFO 140079012972352] Epoch[229] Batch[0] avg_epoch_loss=-3.283683\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=229, batch=0 train loss <loss>=-3.28368258476\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:19 INFO 140079012972352] Epoch[229] Batch[5] avg_epoch_loss=-3.119051\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=229, batch=5 train loss <loss>=-3.11905145645\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:19 INFO 140079012972352] Epoch[229] Batch [5]#011Speed: 195.09 samples/sec#011loss=-3.119051\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] Epoch[229] Batch[10] avg_epoch_loss=-3.121417\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=229, batch=10 train loss <loss>=-3.12425556183\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] Epoch[229] Batch [10]#011Speed: 195.73 samples/sec#011loss=-3.124256\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9526.13091468811, \"sum\": 9526.13091468811, \"min\": 9526.13091468811}}, \"EndTime\": 1612196123.174912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196113.647939}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.828999795 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=229, train loss <loss>=-3.1214169589\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:23 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:26 INFO 140079012972352] Epoch[230] Batch[0] avg_epoch_loss=-2.837569\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=230, batch=0 train loss <loss>=-2.8375685215\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:29 INFO 140079012972352] Epoch[230] Batch[5] avg_epoch_loss=-3.179897\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=230, batch=5 train loss <loss>=-3.1798967123\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:29 INFO 140079012972352] Epoch[230] Batch [5]#011Speed: 193.18 samples/sec#011loss=-3.179897\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] Epoch[230] Batch[10] avg_epoch_loss=-3.190350\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=230, batch=10 train loss <loss>=-3.20289354324\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] Epoch[230] Batch [10]#011Speed: 196.35 samples/sec#011loss=-3.202894\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9564.12410736084, \"sum\": 9564.12410736084, \"min\": 9564.12410736084}}, \"EndTime\": 1612196132.739733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196123.175032}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.086234624 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=230, train loss <loss>=-3.19034981728\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:32 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:35 INFO 140079012972352] Epoch[231] Batch[0] avg_epoch_loss=-3.002765\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=231, batch=0 train loss <loss>=-3.0027654171\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:39 INFO 140079012972352] Epoch[231] Batch[5] avg_epoch_loss=-3.008498\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=231, batch=5 train loss <loss>=-3.00849807262\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:39 INFO 140079012972352] Epoch[231] Batch [5]#011Speed: 194.74 samples/sec#011loss=-3.008498\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] Epoch[231] Batch[10] avg_epoch_loss=-3.028183\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=231, batch=10 train loss <loss>=-3.05180420876\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] Epoch[231] Batch [10]#011Speed: 196.29 samples/sec#011loss=-3.051804\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] processed a total of 1336 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9542.295932769775, \"sum\": 9542.295932769775, \"min\": 9542.295932769775}}, \"EndTime\": 1612196142.282738, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196132.739824}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.00616292 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=231, train loss <loss>=-3.02818267996\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:42 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:45 INFO 140079012972352] Epoch[232] Batch[0] avg_epoch_loss=-3.177350\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=232, batch=0 train loss <loss>=-3.17734956741\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:48 INFO 140079012972352] Epoch[232] Batch[5] avg_epoch_loss=-3.174566\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=232, batch=5 train loss <loss>=-3.17456607024\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:48 INFO 140079012972352] Epoch[232] Batch [5]#011Speed: 196.58 samples/sec#011loss=-3.174566\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:51 INFO 140079012972352] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8879.091024398804, \"sum\": 8879.091024398804, \"min\": 8879.091024398804}}, \"EndTime\": 1612196151.162513, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196142.282831}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:51 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.255710984 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:51 INFO 140079012972352] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=232, train loss <loss>=-3.2568706274\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:51 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:54 INFO 140079012972352] Epoch[233] Batch[0] avg_epoch_loss=-3.283402\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=233, batch=0 train loss <loss>=-3.28340220451\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:57 INFO 140079012972352] Epoch[233] Batch[5] avg_epoch_loss=-3.301712\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=233, batch=5 train loss <loss>=-3.30171159903\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:15:57 INFO 140079012972352] Epoch[233] Batch [5]#011Speed: 195.24 samples/sec#011loss=-3.301712\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:00 INFO 140079012972352] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9238.461017608643, \"sum\": 9238.461017608643, \"min\": 9238.461017608643}}, \"EndTime\": 1612196160.402048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196151.162607}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:00 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.977233963 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:00 INFO 140079012972352] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=233, train loss <loss>=-3.26145904064\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:00 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:03 INFO 140079012972352] Epoch[234] Batch[0] avg_epoch_loss=-3.152378\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=234, batch=0 train loss <loss>=-3.15237784386\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:07 INFO 140079012972352] Epoch[234] Batch[5] avg_epoch_loss=-3.085810\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=234, batch=5 train loss <loss>=-3.08581038316\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:07 INFO 140079012972352] Epoch[234] Batch [5]#011Speed: 170.35 samples/sec#011loss=-3.085810\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] Epoch[234] Batch[10] avg_epoch_loss=-3.214199\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=234, batch=10 train loss <loss>=-3.36826605797\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] Epoch[234] Batch [10]#011Speed: 195.60 samples/sec#011loss=-3.368266\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10393.91303062439, \"sum\": 10393.91303062439, \"min\": 10393.91303062439}}, \"EndTime\": 1612196170.796619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196160.402138}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=125.840610171 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=234, train loss <loss>=-3.21419932626\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:10 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:13 INFO 140079012972352] Epoch[235] Batch[0] avg_epoch_loss=-3.246748\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=235, batch=0 train loss <loss>=-3.24674797058\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:17 INFO 140079012972352] Epoch[235] Batch[5] avg_epoch_loss=-3.130715\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=235, batch=5 train loss <loss>=-3.13071501255\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:17 INFO 140079012972352] Epoch[235] Batch [5]#011Speed: 198.26 samples/sec#011loss=-3.130715\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:19 INFO 140079012972352] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8917.641162872314, \"sum\": 8917.641162872314, \"min\": 8917.641162872314}}, \"EndTime\": 1612196179.714958, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196170.796765}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:19 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.746544504 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:19 INFO 140079012972352] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=235, train loss <loss>=-3.09565036297\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:19 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:22 INFO 140079012972352] Epoch[236] Batch[0] avg_epoch_loss=-3.303077\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=236, batch=0 train loss <loss>=-3.30307722092\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:25 INFO 140079012972352] Epoch[236] Batch[5] avg_epoch_loss=-3.277916\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=236, batch=5 train loss <loss>=-3.27791555723\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:25 INFO 140079012972352] Epoch[236] Batch [5]#011Speed: 197.30 samples/sec#011loss=-3.277916\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:28 INFO 140079012972352] processed a total of 1240 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8814.090967178345, \"sum\": 8814.090967178345, \"min\": 8814.090967178345}}, \"EndTime\": 1612196188.530028, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196179.715172}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:28 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.681043537 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:28 INFO 140079012972352] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=236, train loss <loss>=-3.2675794363\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:28 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:31 INFO 140079012972352] Epoch[237] Batch[0] avg_epoch_loss=-3.292549\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=237, batch=0 train loss <loss>=-3.2925491333\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:34 INFO 140079012972352] Epoch[237] Batch[5] avg_epoch_loss=-3.492952\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=237, batch=5 train loss <loss>=-3.49295206865\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:34 INFO 140079012972352] Epoch[237] Batch [5]#011Speed: 194.92 samples/sec#011loss=-3.492952\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] Epoch[237] Batch[10] avg_epoch_loss=-3.201697\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=237, batch=10 train loss <loss>=-2.85219154358\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] Epoch[237] Batch [10]#011Speed: 198.71 samples/sec#011loss=-2.852192\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9586.086988449097, \"sum\": 9586.086988449097, \"min\": 9586.086988449097}}, \"EndTime\": 1612196198.116983, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196188.53013}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.359334393 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=237, train loss <loss>=-3.20169728453\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:38 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:41 INFO 140079012972352] Epoch[238] Batch[0] avg_epoch_loss=-2.663691\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=238, batch=0 train loss <loss>=-2.66369080544\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:44 INFO 140079012972352] Epoch[238] Batch[5] avg_epoch_loss=-2.639081\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=238, batch=5 train loss <loss>=-2.63908072313\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:44 INFO 140079012972352] Epoch[238] Batch [5]#011Speed: 194.59 samples/sec#011loss=-2.639081\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:47 INFO 140079012972352] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8927.226066589355, \"sum\": 8927.226066589355, \"min\": 8927.226066589355}}, \"EndTime\": 1612196207.044895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196198.117084}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:47 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.931392005 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:47 INFO 140079012972352] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=238, train loss <loss>=-2.74779589176\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:47 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:50 INFO 140079012972352] Epoch[239] Batch[0] avg_epoch_loss=-3.164180\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=239, batch=0 train loss <loss>=-3.16418027878\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:53 INFO 140079012972352] Epoch[239] Batch[5] avg_epoch_loss=-2.980782\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=239, batch=5 train loss <loss>=-2.98078183333\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:53 INFO 140079012972352] Epoch[239] Batch [5]#011Speed: 197.67 samples/sec#011loss=-2.980782\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] Epoch[239] Batch[10] avg_epoch_loss=-3.078186\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=239, batch=10 train loss <loss>=-3.19507145882\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] Epoch[239] Batch [10]#011Speed: 195.77 samples/sec#011loss=-3.195071\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] processed a total of 1323 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9593.852996826172, \"sum\": 9593.852996826172, \"min\": 9593.852996826172}}, \"EndTime\": 1612196216.639448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196207.044985}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.898829156 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=239, train loss <loss>=-3.07818620855\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:56 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:59 INFO 140079012972352] Epoch[240] Batch[0] avg_epoch_loss=-3.177275\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:16:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=240, batch=0 train loss <loss>=-3.17727518082\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:03 INFO 140079012972352] Epoch[240] Batch[5] avg_epoch_loss=-3.268230\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=240, batch=5 train loss <loss>=-3.26822952429\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:03 INFO 140079012972352] Epoch[240] Batch [5]#011Speed: 185.77 samples/sec#011loss=-3.268230\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] Epoch[240] Batch[10] avg_epoch_loss=-3.324629\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=240, batch=10 train loss <loss>=-3.3923075676\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] Epoch[240] Batch [10]#011Speed: 157.18 samples/sec#011loss=-3.392308\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10656.706094741821, \"sum\": 10656.706094741821, \"min\": 10656.706094741821}}, \"EndTime\": 1612196227.296715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196216.639543}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=121.142922342 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=240, train loss <loss>=-3.32462863489\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:07 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:10 INFO 140079012972352] Epoch[241] Batch[0] avg_epoch_loss=-2.390424\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=241, batch=0 train loss <loss>=-2.39042425156\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:13 INFO 140079012972352] Epoch[241] Batch[5] avg_epoch_loss=-2.217594\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=241, batch=5 train loss <loss>=-2.21759370963\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:13 INFO 140079012972352] Epoch[241] Batch [5]#011Speed: 198.71 samples/sec#011loss=-2.217594\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:16 INFO 140079012972352] processed a total of 1273 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8925.608158111572, \"sum\": 8925.608158111572, \"min\": 8925.608158111572}}, \"EndTime\": 1612196236.22293, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196227.296802}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:16 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.620572737 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:16 INFO 140079012972352] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=241, train loss <loss>=-2.35535652637\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:16 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:19 INFO 140079012972352] Epoch[242] Batch[0] avg_epoch_loss=-2.783916\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=242, batch=0 train loss <loss>=-2.78391647339\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:22 INFO 140079012972352] Epoch[242] Batch[5] avg_epoch_loss=-2.811718\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=242, batch=5 train loss <loss>=-2.81171834469\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:22 INFO 140079012972352] Epoch[242] Batch [5]#011Speed: 197.66 samples/sec#011loss=-2.811718\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:25 INFO 140079012972352] processed a total of 1248 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8960.968017578125, \"sum\": 8960.968017578125, \"min\": 8960.968017578125}}, \"EndTime\": 1612196245.184552, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196236.22306}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:25 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.26872616 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:25 INFO 140079012972352] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=242, train loss <loss>=-2.85130164623\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:25 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:28 INFO 140079012972352] Epoch[243] Batch[0] avg_epoch_loss=-2.973918\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=243, batch=0 train loss <loss>=-2.97391819954\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:31 INFO 140079012972352] Epoch[243] Batch[5] avg_epoch_loss=-3.079314\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=243, batch=5 train loss <loss>=-3.07931363583\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:31 INFO 140079012972352] Epoch[243] Batch [5]#011Speed: 193.20 samples/sec#011loss=-3.079314\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] Epoch[243] Batch[10] avg_epoch_loss=-3.147586\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=243, batch=10 train loss <loss>=-3.22951254845\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] Epoch[243] Batch [10]#011Speed: 193.50 samples/sec#011loss=-3.229513\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9661.972045898438, \"sum\": 9661.972045898438, \"min\": 9661.972045898438}}, \"EndTime\": 1612196254.847226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196245.184638}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.858214876 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=243, train loss <loss>=-3.14758586884\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:34 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:37 INFO 140079012972352] Epoch[244] Batch[0] avg_epoch_loss=-2.282057\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=244, batch=0 train loss <loss>=-2.28205657005\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:41 INFO 140079012972352] Epoch[244] Batch[5] avg_epoch_loss=-2.807067\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=244, batch=5 train loss <loss>=-2.80706727505\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:41 INFO 140079012972352] Epoch[244] Batch [5]#011Speed: 194.96 samples/sec#011loss=-2.807067\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:43 INFO 140079012972352] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8912.554025650024, \"sum\": 8912.554025650024, \"min\": 8912.554025650024}}, \"EndTime\": 1612196263.760432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196254.847312}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.166685245 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=244, train loss <loss>=-2.92748577595\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:46 INFO 140079012972352] Epoch[245] Batch[0] avg_epoch_loss=-3.117430\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=245, batch=0 train loss <loss>=-3.11742997169\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:50 INFO 140079012972352] Epoch[245] Batch[5] avg_epoch_loss=-3.119659\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=245, batch=5 train loss <loss>=-3.11965942383\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:50 INFO 140079012972352] Epoch[245] Batch [5]#011Speed: 195.53 samples/sec#011loss=-3.119659\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] Epoch[245] Batch[10] avg_epoch_loss=-3.164796\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=245, batch=10 train loss <loss>=-3.21895890236\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] Epoch[245] Batch [10]#011Speed: 196.76 samples/sec#011loss=-3.218959\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9538.129091262817, \"sum\": 9538.129091262817, \"min\": 9538.129091262817}}, \"EndTime\": 1612196273.299326, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196263.760522}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.72057522 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=245, train loss <loss>=-3.16479555043\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:53 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:56 INFO 140079012972352] Epoch[246] Batch[0] avg_epoch_loss=-3.214193\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=246, batch=0 train loss <loss>=-3.2141931057\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:59 INFO 140079012972352] Epoch[246] Batch[5] avg_epoch_loss=-3.032382\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=246, batch=5 train loss <loss>=-3.03238244851\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:17:59 INFO 140079012972352] Epoch[246] Batch [5]#011Speed: 195.98 samples/sec#011loss=-3.032382\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] Epoch[246] Batch[10] avg_epoch_loss=-3.071319\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=246, batch=10 train loss <loss>=-3.11804189682\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] Epoch[246] Batch [10]#011Speed: 180.72 samples/sec#011loss=-3.118042\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9833.001852035522, \"sum\": 9833.001852035522, \"min\": 9833.001852035522}}, \"EndTime\": 1612196283.132995, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196273.299417}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.527995394 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=246, train loss <loss>=-3.07131856138\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:03 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:06 INFO 140079012972352] Epoch[247] Batch[0] avg_epoch_loss=-2.556958\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=247, batch=0 train loss <loss>=-2.55695772171\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:10 INFO 140079012972352] Epoch[247] Batch[5] avg_epoch_loss=-2.798334\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=247, batch=5 train loss <loss>=-2.7983341217\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:10 INFO 140079012972352] Epoch[247] Batch [5]#011Speed: 182.01 samples/sec#011loss=-2.798334\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] Epoch[247] Batch[10] avg_epoch_loss=-2.929639\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=247, batch=10 train loss <loss>=-3.08720588684\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] Epoch[247] Batch [10]#011Speed: 196.92 samples/sec#011loss=-3.087206\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10261.536836624146, \"sum\": 10261.536836624146, \"min\": 10261.536836624146}}, \"EndTime\": 1612196293.395257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196283.133089}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=126.782395958 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=247, train loss <loss>=-2.92963946949\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:16 INFO 140079012972352] Epoch[248] Batch[0] avg_epoch_loss=-2.999140\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=248, batch=0 train loss <loss>=-2.99914002419\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:19 INFO 140079012972352] Epoch[248] Batch[5] avg_epoch_loss=-2.988837\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=248, batch=5 train loss <loss>=-2.98883664608\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:19 INFO 140079012972352] Epoch[248] Batch [5]#011Speed: 197.55 samples/sec#011loss=-2.988837\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] Epoch[248] Batch[10] avg_epoch_loss=-3.046486\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=248, batch=10 train loss <loss>=-3.11566586494\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] Epoch[248] Batch [10]#011Speed: 194.14 samples/sec#011loss=-3.115666\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] processed a total of 1331 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9653.234004974365, \"sum\": 9653.234004974365, \"min\": 9653.234004974365}}, \"EndTime\": 1612196303.04924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196293.395351}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.878739824 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=248, train loss <loss>=-3.04648629102\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:23 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:26 INFO 140079012972352] Epoch[249] Batch[0] avg_epoch_loss=-3.055153\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=249, batch=0 train loss <loss>=-3.0551533699\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:29 INFO 140079012972352] Epoch[249] Batch[5] avg_epoch_loss=-2.862167\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=249, batch=5 train loss <loss>=-2.86216723919\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:29 INFO 140079012972352] Epoch[249] Batch [5]#011Speed: 198.32 samples/sec#011loss=-2.862167\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] Epoch[249] Batch[10] avg_epoch_loss=-3.051444\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=249, batch=10 train loss <loss>=-3.27857651711\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] Epoch[249] Batch [10]#011Speed: 191.49 samples/sec#011loss=-3.278577\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9609.580993652344, \"sum\": 9609.580993652344, \"min\": 9609.580993652344}}, \"EndTime\": 1612196312.659579, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196303.049369}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.112115613 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=249, train loss <loss>=-3.0514441837\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:32 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:35 INFO 140079012972352] Epoch[250] Batch[0] avg_epoch_loss=-3.105903\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=250, batch=0 train loss <loss>=-3.10590338707\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:39 INFO 140079012972352] Epoch[250] Batch[5] avg_epoch_loss=-3.289555\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=250, batch=5 train loss <loss>=-3.28955535094\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:39 INFO 140079012972352] Epoch[250] Batch [5]#011Speed: 194.54 samples/sec#011loss=-3.289555\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:41 INFO 140079012972352] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9269.114971160889, \"sum\": 9269.114971160889, \"min\": 9269.114971160889}}, \"EndTime\": 1612196321.929379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196312.659677}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:41 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.119974645 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:41 INFO 140079012972352] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=250, train loss <loss>=-3.33848750591\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:41 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:45 INFO 140079012972352] Epoch[251] Batch[0] avg_epoch_loss=-3.392604\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=251, batch=0 train loss <loss>=-3.39260387421\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:48 INFO 140079012972352] Epoch[251] Batch[5] avg_epoch_loss=-3.447831\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=251, batch=5 train loss <loss>=-3.44783067703\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:48 INFO 140079012972352] Epoch[251] Batch [5]#011Speed: 195.12 samples/sec#011loss=-3.447831\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] Epoch[251] Batch[10] avg_epoch_loss=-3.378226\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=251, batch=10 train loss <loss>=-3.29470009804\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] Epoch[251] Batch [10]#011Speed: 196.64 samples/sec#011loss=-3.294700\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9612.650156021118, \"sum\": 9612.650156021118, \"min\": 9612.650156021118}}, \"EndTime\": 1612196331.54265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196321.929472}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.548265614 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=251, train loss <loss>=-3.3782258684\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:51 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:54 INFO 140079012972352] Epoch[252] Batch[0] avg_epoch_loss=-1.722972\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=252, batch=0 train loss <loss>=-1.72297215462\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:57 INFO 140079012972352] Epoch[252] Batch[5] avg_epoch_loss=-1.891351\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=252, batch=5 train loss <loss>=-1.89135106405\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:18:57 INFO 140079012972352] Epoch[252] Batch [5]#011Speed: 197.51 samples/sec#011loss=-1.891351\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:00 INFO 140079012972352] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8909.280061721802, \"sum\": 8909.280061721802, \"min\": 8909.280061721802}}, \"EndTime\": 1612196340.452674, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196331.542748}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:00 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.443438911 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:00 INFO 140079012972352] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=252, train loss <loss>=-2.20368020535\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:00 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:03 INFO 140079012972352] Epoch[253] Batch[0] avg_epoch_loss=-2.721684\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=253, batch=0 train loss <loss>=-2.72168445587\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:07 INFO 140079012972352] Epoch[253] Batch[5] avg_epoch_loss=-2.641831\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=253, batch=5 train loss <loss>=-2.64183119933\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:07 INFO 140079012972352] Epoch[253] Batch [5]#011Speed: 167.34 samples/sec#011loss=-2.641831\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:10 INFO 140079012972352] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9720.881223678589, \"sum\": 9720.881223678589, \"min\": 9720.881223678589}}, \"EndTime\": 1612196350.174554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196340.452779}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:10 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.027386441 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:10 INFO 140079012972352] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=253, train loss <loss>=-2.68226592541\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:10 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:13 INFO 140079012972352] Epoch[254] Batch[0] avg_epoch_loss=-2.731328\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=254, batch=0 train loss <loss>=-2.73132801056\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:16 INFO 140079012972352] Epoch[254] Batch[5] avg_epoch_loss=-2.919524\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=254, batch=5 train loss <loss>=-2.9195240736\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:16 INFO 140079012972352] Epoch[254] Batch [5]#011Speed: 197.79 samples/sec#011loss=-2.919524\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] Epoch[254] Batch[10] avg_epoch_loss=-3.080287\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=254, batch=10 train loss <loss>=-3.27320284843\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] Epoch[254] Batch [10]#011Speed: 198.15 samples/sec#011loss=-3.273203\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9515.84506034851, \"sum\": 9515.84506034851, \"min\": 9515.84506034851}}, \"EndTime\": 1612196359.691158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196350.174655}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.615495457 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=254, train loss <loss>=-3.08028715307\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:19 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:22 INFO 140079012972352] Epoch[255] Batch[0] avg_epoch_loss=-1.246451\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=255, batch=0 train loss <loss>=-1.24645090103\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:25 INFO 140079012972352] Epoch[255] Batch[5] avg_epoch_loss=-1.504790\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=255, batch=5 train loss <loss>=-1.50478963057\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:25 INFO 140079012972352] Epoch[255] Batch [5]#011Speed: 198.23 samples/sec#011loss=-1.504790\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] Epoch[255] Batch[10] avg_epoch_loss=-1.824131\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=255, batch=10 train loss <loss>=-2.20734086037\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] Epoch[255] Batch [10]#011Speed: 196.94 samples/sec#011loss=-2.207341\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9530.69806098938, \"sum\": 9530.69806098938, \"min\": 9530.69806098938}}, \"EndTime\": 1612196369.222697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196359.691257}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.769504184 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=255, train loss <loss>=-1.82413109866\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:29 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:32 INFO 140079012972352] Epoch[256] Batch[0] avg_epoch_loss=-2.262945\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=256, batch=0 train loss <loss>=-2.26294469833\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:35 INFO 140079012972352] Epoch[256] Batch[5] avg_epoch_loss=-2.129323\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=256, batch=5 train loss <loss>=-2.12932346265\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:35 INFO 140079012972352] Epoch[256] Batch [5]#011Speed: 197.71 samples/sec#011loss=-2.129323\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] Epoch[256] Batch[10] avg_epoch_loss=-2.143417\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=256, batch=10 train loss <loss>=-2.16032862663\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] Epoch[256] Batch [10]#011Speed: 196.99 samples/sec#011loss=-2.160329\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] processed a total of 1325 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9571.777820587158, \"sum\": 9571.777820587158, \"min\": 9571.777820587158}}, \"EndTime\": 1612196378.795251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196369.222812}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.42557117 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=256, train loss <loss>=-2.143416719\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:38 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:41 INFO 140079012972352] Epoch[257] Batch[0] avg_epoch_loss=-2.266659\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=257, batch=0 train loss <loss>=-2.26665949821\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:45 INFO 140079012972352] Epoch[257] Batch[5] avg_epoch_loss=-2.383793\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=257, batch=5 train loss <loss>=-2.38379283746\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:45 INFO 140079012972352] Epoch[257] Batch [5]#011Speed: 198.42 samples/sec#011loss=-2.383793\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:47 INFO 140079012972352] processed a total of 1193 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8815.13500213623, \"sum\": 8815.13500213623, \"min\": 8815.13500213623}}, \"EndTime\": 1612196387.611062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196378.795355}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:47 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.333257009 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:47 INFO 140079012972352] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=257, train loss <loss>=-2.47840077877\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:47 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:50 INFO 140079012972352] Epoch[258] Batch[0] avg_epoch_loss=-2.363722\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=258, batch=0 train loss <loss>=-2.36372208595\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:53 INFO 140079012972352] Epoch[258] Batch[5] avg_epoch_loss=-2.444685\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=258, batch=5 train loss <loss>=-2.44468458494\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:53 INFO 140079012972352] Epoch[258] Batch [5]#011Speed: 195.92 samples/sec#011loss=-2.444685\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:56 INFO 140079012972352] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8897.763967514038, \"sum\": 8897.763967514038, \"min\": 8897.763967514038}}, \"EndTime\": 1612196396.509444, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196387.611156}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:56 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.392044626 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:56 INFO 140079012972352] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=258, train loss <loss>=-2.54706866741\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:56 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:59 INFO 140079012972352] Epoch[259] Batch[0] avg_epoch_loss=-2.826080\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:19:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=259, batch=0 train loss <loss>=-2.82607984543\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:03 INFO 140079012972352] Epoch[259] Batch[5] avg_epoch_loss=-2.907943\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=259, batch=5 train loss <loss>=-2.90794340769\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:03 INFO 140079012972352] Epoch[259] Batch [5]#011Speed: 176.30 samples/sec#011loss=-2.907943\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] Epoch[259] Batch[10] avg_epoch_loss=-3.006987\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=259, batch=10 train loss <loss>=-3.12583918571\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] Epoch[259] Batch [10]#011Speed: 159.96 samples/sec#011loss=-3.125839\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10645.063161849976, \"sum\": 10645.063161849976, \"min\": 10645.063161849976}}, \"EndTime\": 1612196407.155218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196396.509604}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=122.778409995 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=259, train loss <loss>=-3.00698694316\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:07 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:10 INFO 140079012972352] Epoch[260] Batch[0] avg_epoch_loss=-3.213679\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=260, batch=0 train loss <loss>=-3.21367907524\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:13 INFO 140079012972352] Epoch[260] Batch[5] avg_epoch_loss=-3.044460\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=260, batch=5 train loss <loss>=-3.04446049531\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:13 INFO 140079012972352] Epoch[260] Batch [5]#011Speed: 196.25 samples/sec#011loss=-3.044460\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] Epoch[260] Batch[10] avg_epoch_loss=-2.790267\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=260, batch=10 train loss <loss>=-2.4852342397\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] Epoch[260] Batch [10]#011Speed: 194.85 samples/sec#011loss=-2.485234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9620.434045791626, \"sum\": 9620.434045791626, \"min\": 9620.434045791626}}, \"EndTime\": 1612196416.776212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196407.155305}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.879788803 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=260, train loss <loss>=-2.79026674276\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:16 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:19 INFO 140079012972352] Epoch[261] Batch[0] avg_epoch_loss=-3.279538\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=261, batch=0 train loss <loss>=-3.27953839302\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:23 INFO 140079012972352] Epoch[261] Batch[5] avg_epoch_loss=-3.298287\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=261, batch=5 train loss <loss>=-3.29828675588\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:23 INFO 140079012972352] Epoch[261] Batch [5]#011Speed: 195.61 samples/sec#011loss=-3.298287\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] Epoch[261] Batch[10] avg_epoch_loss=-3.328672\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=261, batch=10 train loss <loss>=-3.36513504982\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] Epoch[261] Batch [10]#011Speed: 194.24 samples/sec#011loss=-3.365135\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9564.14794921875, \"sum\": 9564.14794921875, \"min\": 9564.14794921875}}, \"EndTime\": 1612196426.340994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196416.776303}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.222749954 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=261, train loss <loss>=-3.32867234403\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:26 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:29 INFO 140079012972352] Epoch[262] Batch[0] avg_epoch_loss=-3.374721\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=262, batch=0 train loss <loss>=-3.37472081184\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:32 INFO 140079012972352] Epoch[262] Batch[5] avg_epoch_loss=-3.142722\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=262, batch=5 train loss <loss>=-3.14272181193\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:32 INFO 140079012972352] Epoch[262] Batch [5]#011Speed: 191.47 samples/sec#011loss=-3.142722\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:35 INFO 140079012972352] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8897.825002670288, \"sum\": 8897.825002670288, \"min\": 8897.825002670288}}, \"EndTime\": 1612196435.239384, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196426.341075}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:35 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.380816514 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:35 INFO 140079012972352] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=262, train loss <loss>=-3.16773169041\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:35 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:38 INFO 140079012972352] Epoch[263] Batch[0] avg_epoch_loss=-2.857762\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=263, batch=0 train loss <loss>=-2.85776162148\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:41 INFO 140079012972352] Epoch[263] Batch[5] avg_epoch_loss=-2.946030\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=263, batch=5 train loss <loss>=-2.94602994124\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:41 INFO 140079012972352] Epoch[263] Batch [5]#011Speed: 196.43 samples/sec#011loss=-2.946030\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] Epoch[263] Batch[10] avg_epoch_loss=-2.972351\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=263, batch=10 train loss <loss>=-3.00393714905\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] Epoch[263] Batch [10]#011Speed: 197.80 samples/sec#011loss=-3.003937\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9501.196146011353, \"sum\": 9501.196146011353, \"min\": 9501.196146011353}}, \"EndTime\": 1612196444.741322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196435.239474}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.349229031 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=263, train loss <loss>=-2.97235139933\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:44 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:47 INFO 140079012972352] Epoch[264] Batch[0] avg_epoch_loss=-2.882366\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=264, batch=0 train loss <loss>=-2.88236570358\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:51 INFO 140079012972352] Epoch[264] Batch[5] avg_epoch_loss=-2.938574\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=264, batch=5 train loss <loss>=-2.93857399623\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:51 INFO 140079012972352] Epoch[264] Batch [5]#011Speed: 197.23 samples/sec#011loss=-2.938574\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:53 INFO 140079012972352] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8884.402990341187, \"sum\": 8884.402990341187, \"min\": 8884.402990341187}}, \"EndTime\": 1612196453.626347, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196444.741424}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:53 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.269814088 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:53 INFO 140079012972352] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=264, train loss <loss>=-2.99726667404\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:53 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:56 INFO 140079012972352] Epoch[265] Batch[0] avg_epoch_loss=-3.090214\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=265, batch=0 train loss <loss>=-3.09021401405\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:59 INFO 140079012972352] Epoch[265] Batch[5] avg_epoch_loss=-3.103203\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=265, batch=5 train loss <loss>=-3.10320321719\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:20:59 INFO 140079012972352] Epoch[265] Batch [5]#011Speed: 198.41 samples/sec#011loss=-3.103203\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] Epoch[265] Batch[10] avg_epoch_loss=-3.040033\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=265, batch=10 train loss <loss>=-2.9642282486\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] Epoch[265] Batch [10]#011Speed: 179.75 samples/sec#011loss=-2.964228\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] processed a total of 1313 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9798.64501953125, \"sum\": 9798.64501953125, \"min\": 9798.64501953125}}, \"EndTime\": 1612196463.425688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196453.626427}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.995549703 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=265, train loss <loss>=-3.04003277692\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:03 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:06 INFO 140079012972352] Epoch[266] Batch[0] avg_epoch_loss=-2.761174\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=266, batch=0 train loss <loss>=-2.76117420197\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:10 INFO 140079012972352] Epoch[266] Batch[5] avg_epoch_loss=-2.907584\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=266, batch=5 train loss <loss>=-2.90758355459\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:10 INFO 140079012972352] Epoch[266] Batch [5]#011Speed: 158.00 samples/sec#011loss=-2.907584\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:13 INFO 140079012972352] processed a total of 1233 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9773.853063583374, \"sum\": 9773.853063583374, \"min\": 9773.853063583374}}, \"EndTime\": 1612196473.200159, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196463.425786}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=126.150859941 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=266, train loss <loss>=-2.96158080101\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:16 INFO 140079012972352] Epoch[267] Batch[0] avg_epoch_loss=-2.787506\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=267, batch=0 train loss <loss>=-2.78750562668\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:19 INFO 140079012972352] Epoch[267] Batch[5] avg_epoch_loss=-2.929566\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=267, batch=5 train loss <loss>=-2.92956618468\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:19 INFO 140079012972352] Epoch[267] Batch [5]#011Speed: 196.20 samples/sec#011loss=-2.929566\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:22 INFO 140079012972352] processed a total of 1253 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8938.18998336792, \"sum\": 8938.18998336792, \"min\": 8938.18998336792}}, \"EndTime\": 1612196482.139203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196473.200269}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:22 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.182252137 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:22 INFO 140079012972352] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=267, train loss <loss>=-2.98874993324\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:22 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:25 INFO 140079012972352] Epoch[268] Batch[0] avg_epoch_loss=-3.292183\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=268, batch=0 train loss <loss>=-3.2921833992\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:28 INFO 140079012972352] Epoch[268] Batch[5] avg_epoch_loss=-3.233144\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=268, batch=5 train loss <loss>=-3.23314368725\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:28 INFO 140079012972352] Epoch[268] Batch [5]#011Speed: 192.95 samples/sec#011loss=-3.233144\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:31 INFO 140079012972352] processed a total of 1220 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9012.553930282593, \"sum\": 9012.553930282593, \"min\": 9012.553930282593}}, \"EndTime\": 1612196491.152489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196482.139331}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.364389536 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=268, train loss <loss>=-3.26060349941\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:31 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:34 INFO 140079012972352] Epoch[269] Batch[0] avg_epoch_loss=-3.484828\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=269, batch=0 train loss <loss>=-3.48482847214\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:37 INFO 140079012972352] Epoch[269] Batch[5] avg_epoch_loss=-3.268562\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=269, batch=5 train loss <loss>=-3.26856223742\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:37 INFO 140079012972352] Epoch[269] Batch [5]#011Speed: 192.70 samples/sec#011loss=-3.268562\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] Epoch[269] Batch[10] avg_epoch_loss=-3.350792\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=269, batch=10 train loss <loss>=-3.44946689606\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] Epoch[269] Batch [10]#011Speed: 197.68 samples/sec#011loss=-3.449467\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9612.662076950073, \"sum\": 9612.662076950073, \"min\": 9612.662076950073}}, \"EndTime\": 1612196500.766167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196491.152596}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.674876097 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=269, train loss <loss>=-3.35079162771\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:40 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:43 INFO 140079012972352] Epoch[270] Batch[0] avg_epoch_loss=-2.932307\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=270, batch=0 train loss <loss>=-2.93230700493\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:47 INFO 140079012972352] Epoch[270] Batch[5] avg_epoch_loss=-2.851595\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=270, batch=5 train loss <loss>=-2.85159488519\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:47 INFO 140079012972352] Epoch[270] Batch [5]#011Speed: 198.41 samples/sec#011loss=-2.851595\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:49 INFO 140079012972352] processed a total of 1280 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8963.486194610596, \"sum\": 8963.486194610596, \"min\": 8963.486194610596}}, \"EndTime\": 1612196509.730445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196500.766337}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:49 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.79946536 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:49 INFO 140079012972352] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=270, train loss <loss>=-2.86151390076\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:49 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:52 INFO 140079012972352] Epoch[271] Batch[0] avg_epoch_loss=-3.240179\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=271, batch=0 train loss <loss>=-3.24017858505\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:56 INFO 140079012972352] Epoch[271] Batch[5] avg_epoch_loss=-3.055822\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=271, batch=5 train loss <loss>=-3.05582165718\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:56 INFO 140079012972352] Epoch[271] Batch [5]#011Speed: 197.89 samples/sec#011loss=-3.055822\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:58 INFO 140079012972352] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8980.881929397583, \"sum\": 8980.881929397583, \"min\": 8980.881929397583}}, \"EndTime\": 1612196518.712002, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196509.730535}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:58 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.520422297 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:58 INFO 140079012972352] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:58 INFO 140079012972352] #quality_metric: host=algo-1, epoch=271, train loss <loss>=-3.09307131767\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:21:58 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:01 INFO 140079012972352] Epoch[272] Batch[0] avg_epoch_loss=-3.240488\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:01 INFO 140079012972352] #quality_metric: host=algo-1, epoch=272, batch=0 train loss <loss>=-3.24048829079\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:05 INFO 140079012972352] Epoch[272] Batch[5] avg_epoch_loss=-3.194003\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=272, batch=5 train loss <loss>=-3.19400302569\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:05 INFO 140079012972352] Epoch[272] Batch [5]#011Speed: 168.51 samples/sec#011loss=-3.194003\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:08 INFO 140079012972352] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10013.658046722412, \"sum\": 10013.658046722412, \"min\": 10013.658046722412}}, \"EndTime\": 1612196528.726372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196518.712112}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:08 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=126.924754251 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:08 INFO 140079012972352] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=272, train loss <loss>=-3.11268064976\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:08 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:11 INFO 140079012972352] Epoch[273] Batch[0] avg_epoch_loss=-3.283023\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=273, batch=0 train loss <loss>=-3.28302288055\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:15 INFO 140079012972352] Epoch[273] Batch[5] avg_epoch_loss=-3.312284\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=273, batch=5 train loss <loss>=-3.31228355567\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:15 INFO 140079012972352] Epoch[273] Batch [5]#011Speed: 196.08 samples/sec#011loss=-3.312284\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:17 INFO 140079012972352] processed a total of 1247 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8920.567035675049, \"sum\": 8920.567035675049, \"min\": 8920.567035675049}}, \"EndTime\": 1612196537.647628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196528.726475}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:17 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.787190448 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:17 INFO 140079012972352] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=273, train loss <loss>=-3.4068716526\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:17 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:20 INFO 140079012972352] Epoch[274] Batch[0] avg_epoch_loss=-3.625226\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=274, batch=0 train loss <loss>=-3.62522602081\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:23 INFO 140079012972352] Epoch[274] Batch[5] avg_epoch_loss=-3.494782\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=274, batch=5 train loss <loss>=-3.4947817723\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:23 INFO 140079012972352] Epoch[274] Batch [5]#011Speed: 195.43 samples/sec#011loss=-3.494782\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:26 INFO 140079012972352] processed a total of 1219 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8933.17198753357, \"sum\": 8933.17198753357, \"min\": 8933.17198753357}}, \"EndTime\": 1612196546.581567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196537.64772}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:26 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.455488762 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:26 INFO 140079012972352] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=274, train loss <loss>=-3.50134403706\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:26 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:29 INFO 140079012972352] Epoch[275] Batch[0] avg_epoch_loss=-2.538530\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=275, batch=0 train loss <loss>=-2.53853034973\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:32 INFO 140079012972352] Epoch[275] Batch[5] avg_epoch_loss=-2.552700\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=275, batch=5 train loss <loss>=-2.55270016193\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:32 INFO 140079012972352] Epoch[275] Batch [5]#011Speed: 194.75 samples/sec#011loss=-2.552700\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:35 INFO 140079012972352] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9008.965015411377, \"sum\": 9008.965015411377, \"min\": 9008.965015411377}}, \"EndTime\": 1612196555.591238, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196546.581667}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:35 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.747478216 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:35 INFO 140079012972352] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=275, train loss <loss>=-2.69350435734\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:35 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:38 INFO 140079012972352] Epoch[276] Batch[0] avg_epoch_loss=-3.077058\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=276, batch=0 train loss <loss>=-3.07705831528\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:42 INFO 140079012972352] Epoch[276] Batch[5] avg_epoch_loss=-2.983614\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=276, batch=5 train loss <loss>=-2.98361372948\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:42 INFO 140079012972352] Epoch[276] Batch [5]#011Speed: 182.62 samples/sec#011loss=-2.983614\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] Epoch[276] Batch[10] avg_epoch_loss=-3.178153\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=276, batch=10 train loss <loss>=-3.41160068512\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] Epoch[276] Batch [10]#011Speed: 195.21 samples/sec#011loss=-3.411601\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9983.751058578491, \"sum\": 9983.751058578491, \"min\": 9983.751058578491}}, \"EndTime\": 1612196565.575669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196555.591332}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=128.807588142 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=276, train loss <loss>=-3.17815325477\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:45 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:48 INFO 140079012972352] Epoch[277] Batch[0] avg_epoch_loss=-3.230794\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=277, batch=0 train loss <loss>=-3.23079371452\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:51 INFO 140079012972352] Epoch[277] Batch[5] avg_epoch_loss=-3.245348\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=277, batch=5 train loss <loss>=-3.24534817537\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:51 INFO 140079012972352] Epoch[277] Batch [5]#011Speed: 197.96 samples/sec#011loss=-3.245348\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:54 INFO 140079012972352] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8957.526922225952, \"sum\": 8957.526922225952, \"min\": 8957.526922225952}}, \"EndTime\": 1612196574.533933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196565.575756}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:54 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.889210293 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:54 INFO 140079012972352] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=277, train loss <loss>=-3.31874520779\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:54 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:57 INFO 140079012972352] Epoch[278] Batch[0] avg_epoch_loss=-3.281291\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:22:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=278, batch=0 train loss <loss>=-3.281291008\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:00 INFO 140079012972352] Epoch[278] Batch[5] avg_epoch_loss=-3.283108\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=278, batch=5 train loss <loss>=-3.2831081152\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:00 INFO 140079012972352] Epoch[278] Batch [5]#011Speed: 195.23 samples/sec#011loss=-3.283108\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:04 INFO 140079012972352] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9473.326921463013, \"sum\": 9473.326921463013, \"min\": 9473.326921463013}}, \"EndTime\": 1612196584.008167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196574.534026}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:04 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.952901109 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:04 INFO 140079012972352] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=278, train loss <loss>=-3.33479747772\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:04 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:07 INFO 140079012972352] Epoch[279] Batch[0] avg_epoch_loss=-3.087748\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=279, batch=0 train loss <loss>=-3.08774781227\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:10 INFO 140079012972352] Epoch[279] Batch[5] avg_epoch_loss=-3.283824\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=279, batch=5 train loss <loss>=-3.28382432461\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:10 INFO 140079012972352] Epoch[279] Batch [5]#011Speed: 196.15 samples/sec#011loss=-3.283824\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] Epoch[279] Batch[10] avg_epoch_loss=-3.352983\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=279, batch=10 train loss <loss>=-3.43597316742\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] Epoch[279] Batch [10]#011Speed: 193.58 samples/sec#011loss=-3.435973\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9816.0400390625, \"sum\": 9816.0400390625, \"min\": 9816.0400390625}}, \"EndTime\": 1612196593.82488, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196584.00827}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=131.823266552 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=279, train loss <loss>=-3.35298288952\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:13 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:16 INFO 140079012972352] Epoch[280] Batch[0] avg_epoch_loss=-3.147546\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=280, batch=0 train loss <loss>=-3.14754581451\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:20 INFO 140079012972352] Epoch[280] Batch[5] avg_epoch_loss=-2.905541\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=280, batch=5 train loss <loss>=-2.90554098288\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:20 INFO 140079012972352] Epoch[280] Batch [5]#011Speed: 194.27 samples/sec#011loss=-2.905541\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] Epoch[280] Batch[10] avg_epoch_loss=-2.801714\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=280, batch=10 train loss <loss>=-2.67712154388\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] Epoch[280] Batch [10]#011Speed: 196.09 samples/sec#011loss=-2.677122\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9547.113180160522, \"sum\": 9547.113180160522, \"min\": 9547.113180160522}}, \"EndTime\": 1612196603.372642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196593.824967}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.698812665 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=280, train loss <loss>=-2.80171396516\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:23 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:26 INFO 140079012972352] Epoch[281] Batch[0] avg_epoch_loss=-2.905033\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=281, batch=0 train loss <loss>=-2.90503311157\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:29 INFO 140079012972352] Epoch[281] Batch[5] avg_epoch_loss=-2.820097\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=281, batch=5 train loss <loss>=-2.82009736697\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:29 INFO 140079012972352] Epoch[281] Batch [5]#011Speed: 195.86 samples/sec#011loss=-2.820097\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:32 INFO 140079012972352] processed a total of 1239 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8982.561111450195, \"sum\": 8982.561111450195, \"min\": 8982.561111450195}}, \"EndTime\": 1612196612.355846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196603.372713}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:32 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.931862798 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:32 INFO 140079012972352] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=281, train loss <loss>=-2.94080305099\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:32 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:35 INFO 140079012972352] Epoch[282] Batch[0] avg_epoch_loss=-3.165688\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=282, batch=0 train loss <loss>=-3.16568827629\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:38 INFO 140079012972352] Epoch[282] Batch[5] avg_epoch_loss=-3.169039\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=282, batch=5 train loss <loss>=-3.16903924942\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:38 INFO 140079012972352] Epoch[282] Batch [5]#011Speed: 197.29 samples/sec#011loss=-3.169039\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] Epoch[282] Batch[10] avg_epoch_loss=-3.289040\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=282, batch=10 train loss <loss>=-3.43304080963\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] Epoch[282] Batch [10]#011Speed: 193.85 samples/sec#011loss=-3.433041\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9566.695928573608, \"sum\": 9566.695928573608, \"min\": 9566.695928573608}}, \"EndTime\": 1612196621.923355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196612.355937}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.78161105 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=282, train loss <loss>=-3.28903995861\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:41 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:45 INFO 140079012972352] Epoch[283] Batch[0] avg_epoch_loss=-2.808733\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=283, batch=0 train loss <loss>=-2.80873322487\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:48 INFO 140079012972352] Epoch[283] Batch[5] avg_epoch_loss=-2.907220\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=283, batch=5 train loss <loss>=-2.90722040335\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:48 INFO 140079012972352] Epoch[283] Batch [5]#011Speed: 191.78 samples/sec#011loss=-2.907220\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:51 INFO 140079012972352] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9143.884897232056, \"sum\": 9143.884897232056, \"min\": 9143.884897232056}}, \"EndTime\": 1612196631.067946, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196621.92345}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:51 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=137.576085801 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:51 INFO 140079012972352] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=283, train loss <loss>=-2.9878921032\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:51 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:54 INFO 140079012972352] Epoch[284] Batch[0] avg_epoch_loss=-3.225958\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=284, batch=0 train loss <loss>=-3.2259581089\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:57 INFO 140079012972352] Epoch[284] Batch[5] avg_epoch_loss=-3.239334\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=284, batch=5 train loss <loss>=-3.23933382829\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:23:57 INFO 140079012972352] Epoch[284] Batch [5]#011Speed: 197.42 samples/sec#011loss=-3.239334\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] Epoch[284] Batch[10] avg_epoch_loss=-3.250778\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=284, batch=10 train loss <loss>=-3.26451129913\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] Epoch[284] Batch [10]#011Speed: 198.36 samples/sec#011loss=-3.264511\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9437.443971633911, \"sum\": 9437.443971633911, \"min\": 9437.443971633911}}, \"EndTime\": 1612196640.506491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196631.068047}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.369214178 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=284, train loss <loss>=-3.25077813322\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:00 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:03 INFO 140079012972352] Epoch[285] Batch[0] avg_epoch_loss=-3.061353\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=285, batch=0 train loss <loss>=-3.06135320663\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:07 INFO 140079012972352] Epoch[285] Batch[5] avg_epoch_loss=-3.137880\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=285, batch=5 train loss <loss>=-3.1378800869\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:07 INFO 140079012972352] Epoch[285] Batch [5]#011Speed: 158.95 samples/sec#011loss=-3.137880\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:10 INFO 140079012972352] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9756.062030792236, \"sum\": 9756.062030792236, \"min\": 9756.062030792236}}, \"EndTime\": 1612196650.263336, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196640.506616}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:10 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.993543529 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:10 INFO 140079012972352] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=285, train loss <loss>=-3.17662599087\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:10 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:13 INFO 140079012972352] Epoch[286] Batch[0] avg_epoch_loss=-3.091022\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=286, batch=0 train loss <loss>=-3.09102153778\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:16 INFO 140079012972352] Epoch[286] Batch[5] avg_epoch_loss=-3.357584\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=286, batch=5 train loss <loss>=-3.35758388042\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:16 INFO 140079012972352] Epoch[286] Batch [5]#011Speed: 198.47 samples/sec#011loss=-3.357584\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] Epoch[286] Batch[10] avg_epoch_loss=-3.412628\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=286, batch=10 train loss <loss>=-3.47868032455\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] Epoch[286] Batch [10]#011Speed: 193.29 samples/sec#011loss=-3.478680\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9607.949018478394, \"sum\": 9607.949018478394, \"min\": 9607.949018478394}}, \"EndTime\": 1612196659.871885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196650.263436}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.886515326 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=286, train loss <loss>=-3.41262771867\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:19 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:22 INFO 140079012972352] Epoch[287] Batch[0] avg_epoch_loss=-2.298429\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:22 INFO 140079012972352] #quality_metric: host=algo-1, epoch=287, batch=0 train loss <loss>=-2.29842948914\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:26 INFO 140079012972352] Epoch[287] Batch[5] avg_epoch_loss=-2.113176\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=287, batch=5 train loss <loss>=-2.11317634583\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:26 INFO 140079012972352] Epoch[287] Batch [5]#011Speed: 198.27 samples/sec#011loss=-2.113176\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] Epoch[287] Batch[10] avg_epoch_loss=-2.284352\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=287, batch=10 train loss <loss>=-2.48976287842\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] Epoch[287] Batch [10]#011Speed: 197.65 samples/sec#011loss=-2.489763\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9440.340042114258, \"sum\": 9440.340042114258, \"min\": 9440.340042114258}}, \"EndTime\": 1612196669.31288, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196659.871973}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.904079202 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=287, train loss <loss>=-2.28435204246\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:29 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:32 INFO 140079012972352] Epoch[288] Batch[0] avg_epoch_loss=-2.474204\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=288, batch=0 train loss <loss>=-2.474203825\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:35 INFO 140079012972352] Epoch[288] Batch[5] avg_epoch_loss=-2.414275\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:35 INFO 140079012972352] #quality_metric: host=algo-1, epoch=288, batch=5 train loss <loss>=-2.41427505016\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:35 INFO 140079012972352] Epoch[288] Batch [5]#011Speed: 198.45 samples/sec#011loss=-2.414275\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:38 INFO 140079012972352] processed a total of 1254 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8871.807098388672, \"sum\": 8871.807098388672, \"min\": 8871.807098388672}}, \"EndTime\": 1612196678.185285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196669.312979}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:38 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.344495081 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:38 INFO 140079012972352] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=288, train loss <loss>=-2.44666090012\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:38 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:41 INFO 140079012972352] Epoch[289] Batch[0] avg_epoch_loss=-2.529001\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=289, batch=0 train loss <loss>=-2.52900147438\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:44 INFO 140079012972352] Epoch[289] Batch[5] avg_epoch_loss=-2.675084\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:44 INFO 140079012972352] #quality_metric: host=algo-1, epoch=289, batch=5 train loss <loss>=-2.67508427302\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:44 INFO 140079012972352] Epoch[289] Batch [5]#011Speed: 198.28 samples/sec#011loss=-2.675084\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] Epoch[289] Batch[10] avg_epoch_loss=-2.837810\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=289, batch=10 train loss <loss>=-3.03308162689\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] Epoch[289] Batch [10]#011Speed: 196.02 samples/sec#011loss=-3.033082\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9591.97998046875, \"sum\": 9591.97998046875, \"min\": 9591.97998046875}}, \"EndTime\": 1612196687.777925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196678.185374}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.485404413 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] #quality_metric: host=algo-1, epoch=289, train loss <loss>=-2.83781034296\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:47 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:50 INFO 140079012972352] Epoch[290] Batch[0] avg_epoch_loss=-2.828115\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=290, batch=0 train loss <loss>=-2.82811522484\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:53 INFO 140079012972352] Epoch[290] Batch[5] avg_epoch_loss=-2.828255\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=290, batch=5 train loss <loss>=-2.82825454076\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:53 INFO 140079012972352] Epoch[290] Batch [5]#011Speed: 198.21 samples/sec#011loss=-2.828255\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] Epoch[290] Batch[10] avg_epoch_loss=-3.037012\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=290, batch=10 train loss <loss>=-3.28752174377\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] Epoch[290] Batch [10]#011Speed: 196.57 samples/sec#011loss=-3.287522\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9464.455127716064, \"sum\": 9464.455127716064, \"min\": 9464.455127716064}}, \"EndTime\": 1612196697.242975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196687.778021}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.402397828 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=290, train loss <loss>=-3.03701236031\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:24:57 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:00 INFO 140079012972352] Epoch[291] Batch[0] avg_epoch_loss=-2.616748\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=291, batch=0 train loss <loss>=-2.61674833298\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:03 INFO 140079012972352] Epoch[291] Batch[5] avg_epoch_loss=-2.877343\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:03 INFO 140079012972352] #quality_metric: host=algo-1, epoch=291, batch=5 train loss <loss>=-2.87734333674\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:03 INFO 140079012972352] Epoch[291] Batch [5]#011Speed: 183.56 samples/sec#011loss=-2.877343\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] Epoch[291] Batch[10] avg_epoch_loss=-3.000083\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=291, batch=10 train loss <loss>=-3.14736967087\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] Epoch[291] Batch [10]#011Speed: 194.17 samples/sec#011loss=-3.147370\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9913.138151168823, \"sum\": 9913.138151168823, \"min\": 9913.138151168823}}, \"EndTime\": 1612196707.156769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196697.243122}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.429636562 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=291, train loss <loss>=-3.00008257953\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:07 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:10 INFO 140079012972352] Epoch[292] Batch[0] avg_epoch_loss=-2.892469\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:10 INFO 140079012972352] #quality_metric: host=algo-1, epoch=292, batch=0 train loss <loss>=-2.89246916771\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:14 INFO 140079012972352] Epoch[292] Batch[5] avg_epoch_loss=-2.925108\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=292, batch=5 train loss <loss>=-2.92510811488\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:14 INFO 140079012972352] Epoch[292] Batch [5]#011Speed: 182.08 samples/sec#011loss=-2.925108\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:16 INFO 140079012972352] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9755.31792640686, \"sum\": 9755.31792640686, \"min\": 9755.31792640686}}, \"EndTime\": 1612196716.912933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196707.156981}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:16 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.798585725 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:16 INFO 140079012972352] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:16 INFO 140079012972352] #quality_metric: host=algo-1, epoch=292, train loss <loss>=-3.01886589527\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:16 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:19 INFO 140079012972352] Epoch[293] Batch[0] avg_epoch_loss=-3.198601\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=293, batch=0 train loss <loss>=-3.19860100746\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:23 INFO 140079012972352] Epoch[293] Batch[5] avg_epoch_loss=-3.001410\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=293, batch=5 train loss <loss>=-3.00140976906\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:23 INFO 140079012972352] Epoch[293] Batch [5]#011Speed: 191.65 samples/sec#011loss=-3.001410\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:25 INFO 140079012972352] processed a total of 1276 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9047.319889068604, \"sum\": 9047.319889068604, \"min\": 9047.319889068604}}, \"EndTime\": 1612196725.961185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196716.913027}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:25 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.033685291 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:25 INFO 140079012972352] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:25 INFO 140079012972352] #quality_metric: host=algo-1, epoch=293, train loss <loss>=-2.9543956995\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:25 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:29 INFO 140079012972352] Epoch[294] Batch[0] avg_epoch_loss=-3.073823\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:29 INFO 140079012972352] #quality_metric: host=algo-1, epoch=294, batch=0 train loss <loss>=-3.07382321358\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:32 INFO 140079012972352] Epoch[294] Batch[5] avg_epoch_loss=-3.172286\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:32 INFO 140079012972352] #quality_metric: host=algo-1, epoch=294, batch=5 train loss <loss>=-3.17228551706\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:32 INFO 140079012972352] Epoch[294] Batch [5]#011Speed: 192.00 samples/sec#011loss=-3.172286\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:34 INFO 140079012972352] processed a total of 1246 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9004.426956176758, \"sum\": 9004.426956176758, \"min\": 9004.426956176758}}, \"EndTime\": 1612196734.966325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196725.961279}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:34 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.373382293 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:34 INFO 140079012972352] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=294, train loss <loss>=-3.1882168293\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:34 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:38 INFO 140079012972352] Epoch[295] Batch[0] avg_epoch_loss=-3.268867\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:38 INFO 140079012972352] #quality_metric: host=algo-1, epoch=295, batch=0 train loss <loss>=-3.26886701584\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:41 INFO 140079012972352] Epoch[295] Batch[5] avg_epoch_loss=-3.240631\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:41 INFO 140079012972352] #quality_metric: host=algo-1, epoch=295, batch=5 train loss <loss>=-3.24063118299\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:41 INFO 140079012972352] Epoch[295] Batch [5]#011Speed: 195.20 samples/sec#011loss=-3.240631\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:43 INFO 140079012972352] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8957.898139953613, \"sum\": 8957.898139953613, \"min\": 8957.898139953613}}, \"EndTime\": 1612196743.925114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196734.966476}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.437268452 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=295, train loss <loss>=-3.24586541653\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:46 INFO 140079012972352] Epoch[296] Batch[0] avg_epoch_loss=-3.088883\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=296, batch=0 train loss <loss>=-3.08888268471\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:50 INFO 140079012972352] Epoch[296] Batch[5] avg_epoch_loss=-3.215076\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=296, batch=5 train loss <loss>=-3.21507569154\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:50 INFO 140079012972352] Epoch[296] Batch [5]#011Speed: 197.61 samples/sec#011loss=-3.215076\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:52 INFO 140079012972352] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8925.453186035156, \"sum\": 8925.453186035156, \"min\": 8925.453186035156}}, \"EndTime\": 1612196752.851297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196743.925207}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:52 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=141.279382897 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:52 INFO 140079012972352] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=296, train loss <loss>=-3.29638915062\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:52 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:55 INFO 140079012972352] Epoch[297] Batch[0] avg_epoch_loss=-3.435744\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=297, batch=0 train loss <loss>=-3.43574357033\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:59 INFO 140079012972352] Epoch[297] Batch[5] avg_epoch_loss=-3.498371\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=297, batch=5 train loss <loss>=-3.49837060769\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:25:59 INFO 140079012972352] Epoch[297] Batch [5]#011Speed: 196.99 samples/sec#011loss=-3.498371\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] Epoch[297] Batch[10] avg_epoch_loss=-3.347400\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=297, batch=10 train loss <loss>=-3.16623535156\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] Epoch[297] Batch [10]#011Speed: 175.75 samples/sec#011loss=-3.166235\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] processed a total of 1349 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9994.296073913574, \"sum\": 9994.296073913574, \"min\": 9994.296073913574}}, \"EndTime\": 1612196762.846308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196752.851382}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.97488407 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=297, train loss <loss>=-3.34740003673\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:02 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:06 INFO 140079012972352] Epoch[298] Batch[0] avg_epoch_loss=-2.903218\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=298, batch=0 train loss <loss>=-2.90321779251\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:09 INFO 140079012972352] Epoch[298] Batch[5] avg_epoch_loss=-2.952234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=298, batch=5 train loss <loss>=-2.95223402977\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:09 INFO 140079012972352] Epoch[298] Batch [5]#011Speed: 197.46 samples/sec#011loss=-2.952234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] Epoch[298] Batch[10] avg_epoch_loss=-3.116460\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=298, batch=10 train loss <loss>=-3.31353225708\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] Epoch[298] Batch [10]#011Speed: 192.24 samples/sec#011loss=-3.313532\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10065.436840057373, \"sum\": 10065.436840057373, \"min\": 10065.436840057373}}, \"EndTime\": 1612196772.913504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196762.846415}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=127.265569764 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=298, train loss <loss>=-3.11646049673\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:12 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:15 INFO 140079012972352] Epoch[299] Batch[0] avg_epoch_loss=-2.959041\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=299, batch=0 train loss <loss>=-2.95904064178\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:19 INFO 140079012972352] Epoch[299] Batch[5] avg_epoch_loss=-3.019143\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:19 INFO 140079012972352] #quality_metric: host=algo-1, epoch=299, batch=5 train loss <loss>=-3.01914266745\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:19 INFO 140079012972352] Epoch[299] Batch [5]#011Speed: 194.86 samples/sec#011loss=-3.019143\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:21 INFO 140079012972352] processed a total of 1209 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8949.622869491577, \"sum\": 8949.622869491577, \"min\": 8949.622869491577}}, \"EndTime\": 1612196781.863778, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196772.913591}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:21 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.087238407 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:21 INFO 140079012972352] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=299, train loss <loss>=-3.13283092976\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:21 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:24 INFO 140079012972352] Epoch[300] Batch[0] avg_epoch_loss=-3.217437\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=300, batch=0 train loss <loss>=-3.21743655205\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:28 INFO 140079012972352] Epoch[300] Batch[5] avg_epoch_loss=-3.401563\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:28 INFO 140079012972352] #quality_metric: host=algo-1, epoch=300, batch=5 train loss <loss>=-3.40156344573\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:28 INFO 140079012972352] Epoch[300] Batch [5]#011Speed: 196.24 samples/sec#011loss=-3.401563\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] Epoch[300] Batch[10] avg_epoch_loss=-3.350055\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=300, batch=10 train loss <loss>=-3.2882437706\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] Epoch[300] Batch [10]#011Speed: 195.26 samples/sec#011loss=-3.288244\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9623.793125152588, \"sum\": 9623.793125152588, \"min\": 9623.793125152588}}, \"EndTime\": 1612196791.488268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196781.86388}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=138.197123524 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] #quality_metric: host=algo-1, epoch=300, train loss <loss>=-3.35005450249\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:31 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:34 INFO 140079012972352] Epoch[301] Batch[0] avg_epoch_loss=-2.755020\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:34 INFO 140079012972352] #quality_metric: host=algo-1, epoch=301, batch=0 train loss <loss>=-2.7550201416\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:37 INFO 140079012972352] Epoch[301] Batch[5] avg_epoch_loss=-2.752862\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:37 INFO 140079012972352] #quality_metric: host=algo-1, epoch=301, batch=5 train loss <loss>=-2.75286169847\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:37 INFO 140079012972352] Epoch[301] Batch [5]#011Speed: 194.87 samples/sec#011loss=-2.752862\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:40 INFO 140079012972352] processed a total of 1277 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8912.732124328613, \"sum\": 8912.732124328613, \"min\": 8912.732124328613}}, \"EndTime\": 1612196800.401722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196791.488367}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:40 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=143.275684544 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:40 INFO 140079012972352] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=301, train loss <loss>=-2.82824854851\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:40 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:43 INFO 140079012972352] Epoch[302] Batch[0] avg_epoch_loss=-3.302618\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=302, batch=0 train loss <loss>=-3.3026175499\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:46 INFO 140079012972352] Epoch[302] Batch[5] avg_epoch_loss=-3.116979\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=302, batch=5 train loss <loss>=-3.11697940032\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:46 INFO 140079012972352] Epoch[302] Batch [5]#011Speed: 196.19 samples/sec#011loss=-3.116979\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] Epoch[302] Batch[10] avg_epoch_loss=-3.240215\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=302, batch=10 train loss <loss>=-3.38809719086\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] Epoch[302] Batch [10]#011Speed: 198.36 samples/sec#011loss=-3.388097\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] processed a total of 1314 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9607.738971710205, \"sum\": 9607.738971710205, \"min\": 9607.738971710205}}, \"EndTime\": 1612196810.010172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196800.401826}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.762867902 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] #quality_metric: host=algo-1, epoch=302, train loss <loss>=-3.24021475965\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:50 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:53 INFO 140079012972352] Epoch[303] Batch[0] avg_epoch_loss=-3.278544\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:53 INFO 140079012972352] #quality_metric: host=algo-1, epoch=303, batch=0 train loss <loss>=-3.27854418755\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:56 INFO 140079012972352] Epoch[303] Batch[5] avg_epoch_loss=-3.077636\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=303, batch=5 train loss <loss>=-3.07763616244\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:56 INFO 140079012972352] Epoch[303] Batch [5]#011Speed: 198.79 samples/sec#011loss=-3.077636\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] Epoch[303] Batch[10] avg_epoch_loss=-3.170652\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=303, batch=10 train loss <loss>=-3.28227000237\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] Epoch[303] Batch [10]#011Speed: 192.68 samples/sec#011loss=-3.282270\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9641.72387123108, \"sum\": 9641.72387123108, \"min\": 9641.72387123108}}, \"EndTime\": 1612196819.652567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196810.010259}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.932371324 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=303, train loss <loss>=-3.17065154422\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:26:59 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:02 INFO 140079012972352] Epoch[304] Batch[0] avg_epoch_loss=-3.009997\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=304, batch=0 train loss <loss>=-3.00999712944\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:06 INFO 140079012972352] Epoch[304] Batch[5] avg_epoch_loss=-2.978963\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=304, batch=5 train loss <loss>=-2.97896297773\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:06 INFO 140079012972352] Epoch[304] Batch [5]#011Speed: 193.81 samples/sec#011loss=-2.978963\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:08 INFO 140079012972352] processed a total of 1267 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9320.478916168213, \"sum\": 9320.478916168213, \"min\": 9320.478916168213}}, \"EndTime\": 1612196828.973647, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196819.652666}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:08 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.934943175 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:08 INFO 140079012972352] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:08 INFO 140079012972352] #quality_metric: host=algo-1, epoch=304, train loss <loss>=-3.03741812706\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:08 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:12 INFO 140079012972352] Epoch[305] Batch[0] avg_epoch_loss=-3.096850\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:12 INFO 140079012972352] #quality_metric: host=algo-1, epoch=305, batch=0 train loss <loss>=-3.09684991837\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:15 INFO 140079012972352] Epoch[305] Batch[5] avg_epoch_loss=-3.357934\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=305, batch=5 train loss <loss>=-3.35793407758\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:15 INFO 140079012972352] Epoch[305] Batch [5]#011Speed: 197.51 samples/sec#011loss=-3.357934\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:18 INFO 140079012972352] processed a total of 1207 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9263.736963272095, \"sum\": 9263.736963272095, \"min\": 9263.736963272095}}, \"EndTime\": 1612196838.238406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196828.973754}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:18 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=130.291217466 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:18 INFO 140079012972352] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=305, train loss <loss>=-3.38331329823\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:18 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:21 INFO 140079012972352] Epoch[306] Batch[0] avg_epoch_loss=-3.071424\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=306, batch=0 train loss <loss>=-3.07142424583\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:24 INFO 140079012972352] Epoch[306] Batch[5] avg_epoch_loss=-2.999477\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=306, batch=5 train loss <loss>=-2.99947698911\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:24 INFO 140079012972352] Epoch[306] Batch [5]#011Speed: 194.82 samples/sec#011loss=-2.999477\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:27 INFO 140079012972352] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9086.477041244507, \"sum\": 9086.477041244507, \"min\": 9086.477041244507}}, \"EndTime\": 1612196847.325626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196838.238488}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:27 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.584027615 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:27 INFO 140079012972352] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=306, train loss <loss>=-3.03455069065\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:27 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:30 INFO 140079012972352] Epoch[307] Batch[0] avg_epoch_loss=-3.339235\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=307, batch=0 train loss <loss>=-3.33923482895\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:33 INFO 140079012972352] Epoch[307] Batch[5] avg_epoch_loss=-3.140582\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=307, batch=5 train loss <loss>=-3.14058236281\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:33 INFO 140079012972352] Epoch[307] Batch [5]#011Speed: 192.06 samples/sec#011loss=-3.140582\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] Epoch[307] Batch[10] avg_epoch_loss=-3.251989\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=307, batch=10 train loss <loss>=-3.38567709923\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] Epoch[307] Batch [10]#011Speed: 197.64 samples/sec#011loss=-3.385677\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] processed a total of 1283 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9613.165140151978, \"sum\": 9613.165140151978, \"min\": 9613.165140151978}}, \"EndTime\": 1612196856.939421, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196847.325721}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=133.460976179 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=307, train loss <loss>=-3.25198906118\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:36 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:40 INFO 140079012972352] Epoch[308] Batch[0] avg_epoch_loss=-2.820553\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:40 INFO 140079012972352] #quality_metric: host=algo-1, epoch=308, batch=0 train loss <loss>=-2.82055258751\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:43 INFO 140079012972352] Epoch[308] Batch[5] avg_epoch_loss=-2.795442\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=308, batch=5 train loss <loss>=-2.79544242223\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:43 INFO 140079012972352] Epoch[308] Batch [5]#011Speed: 193.48 samples/sec#011loss=-2.795442\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] Epoch[308] Batch[10] avg_epoch_loss=-2.711963\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=308, batch=10 train loss <loss>=-2.6117866993\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] Epoch[308] Batch [10]#011Speed: 198.43 samples/sec#011loss=-2.611787\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9640.522003173828, \"sum\": 9640.522003173828, \"min\": 9640.522003173828}}, \"EndTime\": 1612196866.580542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196856.939511}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.948959257 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=308, train loss <loss>=-2.71196254817\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:46 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:49 INFO 140079012972352] Epoch[309] Batch[0] avg_epoch_loss=-3.196832\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=309, batch=0 train loss <loss>=-3.19683241844\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:52 INFO 140079012972352] Epoch[309] Batch[5] avg_epoch_loss=-3.050166\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=309, batch=5 train loss <loss>=-3.05016577244\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:52 INFO 140079012972352] Epoch[309] Batch [5]#011Speed: 196.40 samples/sec#011loss=-3.050166\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] Epoch[309] Batch[10] avg_epoch_loss=-2.991993\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=309, batch=10 train loss <loss>=-2.92218484879\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] Epoch[309] Batch [10]#011Speed: 197.93 samples/sec#011loss=-2.922185\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9520.787954330444, \"sum\": 9520.787954330444, \"min\": 9520.787954330444}}, \"EndTime\": 1612196876.101971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196866.580636}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.856436042 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] #quality_metric: host=algo-1, epoch=309, train loss <loss>=-2.99199262532\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:56 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:59 INFO 140079012972352] Epoch[310] Batch[0] avg_epoch_loss=-2.473129\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:27:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=310, batch=0 train loss <loss>=-2.47312927246\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:02 INFO 140079012972352] Epoch[310] Batch[5] avg_epoch_loss=-2.764543\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=310, batch=5 train loss <loss>=-2.76454297702\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:02 INFO 140079012972352] Epoch[310] Batch [5]#011Speed: 186.09 samples/sec#011loss=-2.764543\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] Epoch[310] Batch[10] avg_epoch_loss=-2.832494\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=310, batch=10 train loss <loss>=-2.9140355587\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] Epoch[310] Batch [10]#011Speed: 156.62 samples/sec#011loss=-2.914036\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 10580.558061599731, \"sum\": 10580.558061599731, \"min\": 10580.558061599731}}, \"EndTime\": 1612196886.683218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196876.102067}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=124.849915778 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] #quality_metric: host=algo-1, epoch=310, train loss <loss>=-2.83249415051\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:06 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:09 INFO 140079012972352] Epoch[311] Batch[0] avg_epoch_loss=-2.896028\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=311, batch=0 train loss <loss>=-2.896027565\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:13 INFO 140079012972352] Epoch[311] Batch[5] avg_epoch_loss=-3.085551\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:13 INFO 140079012972352] #quality_metric: host=algo-1, epoch=311, batch=5 train loss <loss>=-3.08555070559\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:13 INFO 140079012972352] Epoch[311] Batch [5]#011Speed: 195.23 samples/sec#011loss=-3.085551\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:15 INFO 140079012972352] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8954.460859298706, \"sum\": 8954.460859298706, \"min\": 8954.460859298706}}, \"EndTime\": 1612196895.63832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196886.683317}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:15 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=140.262681495 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:15 INFO 140079012972352] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:15 INFO 140079012972352] #quality_metric: host=algo-1, epoch=311, train loss <loss>=-3.18834471703\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:15 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:18 INFO 140079012972352] Epoch[312] Batch[0] avg_epoch_loss=-3.192579\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:18 INFO 140079012972352] #quality_metric: host=algo-1, epoch=312, batch=0 train loss <loss>=-3.19257879257\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:21 INFO 140079012972352] Epoch[312] Batch[5] avg_epoch_loss=-3.278254\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:21 INFO 140079012972352] #quality_metric: host=algo-1, epoch=312, batch=5 train loss <loss>=-3.27825395266\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:21 INFO 140079012972352] Epoch[312] Batch [5]#011Speed: 198.45 samples/sec#011loss=-3.278254\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:24 INFO 140079012972352] processed a total of 1270 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8904.175996780396, \"sum\": 8904.175996780396, \"min\": 8904.175996780396}}, \"EndTime\": 1612196904.543335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196895.638438}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:24 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=142.627223139 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:24 INFO 140079012972352] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:24 INFO 140079012972352] #quality_metric: host=algo-1, epoch=312, train loss <loss>=-3.19616701603\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:24 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:27 INFO 140079012972352] Epoch[313] Batch[0] avg_epoch_loss=-3.359858\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:27 INFO 140079012972352] #quality_metric: host=algo-1, epoch=313, batch=0 train loss <loss>=-3.3598575592\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:30 INFO 140079012972352] Epoch[313] Batch[5] avg_epoch_loss=-3.173204\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=313, batch=5 train loss <loss>=-3.1732039849\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:30 INFO 140079012972352] Epoch[313] Batch [5]#011Speed: 195.51 samples/sec#011loss=-3.173204\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:33 INFO 140079012972352] processed a total of 1256 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8996.402978897095, \"sum\": 8996.402978897095, \"min\": 8996.402978897095}}, \"EndTime\": 1612196913.540388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196904.543439}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:33 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.60936706 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:33 INFO 140079012972352] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=313, train loss <loss>=-3.13195557594\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:33 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:36 INFO 140079012972352] Epoch[314] Batch[0] avg_epoch_loss=-3.206430\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=314, batch=0 train loss <loss>=-3.20642995834\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:39 INFO 140079012972352] Epoch[314] Batch[5] avg_epoch_loss=-3.135252\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=314, batch=5 train loss <loss>=-3.13525172075\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:39 INFO 140079012972352] Epoch[314] Batch [5]#011Speed: 195.41 samples/sec#011loss=-3.135252\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] Epoch[314] Batch[10] avg_epoch_loss=-3.283979\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=314, batch=10 train loss <loss>=-3.46245112419\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] Epoch[314] Batch [10]#011Speed: 194.86 samples/sec#011loss=-3.462451\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] processed a total of 1296 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9612.295866012573, \"sum\": 9612.295866012573, \"min\": 9612.295866012573}}, \"EndTime\": 1612196923.15338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196913.540474}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.825370966 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] #quality_metric: host=algo-1, epoch=314, train loss <loss>=-3.28397872231\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:43 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:46 INFO 140079012972352] Epoch[315] Batch[0] avg_epoch_loss=-2.949953\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:46 INFO 140079012972352] #quality_metric: host=algo-1, epoch=315, batch=0 train loss <loss>=-2.94995284081\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:49 INFO 140079012972352] Epoch[315] Batch[5] avg_epoch_loss=-2.876369\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:49 INFO 140079012972352] #quality_metric: host=algo-1, epoch=315, batch=5 train loss <loss>=-2.87636892001\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:49 INFO 140079012972352] Epoch[315] Batch [5]#011Speed: 194.56 samples/sec#011loss=-2.876369\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] Epoch[315] Batch[10] avg_epoch_loss=-2.952989\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=315, batch=10 train loss <loss>=-3.04493260384\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] Epoch[315] Batch [10]#011Speed: 197.08 samples/sec#011loss=-3.044933\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9586.409091949463, \"sum\": 9586.409091949463, \"min\": 9586.409091949463}}, \"EndTime\": 1612196932.740473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196923.153478}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=134.250099903 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] #quality_metric: host=algo-1, epoch=315, train loss <loss>=-2.95298877629\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:52 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:55 INFO 140079012972352] Epoch[316] Batch[0] avg_epoch_loss=-2.107148\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:55 INFO 140079012972352] #quality_metric: host=algo-1, epoch=316, batch=0 train loss <loss>=-2.10714793205\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:59 INFO 140079012972352] Epoch[316] Batch[5] avg_epoch_loss=-2.153075\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:59 INFO 140079012972352] #quality_metric: host=algo-1, epoch=316, batch=5 train loss <loss>=-2.15307486057\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:28:59 INFO 140079012972352] Epoch[316] Batch [5]#011Speed: 197.01 samples/sec#011loss=-2.153075\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] Epoch[316] Batch[10] avg_epoch_loss=-2.391108\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=316, batch=10 train loss <loss>=-2.67674746513\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] Epoch[316] Batch [10]#011Speed: 185.83 samples/sec#011loss=-2.676747\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9729.712009429932, \"sum\": 9729.712009429932, \"min\": 9729.712009429932}}, \"EndTime\": 1612196942.471253, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196932.74057}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.692023828 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] #quality_metric: host=algo-1, epoch=316, train loss <loss>=-2.39110786265\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:02 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:05 INFO 140079012972352] Epoch[317] Batch[0] avg_epoch_loss=-2.862561\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:05 INFO 140079012972352] #quality_metric: host=algo-1, epoch=317, batch=0 train loss <loss>=-2.86256098747\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:09 INFO 140079012972352] Epoch[317] Batch[5] avg_epoch_loss=-2.749639\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:09 INFO 140079012972352] #quality_metric: host=algo-1, epoch=317, batch=5 train loss <loss>=-2.74963875612\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:09 INFO 140079012972352] Epoch[317] Batch [5]#011Speed: 198.46 samples/sec#011loss=-2.749639\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:11 INFO 140079012972352] processed a total of 1235 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9321.110010147095, \"sum\": 9321.110010147095, \"min\": 9321.110010147095}}, \"EndTime\": 1612196951.793114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196942.471393}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:11 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=132.4930449 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:11 INFO 140079012972352] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:11 INFO 140079012972352] #quality_metric: host=algo-1, epoch=317, train loss <loss>=-2.73341457844\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:11 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:14 INFO 140079012972352] Epoch[318] Batch[0] avg_epoch_loss=-2.679016\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:14 INFO 140079012972352] #quality_metric: host=algo-1, epoch=318, batch=0 train loss <loss>=-2.6790163517\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:17 INFO 140079012972352] Epoch[318] Batch[5] avg_epoch_loss=-2.879517\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:17 INFO 140079012972352] #quality_metric: host=algo-1, epoch=318, batch=5 train loss <loss>=-2.87951747576\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:17 INFO 140079012972352] Epoch[318] Batch [5]#011Speed: 199.07 samples/sec#011loss=-2.879517\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:20 INFO 140079012972352] processed a total of 1220 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8775.596141815186, \"sum\": 8775.596141815186, \"min\": 8775.596141815186}}, \"EndTime\": 1612196960.569696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196951.7932}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:20 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=139.019601786 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:20 INFO 140079012972352] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:20 INFO 140079012972352] #quality_metric: host=algo-1, epoch=318, train loss <loss>=-3.10407631397\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:20 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:23 INFO 140079012972352] Epoch[319] Batch[0] avg_epoch_loss=-3.130264\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:23 INFO 140079012972352] #quality_metric: host=algo-1, epoch=319, batch=0 train loss <loss>=-3.13026380539\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:26 INFO 140079012972352] Epoch[319] Batch[5] avg_epoch_loss=-3.389851\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:26 INFO 140079012972352] #quality_metric: host=algo-1, epoch=319, batch=5 train loss <loss>=-3.38985109329\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:26 INFO 140079012972352] Epoch[319] Batch [5]#011Speed: 197.09 samples/sec#011loss=-3.389851\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] Epoch[319] Batch[10] avg_epoch_loss=-3.256337\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=319, batch=10 train loss <loss>=-3.09611961842\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] Epoch[319] Batch [10]#011Speed: 197.73 samples/sec#011loss=-3.096120\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9499.207019805908, \"sum\": 9499.207019805908, \"min\": 9499.207019805908}}, \"EndTime\": 1612196970.069584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196960.569792}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.904249416 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] #quality_metric: host=algo-1, epoch=319, train loss <loss>=-3.25633678653\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:30 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:33 INFO 140079012972352] Epoch[320] Batch[0] avg_epoch_loss=-2.385594\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:33 INFO 140079012972352] #quality_metric: host=algo-1, epoch=320, batch=0 train loss <loss>=-2.38559436798\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:36 INFO 140079012972352] Epoch[320] Batch[5] avg_epoch_loss=-2.646173\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:36 INFO 140079012972352] #quality_metric: host=algo-1, epoch=320, batch=5 train loss <loss>=-2.64617335796\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:36 INFO 140079012972352] Epoch[320] Batch [5]#011Speed: 193.76 samples/sec#011loss=-2.646173\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:39 INFO 140079012972352] processed a total of 1207 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8939.52202796936, \"sum\": 8939.52202796936, \"min\": 8939.52202796936}}, \"EndTime\": 1612196979.009767, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196970.069672}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:39 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=135.01640946 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:39 INFO 140079012972352] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:39 INFO 140079012972352] #quality_metric: host=algo-1, epoch=320, train loss <loss>=-2.76506347656\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:39 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:42 INFO 140079012972352] Epoch[321] Batch[0] avg_epoch_loss=-2.835460\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:42 INFO 140079012972352] #quality_metric: host=algo-1, epoch=321, batch=0 train loss <loss>=-2.835460186\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:45 INFO 140079012972352] Epoch[321] Batch[5] avg_epoch_loss=-3.037260\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:45 INFO 140079012972352] #quality_metric: host=algo-1, epoch=321, batch=5 train loss <loss>=-3.03726037343\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:45 INFO 140079012972352] Epoch[321] Batch [5]#011Speed: 198.65 samples/sec#011loss=-3.037260\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] Epoch[321] Batch[10] avg_epoch_loss=-3.031297\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=321, batch=10 train loss <loss>=-3.02414050102\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] Epoch[321] Batch [10]#011Speed: 197.30 samples/sec#011loss=-3.024141\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] processed a total of 1302 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9533.189058303833, \"sum\": 9533.189058303833, \"min\": 9533.189058303833}}, \"EndTime\": 1612196988.543663, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196979.009857}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.573532911 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] #quality_metric: host=algo-1, epoch=321, train loss <loss>=-3.03129679506\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:48 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:51 INFO 140079012972352] Epoch[322] Batch[0] avg_epoch_loss=-3.405909\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:51 INFO 140079012972352] #quality_metric: host=algo-1, epoch=322, batch=0 train loss <loss>=-3.40590906143\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:54 INFO 140079012972352] Epoch[322] Batch[5] avg_epoch_loss=-3.531305\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:54 INFO 140079012972352] #quality_metric: host=algo-1, epoch=322, batch=5 train loss <loss>=-3.53130527337\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:54 INFO 140079012972352] Epoch[322] Batch [5]#011Speed: 198.15 samples/sec#011loss=-3.531305\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:57 INFO 140079012972352] processed a total of 1208 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 8881.567001342773, \"sum\": 8881.567001342773, \"min\": 8881.567001342773}}, \"EndTime\": 1612196997.425812, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196988.543759}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:57 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=136.010012094 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:57 INFO 140079012972352] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:57 INFO 140079012972352] #quality_metric: host=algo-1, epoch=322, train loss <loss>=-3.47745246887\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:29:57 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:00 INFO 140079012972352] Epoch[323] Batch[0] avg_epoch_loss=-3.593741\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:00 INFO 140079012972352] #quality_metric: host=algo-1, epoch=323, batch=0 train loss <loss>=-3.59374117851\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:04 INFO 140079012972352] Epoch[323] Batch[5] avg_epoch_loss=-2.831684\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:04 INFO 140079012972352] #quality_metric: host=algo-1, epoch=323, batch=5 train loss <loss>=-2.83168367545\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:04 INFO 140079012972352] Epoch[323] Batch [5]#011Speed: 160.58 samples/sec#011loss=-2.831684\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] processed a total of 1242 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 9984.15994644165, \"sum\": 9984.15994644165, \"min\": 9984.15994644165}}, \"EndTime\": 1612197007.410593, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612196997.425902}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] #throughput_metric: host=algo-1, train throughput=124.394951375 records/second\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] #quality_metric: host=algo-1, epoch=323, train loss <loss>=-2.79796075821\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] Loading parameters from best epoch (223)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 14.586925506591797, \"sum\": 14.586925506591797, \"min\": 14.586925506591797}}, \"EndTime\": 1612197007.426065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612197007.410716}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] stopping training now\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] Final loss: -3.57149371234 (occurred at epoch 223)\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] #quality_metric: host=algo-1, train final_loss <loss>=-3.57149371234\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 WARNING 140079012972352] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 251.15609169006348, \"sum\": 251.15609169006348, \"min\": 251.15609169006348}}, \"EndTime\": 1612197007.678207, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612197007.426132}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 360.8379364013672, \"sum\": 360.8379364013672, \"min\": 360.8379364013672}}, \"EndTime\": 1612197007.787805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612197007.678295}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 18.83697509765625, \"sum\": 18.83697509765625, \"min\": 18.83697509765625}}, \"EndTime\": 1612197007.806789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612197007.787898}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[02/01/2021 16:30:07 INFO 140079012972352] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 3047063.8649463654, \"sum\": 3047063.8649463654, \"min\": 3047063.8649463654}, \"setuptime\": {\"count\": 1, \"max\": 10.701894760131836, \"sum\": 10.701894760131836, \"min\": 10.701894760131836}}, \"EndTime\": 1612197007.833453, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612197007.806849}\n",
      "\u001b[0m\n",
      "\n",
      "2021-02-01 16:30:21 Uploading - Uploading generated training model\n",
      "2021-02-01 16:30:21 Completed - Training job completed\n",
      "Training seconds: 3118\n",
      "Billable seconds: 3118\n"
     ]
    }
   ],
   "source": [
    "container = get_image_uri(boto3.Session().region_name,'forecasting-deepar')\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container,\n",
    "                      role=role,   \n",
    "                      train_instance_count=1, \n",
    "                      train_instance_type='ml.m4.xlarge', \n",
    "                      output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                      sagemaker_session=session)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": \"500\",\n",
    "    \"time_freq\": freq,\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"context_length\": str(context_length),\n",
    "    \"num_cells\": \"100\",\n",
    "    \"num_layers\": \"4\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"0.01\",\n",
    "    \"early_stopping_patience\": \"20\"\n",
    "}\n",
    "estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "estimator.fit({'train': train_location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Using already existing model: forecasting-deepar-2021-02-01-15-36-14-909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predicting on Test Set </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(features, cutoff, days, feat_cols, target, num_samples=50, quantiles=['0.1', '0.5', '0.9']):\n",
    "    instances = []\n",
    "    symbols = features['sym'].unique()\n",
    "    \n",
    "    for idx, sym in enumerate(symbols):\n",
    "        look_back_date = cutoff - pd.Timedelta(days, 'D')\n",
    "        window = features.query(\"sym == @sym & time > @look_back_date & time <= @cutoff\")\n",
    "        \n",
    "        json_obj = {\"start\": str(list(window['time'])[0]), \n",
    "                    \"target\": list(window[TARGET])[:-1],\n",
    "                    \"cat\":[idx], \n",
    "                    \"dynamic_feat\":[list(window[column]) for column in feat_cols]}\n",
    "        instances.append(json_obj)\n",
    "            \n",
    "#     configuration = {\"num_samples\": num_samples, \n",
    "#                      \"output_types\": [\"quantiles\"], \n",
    "#                      \"quantiles\": quantiles}\n",
    "    configuration = {\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"mean\"]}\n",
    "\n",
    "    request_data = {\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "\n",
    "    json_request = json.dumps(request_data).encode('utf-8')\n",
    "    \n",
    "    return json_request\n",
    "\n",
    "def decode_prediction(prediction, encoding='utf-8'):\n",
    "    '''Accepts a JSON prediction and returns a list of prediction data.\n",
    "    '''\n",
    "    prediction_data = json.loads(prediction.decode(encoding))\n",
    "    prediction_list = []\n",
    "    for k in range(len(prediction_data['predictions'])):\n",
    "        prediction_list.append(pd.DataFrame(data=prediction_data['predictions'][k]['quantiles']))\n",
    "    return prediction_list\n",
    "\n",
    "def loop_predict(features_df, cutoff, days, feat_cols, TARGET):\n",
    "    dates = list(set(features_df[features_df.time >= cutoff]['time']))\n",
    "    \n",
    "    df = pd.DataFrame([])\n",
    "    for date in dates:\n",
    "        test_features = json_predictor_input(features_df, date, context_length, feat_cols, TARGET, quantiles=['0.5'])\n",
    "        json_prediction = predictor.predict(test_features)\n",
    "        pred = [x['0.5'].values[0] for x in decode_prediction(json_prediction)]\n",
    "        temp_df = pd.DataFrame(zip([sym for sym in features_df['sym'].unique()], pred), columns=['sym', 'pred'])\n",
    "        temp_df['time'] = date\n",
    "        \n",
    "        df = df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = loop_predict(test_scaled, TEST_START, context_length, feat_columns, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test.copy()\n",
    "test_result = pd.merge(test_result, pred_Y, how='left', left_on=['time', 'sym'], right_on=['time', 'sym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = test_scaled[TARGET+'_mean'].reset_index(drop=True)\n",
    "pred_std = test_scaled[TARGET+'_std'].reset_index(drop=True)\n",
    "test_result['pred'] = (test_result['pred'] * pred_std) + pred_mean\n",
    "test_result = test_result.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Compute mean absolute percentage error (MAPE)\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.10392781500582"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mape(test_result[TARGET], test_result['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "test_result[test_result['sym'] == 'ETH']['pred'].plot(label='Prediction', lw=1)\n",
    "test_result[test_result['sym'] == 'ETH'][TARGET].plot(label='Actual', lw=1)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predicting on Train Set </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = loop_predict(train_scaled, TRAIN_START, context_length, feat_columns, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = train.copy()\n",
    "train_result = pd.merge(train_result, pred_Y, how='left', left_on=['time', 'sym'], right_on=['time', 'sym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = train[TARGET+'_mean'].reset_index(drop=True)\n",
    "pred_std = train[TARGET+'_std'].reset_index(drop=True)\n",
    "train_result['pred'] = (train_result['pred'] * math.sqrt(scaler_train.var_[-1])) + scaler_train.mean_[-1]\n",
    "# train_result['pred'] = (train_result['pred'] * pred_std) + pred_mean\n",
    "train_result = train_result.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "train_result[train_result['sym'] == 'ETH']['pred'].plot(label='Prediction', lw=1)\n",
    "train_result[train_result['sym'] == 'ETH']['price'].plot(label='Actual', lw=1)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Trading </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mean(mean, t, new_value):\n",
    "    if t == 0:\n",
    "        return new_value\n",
    "    else:\n",
    "        return (mean * (t - 1) + new_value) / t\n",
    "\n",
    "# Function to update standard deviation based on new value\n",
    "def update_std(std, mean, new_mean, t, new_value):\n",
    "    if t == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sqrt((std ** 2 * (t - 1) + (new_value - new_mean) * (new_value - mean)) / t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute the trading strategy over the trading horizon using asset matrix\n",
    "def evaluate_strategy(result, n=10, initial_value=10000):\n",
    "    total_value = initial_value\n",
    "    mean_roi = 0\n",
    "    std_roi = 0\n",
    "    \n",
    "    value_history = []\n",
    "    roi_history = []\n",
    "    \n",
    "    t = 1\n",
    "\n",
    "    dates = list(set(result.index))\n",
    "    dates.sort()\n",
    "    \n",
    "    df = result.copy()\n",
    "    df['predicted_roi'] = (df['pred']/df['price_lag_1']) - 1\n",
    "    df.sort_values(by='predicted_roi', ascending=False, inplace=True)\n",
    "\n",
    "    for date in dates:\n",
    "        temp_df = df.query('time == @date & predicted_roi > 0')\n",
    "\n",
    "        if not temp_df.empty:\n",
    "            top_n = temp_df.nlargest(n, 'predicted_roi')\n",
    "            selected_n = len(top_n)\n",
    "            day_return = sum(top_n['roi'] * total_value / selected_n)\n",
    "            day_roi = day_return/total_value\n",
    "        else:\n",
    "            day_return = 0\n",
    "            day_roi = 0\n",
    "        \n",
    "        total_value += day_return\n",
    "        percent_returns = (total_value/initial_value - 1) * 100\n",
    "\n",
    "        prev_mean_roi = mean_roi\n",
    "        mean_roi = update_mean(prev_mean_roi, t, day_roi)\n",
    "        std_roi = update_std(std_roi, prev_mean_roi, mean_roi, t, day_roi)\n",
    "        sharpe_ratio = mean_roi/std_roi\n",
    "        \n",
    "        value_history.append(total_value)\n",
    "        roi_history.append(day_roi)\n",
    "\n",
    "        t += 1\n",
    "        \n",
    "    print('Cumulative Returns: {:.2e}%'.format(percent_returns))\n",
    "    \n",
    "    history = pd.concat([pd.DataFrame(dates), pd.DataFrame(value_history), pd.DataFrame(roi_history)], axis=1)\n",
    "    history.columns = ['time', 'total_value', 'roi']\n",
    "    history = history.set_index('time')\n",
    "    return sharpe_ratio, percent_returns, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Returns: -4.44e+01%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.07614026886487985,\n",
       " -44.4133779044955,\n",
       "              total_value       roi\n",
       " time                              \n",
       " 2017-10-25  10000.000000  0.000000\n",
       " 2017-10-26  10000.000000  0.000000\n",
       " 2017-10-27  10037.799528  0.003780\n",
       " 2017-10-28  10037.799528  0.000000\n",
       " 2017-10-29   9951.039018 -0.008643\n",
       " ...                  ...       ...\n",
       " 2018-04-07   5558.662210 -0.037952\n",
       " 2018-04-08   5558.662210  0.000000\n",
       " 2018-04-09   5558.662210  0.000000\n",
       " 2018-04-10   5558.662210  0.000000\n",
       " 2018-04-11   5558.662210  0.000000\n",
       " \n",
       " [169 rows x 2 columns])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_strategy(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
