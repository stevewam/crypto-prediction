{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.4)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.4.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.7)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.16.37)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.37)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.14.12->sagemaker==1.72.0) (1.25.11)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from botocore<1.20.0,>=1.19.37->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.14.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.37 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.19.37)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 21.0 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import source\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_boston\n",
    "import sklearn.model_selection\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.predictor import csv_serializer\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './sound/beep.wav'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Processing </h2>\n",
    "\n",
    "ROI is calculated using next price because we are trying to predict future ROI using the data from current time period.\n",
    "Referred to https://github.com/NGYB/Stocks/blob/master/StockPricePrediction/StockPricePrediction_v1c_xgboost.ipynb & https://towardsdatascience.com/cryptocurrency-price-prediction-using-lstms-tensorflow-for-hackers-part-iii-264fcdbccd3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(os.path.join(DATA_DIR, 'crypto-historical-data.csv'), \n",
    "                   parse_dates=['time'], \n",
    "                   index_col=0, \n",
    "                   keep_default_na=False,\n",
    "                   header=0,\n",
    "                   names=['market_cap', 'name', 'price', 'sym', 'time', 'volume'])\n",
    "\n",
    "data.sort_values(by=['sym', 'time'], inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rank'] = data.groupby(\"time\")[\"market_cap\"] \\\n",
    "                    .rank(\"dense\", ascending=False) \\\n",
    "                    .astype(int)\n",
    "\n",
    "data['market_share'] = data.groupby('time')[\"market_cap\"] \\\n",
    "                    .apply(lambda x: x/float(x.sum()))\n",
    "\n",
    "data['age'] = data.groupby(['sym'])[\"time\"] \\\n",
    "                    .apply(lambda x: x - min(x)) \\\n",
    "                    .dt.days + 1\n",
    "\n",
    "previous_price = data.groupby(['sym'])['price'].shift(-1)\n",
    "data['roi'] = data['price']/previous_price - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>sym</th>\n",
       "      <th>time</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>224231</th>\n",
       "      <td>167911000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.753325</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-08</td>\n",
       "      <td>674188.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.034732</td>\n",
       "      <td>1</td>\n",
       "      <td>0.073270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224232</th>\n",
       "      <td>42637600.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.701897</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-09</td>\n",
       "      <td>532170.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224233</th>\n",
       "      <td>43130000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.708448</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-10</td>\n",
       "      <td>405283.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009581</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.337899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224234</th>\n",
       "      <td>42796500.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-11</td>\n",
       "      <td>1463100.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.122951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224235</th>\n",
       "      <td>64018400.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>2150620.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224236</th>\n",
       "      <td>73935400.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-13</td>\n",
       "      <td>4068680.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016312</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224237</th>\n",
       "      <td>109594000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.830000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-14</td>\n",
       "      <td>4637030.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>7</td>\n",
       "      <td>0.082840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224238</th>\n",
       "      <td>109160000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.690000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>2554360.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.023926</td>\n",
       "      <td>8</td>\n",
       "      <td>0.076433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224239</th>\n",
       "      <td>102028000.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.570000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-16</td>\n",
       "      <td>3550790.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.022676</td>\n",
       "      <td>9</td>\n",
       "      <td>0.308333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224240</th>\n",
       "      <td>95819700.0</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>1942830.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.021582</td>\n",
       "      <td>10</td>\n",
       "      <td>0.100917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         market_cap      name     price  sym       time     volume  rank  \\\n",
       "224231  167911000.0  Ethereum  0.753325  ETH 2015-08-08   674188.0     4   \n",
       "224232   42637600.0  Ethereum  0.701897  ETH 2015-08-09   532170.0     4   \n",
       "224233   43130000.0  Ethereum  0.708448  ETH 2015-08-10   405283.0     4   \n",
       "224234   42796500.0  Ethereum  1.070000  ETH 2015-08-11  1463100.0     4   \n",
       "224235   64018400.0  Ethereum  1.220000  ETH 2015-08-12  2150620.0     4   \n",
       "224236   73935400.0  Ethereum  1.830000  ETH 2015-08-13  4068680.0     4   \n",
       "224237  109594000.0  Ethereum  1.830000  ETH 2015-08-14  4637030.0     4   \n",
       "224238  109160000.0  Ethereum  1.690000  ETH 2015-08-15  2554360.0     4   \n",
       "224239  102028000.0  Ethereum  1.570000  ETH 2015-08-16  3550790.0     4   \n",
       "224240   95819700.0  Ethereum  1.200000  ETH 2015-08-17  1942830.0     4   \n",
       "\n",
       "        market_share  age       roi  \n",
       "224231      0.034732    1  0.073270  \n",
       "224232      0.009641    2 -0.009247  \n",
       "224233      0.009581    3 -0.337899  \n",
       "224234      0.009541    4 -0.122951  \n",
       "224235      0.013904    5 -0.333333  \n",
       "224236      0.016312    6  0.000000  \n",
       "224237      0.024201    7  0.082840  \n",
       "224238      0.023926    8  0.076433  \n",
       "224239      0.022676    9  0.308333  \n",
       "224240      0.021582   10  0.100917  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['sym'] == 'ETH'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Selection </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START = min(data['time'])\n",
    "VAL_START = pd.Timestamp('2016-04-25')\n",
    "TEST_START = pd.Timestamp('2017-04-25')\n",
    "END_TRAIN = TEST_START - pd.Timedelta(1, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2013-04-28 00:00:00')"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2017-04-24 00:00:00')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "END_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window = data.query('time < @VAL_START')\n",
    "mean_daily = train_window.groupby(['sym']).mean()\n",
    "SYMBOLS = mean_daily.query(\"market_cap > 1000000 & volume > 10000\").index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = data.query('sym in @SYMBOLS')\n",
    "# filtered = filtered.query('sym == \"ETH\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.sort_values(by=['sym', 'time'], inplace=True)\n",
    "filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Feature Engineering </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 7\n",
    "TARGET = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = ['market_cap', 'price', 'volume', 'rank', 'market_share', 'age', 'roi']\n",
    "# properties = ['market_cap', 'price', 'volume', 'age', 'roi']\n",
    "feat_columns = []\n",
    "\n",
    "for p in properties:\n",
    "    for w in range(1, W+1):\n",
    "        col_name = \"{}_lag_{}\".format(p, w)\n",
    "        feat[col_name] = feat.groupby(['sym'])[p].shift(w)\n",
    "        feat_columns.append(col_name)\n",
    "    \n",
    "    feat[p + '_mean'] = feat.groupby(['sym'])[p].shift(1) \\\n",
    "                                .transform(lambda x: x.rolling(W, min_periods=1).mean())\n",
    "    feat[p + '_std'] = feat.groupby(['sym'])[p].shift(1) \\\n",
    "                                .transform(lambda x: x.rolling(W, min_periods=1).std())\n",
    "\n",
    "feat.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>sym</th>\n",
       "      <th>time</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "      <th>...</th>\n",
       "      <th>age_std</th>\n",
       "      <th>roi_lag_1</th>\n",
       "      <th>roi_lag_2</th>\n",
       "      <th>roi_lag_3</th>\n",
       "      <th>roi_lag_4</th>\n",
       "      <th>roi_lag_5</th>\n",
       "      <th>roi_lag_6</th>\n",
       "      <th>roi_lag_7</th>\n",
       "      <th>roi_mean</th>\n",
       "      <th>roi_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>822920.0</td>\n",
       "      <td>Synereo</td>\n",
       "      <td>0.004448</td>\n",
       "      <td>AMP</td>\n",
       "      <td>2015-12-25</td>\n",
       "      <td>2.0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>...</td>\n",
       "      <td>2.690371</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.066682</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>-0.063481</td>\n",
       "      <td>-0.110180</td>\n",
       "      <td>0.259629</td>\n",
       "      <td>-0.210646</td>\n",
       "      <td>-0.025460</td>\n",
       "      <td>0.146058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>834549.0</td>\n",
       "      <td>Synereo</td>\n",
       "      <td>0.004540</td>\n",
       "      <td>AMP</td>\n",
       "      <td>2015-12-26</td>\n",
       "      <td>18.0</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.013043</td>\n",
       "      <td>...</td>\n",
       "      <td>2.992053</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.066682</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>-0.063481</td>\n",
       "      <td>-0.110180</td>\n",
       "      <td>0.259629</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.121486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>837453.0</td>\n",
       "      <td>Synereo</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>AMP</td>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>26.0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>...</td>\n",
       "      <td>3.132016</td>\n",
       "      <td>-0.013043</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.066682</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>-0.063481</td>\n",
       "      <td>-0.110180</td>\n",
       "      <td>-0.037216</td>\n",
       "      <td>0.044049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>851183.0</td>\n",
       "      <td>Synereo</td>\n",
       "      <td>0.004611</td>\n",
       "      <td>AMP</td>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>23.0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>13</td>\n",
       "      <td>0.064896</td>\n",
       "      <td>...</td>\n",
       "      <td>3.132016</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.013043</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.066682</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>-0.063481</td>\n",
       "      <td>-0.021817</td>\n",
       "      <td>0.031283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>864300.0</td>\n",
       "      <td>Synereo</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>AMP</td>\n",
       "      <td>2015-12-29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.296735</td>\n",
       "      <td>...</td>\n",
       "      <td>2.992053</td>\n",
       "      <td>0.064896</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>-0.013043</td>\n",
       "      <td>-0.020264</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>-0.066682</td>\n",
       "      <td>0.011787</td>\n",
       "      <td>-0.003478</td>\n",
       "      <td>0.039371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    market_cap     name     price  sym       time  volume  rank  market_share  \\\n",
       "7     822920.0  Synereo  0.004448  AMP 2015-12-25     2.0    37      0.000111   \n",
       "8     834549.0  Synereo  0.004540  AMP 2015-12-26    18.0    38      0.000112   \n",
       "9     837453.0  Synereo  0.004600  AMP 2015-12-27    26.0    36      0.000123   \n",
       "10    851183.0  Synereo  0.004611  AMP 2015-12-28    23.0    37      0.000123   \n",
       "11    864300.0  Synereo  0.004330  AMP 2015-12-29     1.0    37      0.000125   \n",
       "\n",
       "    age       roi  ...   age_std  roi_lag_1  roi_lag_2  roi_lag_3  roi_lag_4  \\\n",
       "7    10 -0.020264  ...  2.690371   0.001349  -0.066682   0.011787  -0.063481   \n",
       "8    11 -0.013043  ...  2.992053  -0.020264   0.001349  -0.066682   0.011787   \n",
       "9    12 -0.002386  ...  3.132016  -0.013043  -0.020264   0.001349  -0.066682   \n",
       "10   13  0.064896  ...  3.132016  -0.002386  -0.013043  -0.020264   0.001349   \n",
       "11   14 -0.296735  ...  2.992053   0.064896  -0.002386  -0.013043  -0.020264   \n",
       "\n",
       "    roi_lag_5  roi_lag_6  roi_lag_7  roi_mean   roi_std  \n",
       "7   -0.110180   0.259629  -0.210646 -0.025460  0.146058  \n",
       "8   -0.063481  -0.110180   0.259629  0.001737  0.121486  \n",
       "9    0.011787  -0.063481  -0.110180 -0.037216  0.044049  \n",
       "10  -0.066682   0.011787  -0.063481 -0.021817  0.031283  \n",
       "11   0.001349  -0.066682   0.011787 -0.003478  0.039371  \n",
       "\n",
       "[5 rows x 73 columns]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Data Split </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. scale train based on symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feat.query(\"time < @VAL_START\")\n",
    "val = feat.query(\"time >= @VAL_START & time < @TEST_START\")\n",
    "trainval = feat.query(\"time <= @TEST_START\")\n",
    "test = feat.query(\"time >= @TEST_START\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = feat_columns + [TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_dict = {}\n",
    "# train_scaled = pd.DataFrame()\n",
    "\n",
    "# for sym in SYMBOLS:\n",
    "#     scaler_train = StandardScaler()\n",
    "#     sym_train = train.query(\"sym == @sym\")\n",
    "#     sym_train[cols_to_scale] = scaler_train.fit_transform(sym_train[cols_to_scale])\n",
    "    \n",
    "#     train_scaled = pd.concat([train_scaled, sym_train], axis=0)\n",
    "    \n",
    "#     scaler_dict[sym] = scaler_train\n",
    "\n",
    "# train_scaled.sort_values(by=['sym', 'time'], inplace=True)\n",
    "# train_scaled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_col(df, base, col):\n",
    "    mean = df[base + '_mean']\n",
    "    std = df[base + '_std']\n",
    "    std = np.where(std == 0, 0.001, std)\n",
    "    return (df[col] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_scaled = trainval.copy()\n",
    "for p in properties:\n",
    "    trainval_scaled[p] = scale_col(trainval_scaled, p, p)\n",
    "    for w in range(1, W+1):\n",
    "        col_name = \"{}_lag_{}\".format(p, w)\n",
    "        trainval_scaled[col_name] = scale_col(trainval_scaled, p, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scaled = val.copy()\n",
    "for p in properties:\n",
    "    val_scaled[p] = scale_col(val_scaled, p, p)\n",
    "    for w in range(1, W+1):\n",
    "        col_name = \"{}_lag_{}\".format(p, w)\n",
    "        val_scaled[col_name] = scale_col(val_scaled, p, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = test.copy()\n",
    "for p in properties:\n",
    "    test_scaled[p] = scale_col(test_scaled, p, p)\n",
    "    \n",
    "    for w in range(1, W+1):\n",
    "        col_name = \"{}_lag_{}\".format(p, w)\n",
    "        test_scaled[col_name] = scale_col(test_scaled, p, col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>price</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "      <th>market_cap_lag_1</th>\n",
       "      <th>market_cap_lag_2</th>\n",
       "      <th>market_cap_lag_3</th>\n",
       "      <th>...</th>\n",
       "      <th>age_std</th>\n",
       "      <th>roi_lag_1</th>\n",
       "      <th>roi_lag_2</th>\n",
       "      <th>roi_lag_3</th>\n",
       "      <th>roi_lag_4</th>\n",
       "      <th>roi_lag_5</th>\n",
       "      <th>roi_lag_6</th>\n",
       "      <th>roi_lag_7</th>\n",
       "      <th>roi_mean</th>\n",
       "      <th>roi_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13007.000000</td>\n",
       "      <td>1.300700e+04</td>\n",
       "      <td>1.300700e+04</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "      <td>13007.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.270739</td>\n",
       "      <td>1.679970e-01</td>\n",
       "      <td>6.092668e+03</td>\n",
       "      <td>66.622537</td>\n",
       "      <td>-0.129323</td>\n",
       "      <td>1.851912</td>\n",
       "      <td>-0.000730</td>\n",
       "      <td>0.048943</td>\n",
       "      <td>0.017197</td>\n",
       "      <td>-0.002063</td>\n",
       "      <td>...</td>\n",
       "      <td>2.163436</td>\n",
       "      <td>-0.008858</td>\n",
       "      <td>-0.004928</td>\n",
       "      <td>-0.004355</td>\n",
       "      <td>-0.004699</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.006045</td>\n",
       "      <td>0.109214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.585780</td>\n",
       "      <td>2.021850e+00</td>\n",
       "      <td>2.851716e+05</td>\n",
       "      <td>3129.445036</td>\n",
       "      <td>2.237954</td>\n",
       "      <td>0.059661</td>\n",
       "      <td>1.567497</td>\n",
       "      <td>1.093715</td>\n",
       "      <td>0.865874</td>\n",
       "      <td>0.774281</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134888</td>\n",
       "      <td>0.922142</td>\n",
       "      <td>0.922319</td>\n",
       "      <td>0.926532</td>\n",
       "      <td>0.925193</td>\n",
       "      <td>0.927509</td>\n",
       "      <td>0.929121</td>\n",
       "      <td>0.927949</td>\n",
       "      <td>0.058275</td>\n",
       "      <td>0.131429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-18.753049</td>\n",
       "      <td>-1.115415e+01</td>\n",
       "      <td>-7.922857e+05</td>\n",
       "      <td>-89426.540288</td>\n",
       "      <td>-12.715254</td>\n",
       "      <td>0.878310</td>\n",
       "      <td>-17.418439</td>\n",
       "      <td>-2.246479</td>\n",
       "      <td>-2.192674</td>\n",
       "      <td>-2.202570</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>-2.257281</td>\n",
       "      <td>-2.252014</td>\n",
       "      <td>-2.255698</td>\n",
       "      <td>-2.232111</td>\n",
       "      <td>-2.217908</td>\n",
       "      <td>-2.225340</td>\n",
       "      <td>-2.212337</td>\n",
       "      <td>-0.293394</td>\n",
       "      <td>0.002964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.980566</td>\n",
       "      <td>-9.616326e-01</td>\n",
       "      <td>-8.722231e-01</td>\n",
       "      <td>-0.377964</td>\n",
       "      <td>-1.268290</td>\n",
       "      <td>1.851640</td>\n",
       "      <td>-0.719682</td>\n",
       "      <td>-0.846917</td>\n",
       "      <td>-0.664355</td>\n",
       "      <td>-0.548209</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>-0.649406</td>\n",
       "      <td>-0.662666</td>\n",
       "      <td>-0.677918</td>\n",
       "      <td>-0.678834</td>\n",
       "      <td>-0.672873</td>\n",
       "      <td>-0.665673</td>\n",
       "      <td>-0.646577</td>\n",
       "      <td>-0.019002</td>\n",
       "      <td>0.055748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.013138</td>\n",
       "      <td>-9.578925e-08</td>\n",
       "      <td>-3.563138e-01</td>\n",
       "      <td>0.347013</td>\n",
       "      <td>-0.408264</td>\n",
       "      <td>1.851640</td>\n",
       "      <td>-0.030504</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>-0.014068</td>\n",
       "      <td>-0.030740</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>-0.026439</td>\n",
       "      <td>-0.026806</td>\n",
       "      <td>-0.011559</td>\n",
       "      <td>-0.007699</td>\n",
       "      <td>-0.005522</td>\n",
       "      <td>-0.010338</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.084558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.112692</td>\n",
       "      <td>1.081497e+00</td>\n",
       "      <td>6.155262e-01</td>\n",
       "      <td>1.315335</td>\n",
       "      <td>0.645527</td>\n",
       "      <td>1.851640</td>\n",
       "      <td>0.685076</td>\n",
       "      <td>0.948235</td>\n",
       "      <td>0.691611</td>\n",
       "      <td>0.519800</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>0.626531</td>\n",
       "      <td>0.622692</td>\n",
       "      <td>0.655993</td>\n",
       "      <td>0.659165</td>\n",
       "      <td>0.674698</td>\n",
       "      <td>0.676594</td>\n",
       "      <td>0.650309</td>\n",
       "      <td>0.025285</td>\n",
       "      <td>0.127210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>946.026299</td>\n",
       "      <td>4.205749e+01</td>\n",
       "      <td>2.834043e+07</td>\n",
       "      <td>99555.191430</td>\n",
       "      <td>90.278672</td>\n",
       "      <td>8.332381</td>\n",
       "      <td>32.125677</td>\n",
       "      <td>2.267779</td>\n",
       "      <td>2.258916</td>\n",
       "      <td>2.252378</td>\n",
       "      <td>...</td>\n",
       "      <td>9.416298</td>\n",
       "      <td>2.262815</td>\n",
       "      <td>2.259329</td>\n",
       "      <td>2.259030</td>\n",
       "      <td>2.258160</td>\n",
       "      <td>2.253826</td>\n",
       "      <td>2.253327</td>\n",
       "      <td>2.249147</td>\n",
       "      <td>1.325704</td>\n",
       "      <td>3.442886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         market_cap         price        volume          rank  market_share  \\\n",
       "count  13007.000000  1.300700e+04  1.300700e+04  13007.000000  13007.000000   \n",
       "mean       0.270739  1.679970e-01  6.092668e+03     66.622537     -0.129323   \n",
       "std        8.585780  2.021850e+00  2.851716e+05   3129.445036      2.237954   \n",
       "min      -18.753049 -1.115415e+01 -7.922857e+05 -89426.540288    -12.715254   \n",
       "25%       -0.980566 -9.616326e-01 -8.722231e-01     -0.377964     -1.268290   \n",
       "50%        0.013138 -9.578925e-08 -3.563138e-01      0.347013     -0.408264   \n",
       "75%        1.112692  1.081497e+00  6.155262e-01      1.315335      0.645527   \n",
       "max      946.026299  4.205749e+01  2.834043e+07  99555.191430     90.278672   \n",
       "\n",
       "                age           roi  market_cap_lag_1  market_cap_lag_2  \\\n",
       "count  13007.000000  13007.000000      13007.000000      13007.000000   \n",
       "mean       1.851912     -0.000730          0.048943          0.017197   \n",
       "std        0.059661      1.567497          1.093715          0.865874   \n",
       "min        0.878310    -17.418439         -2.246479         -2.192674   \n",
       "25%        1.851640     -0.719682         -0.846917         -0.664355   \n",
       "50%        1.851640     -0.030504          0.007387         -0.014068   \n",
       "75%        1.851640      0.685076          0.948235          0.691611   \n",
       "max        8.332381     32.125677          2.267779          2.258916   \n",
       "\n",
       "       market_cap_lag_3  ...       age_std     roi_lag_1     roi_lag_2  \\\n",
       "count      13007.000000  ...  13007.000000  13007.000000  13007.000000   \n",
       "mean          -0.002063  ...      2.163436     -0.008858     -0.004928   \n",
       "std            0.774281  ...      0.134888      0.922142      0.922319   \n",
       "min           -2.202570  ...      2.160247     -2.257281     -2.252014   \n",
       "25%           -0.548209  ...      2.160247     -0.649406     -0.662666   \n",
       "50%           -0.030740  ...      2.160247     -0.026439     -0.026806   \n",
       "75%            0.519800  ...      2.160247      0.626531      0.622692   \n",
       "max            2.252378  ...      9.416298      2.262815      2.259329   \n",
       "\n",
       "          roi_lag_3     roi_lag_4     roi_lag_5     roi_lag_6     roi_lag_7  \\\n",
       "count  13007.000000  13007.000000  13007.000000  13007.000000  13007.000000   \n",
       "mean      -0.004355     -0.004699      0.001808      0.011210      0.009822   \n",
       "std        0.926532      0.925193      0.927509      0.929121      0.927949   \n",
       "min       -2.255698     -2.232111     -2.217908     -2.225340     -2.212337   \n",
       "25%       -0.677918     -0.678834     -0.672873     -0.665673     -0.646577   \n",
       "50%       -0.011559     -0.007699     -0.005522     -0.010338     -0.017433   \n",
       "75%        0.655993      0.659165      0.674698      0.676594      0.650309   \n",
       "max        2.259030      2.258160      2.253826      2.253327      2.249147   \n",
       "\n",
       "           roi_mean       roi_std  \n",
       "count  13007.000000  13007.000000  \n",
       "mean       0.006045      0.109214  \n",
       "std        0.058275      0.131429  \n",
       "min       -0.293394      0.002964  \n",
       "25%       -0.019002      0.055748  \n",
       "50%        0.002450      0.084558  \n",
       "75%        0.025285      0.127210  \n",
       "max        1.325704      3.442886  \n",
       "\n",
       "[8 rows x 70 columns]"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_lag_1</th>\n",
       "      <th>price_lag_2</th>\n",
       "      <th>price_lag_3</th>\n",
       "      <th>price_lag_4</th>\n",
       "      <th>price_lag_5</th>\n",
       "      <th>price_lag_6</th>\n",
       "      <th>price_lag_7</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>price_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>49.89</td>\n",
       "      <td>50.03</td>\n",
       "      <td>48.49</td>\n",
       "      <td>48.55</td>\n",
       "      <td>48.22</td>\n",
       "      <td>49.67</td>\n",
       "      <td>48.31</td>\n",
       "      <td>50.71</td>\n",
       "      <td>49.140000</td>\n",
       "      <td>0.986898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13654</th>\n",
       "      <td>52.72</td>\n",
       "      <td>49.89</td>\n",
       "      <td>50.03</td>\n",
       "      <td>48.49</td>\n",
       "      <td>48.55</td>\n",
       "      <td>48.22</td>\n",
       "      <td>49.67</td>\n",
       "      <td>48.31</td>\n",
       "      <td>49.022857</td>\n",
       "      <td>0.800556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>62.17</td>\n",
       "      <td>52.72</td>\n",
       "      <td>49.89</td>\n",
       "      <td>50.03</td>\n",
       "      <td>48.49</td>\n",
       "      <td>48.55</td>\n",
       "      <td>48.22</td>\n",
       "      <td>49.67</td>\n",
       "      <td>49.652857</td>\n",
       "      <td>1.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13656</th>\n",
       "      <td>70.16</td>\n",
       "      <td>62.17</td>\n",
       "      <td>52.72</td>\n",
       "      <td>49.89</td>\n",
       "      <td>50.03</td>\n",
       "      <td>48.49</td>\n",
       "      <td>48.55</td>\n",
       "      <td>48.22</td>\n",
       "      <td>51.438571</td>\n",
       "      <td>4.976359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13657</th>\n",
       "      <td>68.38</td>\n",
       "      <td>70.16</td>\n",
       "      <td>62.17</td>\n",
       "      <td>52.72</td>\n",
       "      <td>49.89</td>\n",
       "      <td>50.03</td>\n",
       "      <td>48.49</td>\n",
       "      <td>48.55</td>\n",
       "      <td>54.572857</td>\n",
       "      <td>8.366116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>385.31</td>\n",
       "      <td>370.29</td>\n",
       "      <td>383.23</td>\n",
       "      <td>380.54</td>\n",
       "      <td>416.89</td>\n",
       "      <td>386.43</td>\n",
       "      <td>379.61</td>\n",
       "      <td>396.46</td>\n",
       "      <td>387.635714</td>\n",
       "      <td>15.111697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14001</th>\n",
       "      <td>400.51</td>\n",
       "      <td>385.31</td>\n",
       "      <td>370.29</td>\n",
       "      <td>383.23</td>\n",
       "      <td>380.54</td>\n",
       "      <td>416.89</td>\n",
       "      <td>386.43</td>\n",
       "      <td>379.61</td>\n",
       "      <td>386.042857</td>\n",
       "      <td>14.605712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14002</th>\n",
       "      <td>398.53</td>\n",
       "      <td>400.51</td>\n",
       "      <td>385.31</td>\n",
       "      <td>370.29</td>\n",
       "      <td>383.23</td>\n",
       "      <td>380.54</td>\n",
       "      <td>416.89</td>\n",
       "      <td>386.43</td>\n",
       "      <td>389.028571</td>\n",
       "      <td>15.195811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14003</th>\n",
       "      <td>414.24</td>\n",
       "      <td>398.53</td>\n",
       "      <td>400.51</td>\n",
       "      <td>385.31</td>\n",
       "      <td>370.29</td>\n",
       "      <td>383.23</td>\n",
       "      <td>380.54</td>\n",
       "      <td>416.89</td>\n",
       "      <td>390.757143</td>\n",
       "      <td>15.535363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14004</th>\n",
       "      <td>430.54</td>\n",
       "      <td>414.24</td>\n",
       "      <td>398.53</td>\n",
       "      <td>400.51</td>\n",
       "      <td>385.31</td>\n",
       "      <td>370.29</td>\n",
       "      <td>383.23</td>\n",
       "      <td>380.54</td>\n",
       "      <td>390.378571</td>\n",
       "      <td>14.807656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price  price_lag_1  price_lag_2  price_lag_3  price_lag_4  \\\n",
       "13653   49.89        50.03        48.49        48.55        48.22   \n",
       "13654   52.72        49.89        50.03        48.49        48.55   \n",
       "13655   62.17        52.72        49.89        50.03        48.49   \n",
       "13656   70.16        62.17        52.72        49.89        50.03   \n",
       "13657   68.38        70.16        62.17        52.72        49.89   \n",
       "...       ...          ...          ...          ...          ...   \n",
       "14000  385.31       370.29       383.23       380.54       416.89   \n",
       "14001  400.51       385.31       370.29       383.23       380.54   \n",
       "14002  398.53       400.51       385.31       370.29       383.23   \n",
       "14003  414.24       398.53       400.51       385.31       370.29   \n",
       "14004  430.54       414.24       398.53       400.51       385.31   \n",
       "\n",
       "       price_lag_5  price_lag_6  price_lag_7  price_mean  price_std  \n",
       "13653        49.67        48.31        50.71   49.140000   0.986898  \n",
       "13654        48.22        49.67        48.31   49.022857   0.800556  \n",
       "13655        48.55        48.22        49.67   49.652857   1.539900  \n",
       "13656        48.49        48.55        48.22   51.438571   4.976359  \n",
       "13657        50.03        48.49        48.55   54.572857   8.366116  \n",
       "...            ...          ...          ...         ...        ...  \n",
       "14000       386.43       379.61       396.46  387.635714  15.111697  \n",
       "14001       416.89       386.43       379.61  386.042857  14.605712  \n",
       "14002       380.54       416.89       386.43  389.028571  15.195811  \n",
       "14003       383.23       380.54       416.89  390.757143  15.535363  \n",
       "14004       370.29       383.23       380.54  390.378571  14.807656  \n",
       "\n",
       "[352 rows x 10 columns]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.query('sym == \"ETH\"')[[col for col in test.columns if 'price' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>price_lag_1</th>\n",
       "      <th>price_lag_2</th>\n",
       "      <th>price_lag_3</th>\n",
       "      <th>price_lag_4</th>\n",
       "      <th>price_lag_5</th>\n",
       "      <th>price_lag_6</th>\n",
       "      <th>price_lag_7</th>\n",
       "      <th>price_mean</th>\n",
       "      <th>price_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>0.759957</td>\n",
       "      <td>0.901816</td>\n",
       "      <td>-0.658630</td>\n",
       "      <td>-0.597833</td>\n",
       "      <td>-0.932214</td>\n",
       "      <td>0.537037</td>\n",
       "      <td>-0.841019</td>\n",
       "      <td>1.590844</td>\n",
       "      <td>49.140000</td>\n",
       "      <td>0.986898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13654</th>\n",
       "      <td>4.618217</td>\n",
       "      <td>1.083175</td>\n",
       "      <td>1.258054</td>\n",
       "      <td>-0.665609</td>\n",
       "      <td>-0.590661</td>\n",
       "      <td>-1.002874</td>\n",
       "      <td>0.808366</td>\n",
       "      <td>-0.890452</td>\n",
       "      <td>49.022857</td>\n",
       "      <td>0.800556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13655</th>\n",
       "      <td>8.128545</td>\n",
       "      <td>1.991781</td>\n",
       "      <td>0.153999</td>\n",
       "      <td>0.244914</td>\n",
       "      <td>-0.755151</td>\n",
       "      <td>-0.716188</td>\n",
       "      <td>-0.930487</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>49.652857</td>\n",
       "      <td>1.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13656</th>\n",
       "      <td>3.762074</td>\n",
       "      <td>2.156482</td>\n",
       "      <td>0.257503</td>\n",
       "      <td>-0.311186</td>\n",
       "      <td>-0.283053</td>\n",
       "      <td>-0.592516</td>\n",
       "      <td>-0.580459</td>\n",
       "      <td>-0.646772</td>\n",
       "      <td>51.438571</td>\n",
       "      <td>4.976359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13657</th>\n",
       "      <td>1.650365</td>\n",
       "      <td>1.863128</td>\n",
       "      <td>0.908085</td>\n",
       "      <td>-0.221472</td>\n",
       "      <td>-0.559741</td>\n",
       "      <td>-0.543007</td>\n",
       "      <td>-0.727083</td>\n",
       "      <td>-0.719911</td>\n",
       "      <td>54.572857</td>\n",
       "      <td>8.366116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14000</th>\n",
       "      <td>-0.153902</td>\n",
       "      <td>-1.147834</td>\n",
       "      <td>-0.291543</td>\n",
       "      <td>-0.469551</td>\n",
       "      <td>1.935870</td>\n",
       "      <td>-0.079787</td>\n",
       "      <td>-0.531093</td>\n",
       "      <td>0.583937</td>\n",
       "      <td>387.635714</td>\n",
       "      <td>15.111697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14001</th>\n",
       "      <td>0.990513</td>\n",
       "      <td>-0.050176</td>\n",
       "      <td>-1.078541</td>\n",
       "      <td>-0.192586</td>\n",
       "      <td>-0.376761</td>\n",
       "      <td>2.111992</td>\n",
       "      <td>0.026506</td>\n",
       "      <td>-0.440434</td>\n",
       "      <td>386.042857</td>\n",
       "      <td>14.605712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14002</th>\n",
       "      <td>0.625266</td>\n",
       "      <td>0.755565</td>\n",
       "      <td>-0.244710</td>\n",
       "      <td>-1.233141</td>\n",
       "      <td>-0.381590</td>\n",
       "      <td>-0.558613</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>-0.171006</td>\n",
       "      <td>389.028571</td>\n",
       "      <td>15.195811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14003</th>\n",
       "      <td>1.511574</td>\n",
       "      <td>0.500333</td>\n",
       "      <td>0.627784</td>\n",
       "      <td>-0.350629</td>\n",
       "      <td>-1.317455</td>\n",
       "      <td>-0.484517</td>\n",
       "      <td>-0.657670</td>\n",
       "      <td>1.682153</td>\n",
       "      <td>390.757143</td>\n",
       "      <td>15.535363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14004</th>\n",
       "      <td>2.712207</td>\n",
       "      <td>1.611425</td>\n",
       "      <td>0.550487</td>\n",
       "      <td>0.684202</td>\n",
       "      <td>-0.342294</td>\n",
       "      <td>-1.356634</td>\n",
       "      <td>-0.482762</td>\n",
       "      <td>-0.664425</td>\n",
       "      <td>390.378571</td>\n",
       "      <td>14.807656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>352 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  price_lag_1  price_lag_2  price_lag_3  price_lag_4  \\\n",
       "13653  0.759957     0.901816    -0.658630    -0.597833    -0.932214   \n",
       "13654  4.618217     1.083175     1.258054    -0.665609    -0.590661   \n",
       "13655  8.128545     1.991781     0.153999     0.244914    -0.755151   \n",
       "13656  3.762074     2.156482     0.257503    -0.311186    -0.283053   \n",
       "13657  1.650365     1.863128     0.908085    -0.221472    -0.559741   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "14000 -0.153902    -1.147834    -0.291543    -0.469551     1.935870   \n",
       "14001  0.990513    -0.050176    -1.078541    -0.192586    -0.376761   \n",
       "14002  0.625266     0.755565    -0.244710    -1.233141    -0.381590   \n",
       "14003  1.511574     0.500333     0.627784    -0.350629    -1.317455   \n",
       "14004  2.712207     1.611425     0.550487     0.684202    -0.342294   \n",
       "\n",
       "       price_lag_5  price_lag_6  price_lag_7  price_mean  price_std  \n",
       "13653     0.537037    -0.841019     1.590844   49.140000   0.986898  \n",
       "13654    -1.002874     0.808366    -0.890452   49.022857   0.800556  \n",
       "13655    -0.716188    -0.930487     0.011132   49.652857   1.539900  \n",
       "13656    -0.592516    -0.580459    -0.646772   51.438571   4.976359  \n",
       "13657    -0.543007    -0.727083    -0.719911   54.572857   8.366116  \n",
       "...            ...          ...          ...         ...        ...  \n",
       "14000    -0.079787    -0.531093     0.583937  387.635714  15.111697  \n",
       "14001     2.111992     0.026506    -0.440434  386.042857  14.605712  \n",
       "14002    -0.558613     1.833494    -0.171006  389.028571  15.195811  \n",
       "14003    -0.484517    -0.657670     1.682153  390.757143  15.535363  \n",
       "14004    -1.356634    -0.482762    -0.664425  390.378571  14.807656  \n",
       "\n",
       "[352 rows x 10 columns]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_scaled.query('sym == \"ETH\"')[[col for col in test_scaled.columns if 'price' in col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>market_cap</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>sym</th>\n",
       "      <th>time</th>\n",
       "      <th>volume</th>\n",
       "      <th>rank</th>\n",
       "      <th>market_share</th>\n",
       "      <th>age</th>\n",
       "      <th>roi</th>\n",
       "      <th>...</th>\n",
       "      <th>age_std</th>\n",
       "      <th>roi_lag_1</th>\n",
       "      <th>roi_lag_2</th>\n",
       "      <th>roi_lag_3</th>\n",
       "      <th>roi_lag_4</th>\n",
       "      <th>roi_lag_5</th>\n",
       "      <th>roi_lag_6</th>\n",
       "      <th>roi_lag_7</th>\n",
       "      <th>roi_mean</th>\n",
       "      <th>roi_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13034</th>\n",
       "      <td>0.675510</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.065853</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-15</td>\n",
       "      <td>0.325754</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.745782</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>0.942114</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>0.977851</td>\n",
       "      <td>0.515793</td>\n",
       "      <td>-1.343438</td>\n",
       "      <td>-0.169989</td>\n",
       "      <td>-1.368904</td>\n",
       "      <td>0.464216</td>\n",
       "      <td>0.924471</td>\n",
       "      <td>-0.092474</td>\n",
       "      <td>0.179286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13035</th>\n",
       "      <td>1.094176</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.557823</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-16</td>\n",
       "      <td>0.789225</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.132440</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>2.226967</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>0.937029</td>\n",
       "      <td>0.972668</td>\n",
       "      <td>0.511872</td>\n",
       "      <td>-1.342285</td>\n",
       "      <td>-0.172038</td>\n",
       "      <td>-1.367682</td>\n",
       "      <td>0.460436</td>\n",
       "      <td>-0.092022</td>\n",
       "      <td>0.179776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13036</th>\n",
       "      <td>0.611008</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-0.506540</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-17</td>\n",
       "      <td>-0.498707</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.681093</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>0.626460</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>1.506968</td>\n",
       "      <td>0.522521</td>\n",
       "      <td>0.549720</td>\n",
       "      <td>0.198052</td>\n",
       "      <td>-1.216993</td>\n",
       "      <td>-0.323891</td>\n",
       "      <td>-1.236376</td>\n",
       "      <td>-0.046654</td>\n",
       "      <td>0.235564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13037</th>\n",
       "      <td>0.109567</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-1.244578</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-18</td>\n",
       "      <td>-1.198626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.184824</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>-0.751049</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>0.422321</td>\n",
       "      <td>1.454282</td>\n",
       "      <td>0.300504</td>\n",
       "      <td>0.332381</td>\n",
       "      <td>-0.079776</td>\n",
       "      <td>-1.738217</td>\n",
       "      <td>-0.691496</td>\n",
       "      <td>0.016034</td>\n",
       "      <td>0.200992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13038</th>\n",
       "      <td>-0.407962</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-0.730619</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2015-08-19</td>\n",
       "      <td>-1.205497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370173</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>-0.747518</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>-0.737313</td>\n",
       "      <td>0.427795</td>\n",
       "      <td>1.452489</td>\n",
       "      <td>0.306835</td>\n",
       "      <td>0.338488</td>\n",
       "      <td>-0.070766</td>\n",
       "      <td>-1.717529</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>0.202417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13649</th>\n",
       "      <td>0.720030</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-0.668560</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-04-21</td>\n",
       "      <td>-1.010141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.539577</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>-0.152828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>0.891740</td>\n",
       "      <td>-0.736021</td>\n",
       "      <td>1.447310</td>\n",
       "      <td>-1.306772</td>\n",
       "      <td>0.286127</td>\n",
       "      <td>0.260742</td>\n",
       "      <td>-0.843125</td>\n",
       "      <td>-0.001403</td>\n",
       "      <td>0.035295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13650</th>\n",
       "      <td>-0.603094</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-0.495874</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-04-22</td>\n",
       "      <td>-1.155688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.407423</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>-0.025458</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>-0.268946</td>\n",
       "      <td>0.848338</td>\n",
       "      <td>-0.892737</td>\n",
       "      <td>1.442583</td>\n",
       "      <td>-1.503219</td>\n",
       "      <td>0.200567</td>\n",
       "      <td>0.173414</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.032998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13651</th>\n",
       "      <td>-0.317267</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>-0.468493</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>-0.933584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.506766</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>-0.970229</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>-0.241241</td>\n",
       "      <td>0.879322</td>\n",
       "      <td>-0.866864</td>\n",
       "      <td>1.475312</td>\n",
       "      <td>-1.479138</td>\n",
       "      <td>0.229649</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.032901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13652</th>\n",
       "      <td>0.039796</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>1.206528</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>-0.057712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.268581</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>0.210403</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>-0.756902</td>\n",
       "      <td>0.165222</td>\n",
       "      <td>-0.066166</td>\n",
       "      <td>0.995599</td>\n",
       "      <td>-0.658961</td>\n",
       "      <td>1.560316</td>\n",
       "      <td>-1.239108</td>\n",
       "      <td>-0.004500</td>\n",
       "      <td>0.034723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>1.289392</td>\n",
       "      <td>Ethereum</td>\n",
       "      <td>0.759957</td>\n",
       "      <td>ETH</td>\n",
       "      <td>2017-04-25</td>\n",
       "      <td>-1.027991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.626254</td>\n",
       "      <td>1.85164</td>\n",
       "      <td>-1.938360</td>\n",
       "      <td>...</td>\n",
       "      <td>2.160247</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>-1.150976</td>\n",
       "      <td>-0.049969</td>\n",
       "      <td>-0.326244</td>\n",
       "      <td>0.941491</td>\n",
       "      <td>-1.034035</td>\n",
       "      <td>1.615757</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.029081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>620 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       market_cap      name     price  sym       time    volume  rank  \\\n",
       "13034    0.675510  Ethereum  1.065853  ETH 2015-08-15  0.325754   0.0   \n",
       "13035    1.094176  Ethereum  0.557823  ETH 2015-08-16  0.789225   0.0   \n",
       "13036    0.611008  Ethereum -0.506540  ETH 2015-08-17 -0.498707   0.0   \n",
       "13037    0.109567  Ethereum -1.244578  ETH 2015-08-18 -1.198626   0.0   \n",
       "13038   -0.407962  Ethereum -0.730619  ETH 2015-08-19 -1.205497   0.0   \n",
       "...           ...       ...       ...  ...        ...       ...   ...   \n",
       "13649    0.720030  Ethereum -0.668560  ETH 2017-04-21 -1.010141   0.0   \n",
       "13650   -0.603094  Ethereum -0.495874  ETH 2017-04-22 -1.155688   0.0   \n",
       "13651   -0.317267  Ethereum -0.468493  ETH 2017-04-23 -0.933584   0.0   \n",
       "13652    0.039796  Ethereum  1.206528  ETH 2017-04-24 -0.057712   0.0   \n",
       "13653    1.289392  Ethereum  0.759957  ETH 2017-04-25 -1.027991   0.0   \n",
       "\n",
       "       market_share      age       roi  ...   age_std  roi_lag_1  roi_lag_2  \\\n",
       "13034      0.745782  1.85164  0.942114  ...  2.160247   0.977851   0.515793   \n",
       "13035      1.132440  1.85164  2.226967  ...  2.160247   0.937029   0.972668   \n",
       "13036      0.681093  1.85164  0.626460  ...  2.160247   1.506968   0.522521   \n",
       "13037      0.184824  1.85164 -0.751049  ...  2.160247   0.422321   1.454282   \n",
       "13038      0.370173  1.85164 -0.747518  ...  2.160247  -0.737313   0.427795   \n",
       "...             ...      ...       ...  ...       ...        ...        ...   \n",
       "13649     -0.539577  1.85164 -0.152828  ...  2.160247   0.891740  -0.736021   \n",
       "13650     -2.407423  1.85164 -0.025458  ...  2.160247  -0.268946   0.848338   \n",
       "13651     -1.506766  1.85164 -0.970229  ...  2.160247   0.002961  -0.241241   \n",
       "13652     -0.268581  1.85164  0.210403  ...  2.160247  -0.756902   0.165222   \n",
       "13653     -0.626254  1.85164 -1.938360  ...  2.160247   0.003976  -1.150976   \n",
       "\n",
       "       roi_lag_3  roi_lag_4  roi_lag_5  roi_lag_6  roi_lag_7  roi_mean  \\\n",
       "13034  -1.343438  -0.169989  -1.368904   0.464216   0.924471 -0.092474   \n",
       "13035   0.511872  -1.342285  -0.172038  -1.367682   0.460436 -0.092022   \n",
       "13036   0.549720   0.198052  -1.216993  -0.323891  -1.236376 -0.046654   \n",
       "13037   0.300504   0.332381  -0.079776  -1.738217  -0.691496  0.016034   \n",
       "13038   1.452489   0.306835   0.338488  -0.070766  -1.717529  0.014324   \n",
       "...          ...        ...        ...        ...        ...       ...   \n",
       "13649   1.447310  -1.306772   0.286127   0.260742  -0.843125 -0.001403   \n",
       "13650  -0.892737   1.442583  -1.503219   0.200567   0.173414  0.002077   \n",
       "13651   0.879322  -0.866864   1.475312  -1.479138   0.229649  0.001140   \n",
       "13652  -0.066166   0.995599  -0.658961   1.560316  -1.239108 -0.004500   \n",
       "13653  -0.049969  -0.326244   0.941491  -1.034035   1.615757  0.002691   \n",
       "\n",
       "        roi_std  \n",
       "13034  0.179286  \n",
       "13035  0.179776  \n",
       "13036  0.235564  \n",
       "13037  0.200992  \n",
       "13038  0.202417  \n",
       "...         ...  \n",
       "13649  0.035295  \n",
       "13650  0.032998  \n",
       "13651  0.032901  \n",
       "13652  0.034723  \n",
       "13653  0.029081  \n",
       "\n",
       "[620 rows x 73 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainval_scaled.query('sym == \"ETH\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X = train_scaled[feat_columns]\n",
    "# train_Y = train_scaled[[TARGET]]\n",
    "\n",
    "# # trainval_X = trainval_scaled[feat_columns]\n",
    "# # trainval_Y = trainval_scaled[[TARGET]]\n",
    "\n",
    "# val_X = val_scaled[feat_columns]\n",
    "# val_Y = val_scaled[[TARGET]]\n",
    "\n",
    "# test_X = test_scaled[feat_columns]\n",
    "# test_Y = test_scaled[[TARGET]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_to_json_obj(sym_df, cat):\n",
    "    json_obj = {\"start\": str(sym_df['time'][0]), \n",
    "                \"target\": list(sym_df['target']), \n",
    "                \"cat\":[cat], \n",
    "                \"dynamic_feat\":[list(sym_df[column]) for column in sym_df.columns]}\n",
    "    return json_obj\n",
    "\n",
    "def write_json_dataset(features_df, filename, target, feat_columns): \n",
    "    mapping = {}\n",
    "    symbols = features_df['sym'].unique()\n",
    "    with open(filename, 'wb') as f:\n",
    "        \n",
    "        for idx, sym in enumerate(symbols):\n",
    "            sym_df = features_df[features_df['sym'] == sym]\n",
    "            sym_df = sym_df.drop(columns='sym')\n",
    "            \n",
    "            json_obj = {\"start\": str(sym_df['time'].iloc[0]), \n",
    "                        \"target\": list(sym_df[target]), \n",
    "                        \"cat\":[idx], \n",
    "                        \"dynamic_feat\":[list(sym_df[column]) for column in feat_columns]}\n",
    "            \n",
    "            json_line = json.dumps(json_obj) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "                                   \n",
    "            f.write(json_line)\n",
    "            \n",
    "            mapping[sym] = idx\n",
    "    print('JSON file created at ' + filename)\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at ./data/trainval.json\n"
     ]
    }
   ],
   "source": [
    "mapping = write_json_dataset(trainval_scaled, './data/trainval.json', TARGET, feat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'deepar-new'\n",
    "\n",
    "train_location = session.upload_data(os.path.join(DATA_DIR, 'trainval.json'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 'D'\n",
    "context_length = 50\n",
    "prediction_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-03 05:18:07 Starting - Starting the training job...\n",
      "2021-02-03 05:18:09 Starting - Launching requested ML instances......\n",
      "2021-02-03 05:19:30 Starting - Preparing the instances for training......\n",
      "2021-02-03 05:20:20 Downloading - Downloading input data...\n",
      "2021-02-03 05:20:59 Training - Downloading the training image...\n",
      "2021-02-03 05:21:19 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.1', u'num_cells': u'100', u'prediction_length': u'1', u'epochs': u'500', u'time_freq': u'D', u'context_length': u'50', u'num_layers': u'4', u'mini_batch_size': u'128', u'early_stopping_patience': u'20'}\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.1', u'num_layers': u'4', u'epochs': u'500', u'embedding_dimension': u'10', u'num_cells': u'100', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'20'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Using early stopping with patience 20\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/trainval.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/trainval.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] [cardinality=auto] Inferred value of cardinality=[38] from dataset.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=49 from dataset.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Training set statistics:\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Real time series\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] number of time series: 38\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] number of observations: 39673\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] mean target length: 1044\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] min/mean/max target: -27.7395515442/-0.0592582479126/100.898200989\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] mean abs(target): 1.17569982634\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] contains missing values: no\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Small number of time series. Doing 34 passes over dataset with prob 0.990712074303 per epoch.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] No test channel found not running evaluations\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] nvidia-smi took: 0.0252749919891 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:22 INFO 140101591476032] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 300.16303062438965, \"sum\": 300.16303062438965, \"min\": 300.16303062438965}}, \"EndTime\": 1612329683.054745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329682.753468}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:23 INFO 140101591476032] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 691.2429332733154, \"sum\": 691.2429332733154, \"min\": 691.2429332733154}}, \"EndTime\": 1612329683.444882, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329683.054848}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:32 INFO 140101591476032] Epoch[0] Batch[0] avg_epoch_loss=1.490689\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:32 INFO 140101591476032] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.49068880081\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:35 INFO 140101591476032] Epoch[0] Batch[5] avg_epoch_loss=6.522177\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:35 INFO 140101591476032] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=6.52217670282\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:35 INFO 140101591476032] Epoch[0] Batch [5]#011Speed: 187.03 samples/sec#011loss=6.522177\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] Epoch[0] Batch[10] avg_epoch_loss=8.068936\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=9.92504772544\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] Epoch[0] Batch [10]#011Speed: 145.83 samples/sec#011loss=9.925048\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 500, \"sum\": 500.0, \"min\": 500}, \"update.time\": {\"count\": 1, \"max\": 16897.58610725403, \"sum\": 16897.58610725403, \"min\": 16897.58610725403}}, \"EndTime\": 1612329700.342717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329683.444965}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=75.9863443494 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] #quality_metric: host=algo-1, epoch=0, train loss <loss>=8.06893625855\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:40 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_40340511-6b9f-43f0-bed9-d679cd7acb8f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 39.309024810791016, \"sum\": 39.309024810791016, \"min\": 39.309024810791016}}, \"EndTime\": 1612329700.382833, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329700.342834}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:49 INFO 140101591476032] Epoch[1] Batch[0] avg_epoch_loss=2.645404\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:49 INFO 140101591476032] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=2.645403862\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:52 INFO 140101591476032] Epoch[1] Batch[5] avg_epoch_loss=2.598604\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:52 INFO 140101591476032] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=2.59860384464\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:52 INFO 140101591476032] Epoch[1] Batch [5]#011Speed: 181.04 samples/sec#011loss=2.598604\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] Epoch[1] Batch[10] avg_epoch_loss=2.316444\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=1.97785170078\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] Epoch[1] Batch [10]#011Speed: 165.45 samples/sec#011loss=1.977852\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] processed a total of 1301 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16438.006162643433, \"sum\": 16438.006162643433, \"min\": 16438.006162643433}}, \"EndTime\": 1612329716.821001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329700.382928}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=79.1452032415 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] #quality_metric: host=algo-1, epoch=1, train loss <loss>=2.31644377925\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:21:56 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_f29f5f60-0bc1-445b-afb4-34d6006c0c98-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.34898567199707, \"sum\": 37.34898567199707, \"min\": 37.34898567199707}}, \"EndTime\": 1612329716.859111, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329716.8211}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:06 INFO 140101591476032] Epoch[2] Batch[0] avg_epoch_loss=2.097277\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:06 INFO 140101591476032] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=2.09727692604\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:10 INFO 140101591476032] Epoch[2] Batch[5] avg_epoch_loss=1.788683\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:10 INFO 140101591476032] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.78868349393\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:10 INFO 140101591476032] Epoch[2] Batch [5]#011Speed: 168.25 samples/sec#011loss=1.788683\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] Epoch[2] Batch[10] avg_epoch_loss=1.838705\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.89873058796\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] Epoch[2] Batch [10]#011Speed: 151.81 samples/sec#011loss=1.898731\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17675.889015197754, \"sum\": 17675.889015197754, \"min\": 17675.889015197754}}, \"EndTime\": 1612329734.535162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329716.859195}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=74.5642880545 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.83870490031\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:14 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_fbcc09fd-22a3-4d48-965a-e360f9656308-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.68610954284668, \"sum\": 37.68610954284668, \"min\": 37.68610954284668}}, \"EndTime\": 1612329734.573659, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329734.535247}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:23 INFO 140101591476032] Epoch[3] Batch[0] avg_epoch_loss=2.357552\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:23 INFO 140101591476032] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=2.35755157471\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:28 INFO 140101591476032] Epoch[3] Batch[5] avg_epoch_loss=2.094537\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:28 INFO 140101591476032] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=2.09453715881\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:28 INFO 140101591476032] Epoch[3] Batch [5]#011Speed: 150.63 samples/sec#011loss=2.094537\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] Epoch[3] Batch[10] avg_epoch_loss=2.046376\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.98858335018\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] Epoch[3] Batch [10]#011Speed: 140.28 samples/sec#011loss=1.988583\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18207.512140274048, \"sum\": 18207.512140274048, \"min\": 18207.512140274048}}, \"EndTime\": 1612329752.781357, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329734.573749}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=71.6181538676 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] #progress_metric: host=algo-1, completed 0 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] #quality_metric: host=algo-1, epoch=3, train loss <loss>=2.0463763367\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:32 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:42 INFO 140101591476032] Epoch[4] Batch[0] avg_epoch_loss=1.711566\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:42 INFO 140101591476032] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.71156585217\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:46 INFO 140101591476032] Epoch[4] Batch[5] avg_epoch_loss=1.942680\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:46 INFO 140101591476032] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.94268012047\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:46 INFO 140101591476032] Epoch[4] Batch [5]#011Speed: 145.95 samples/sec#011loss=1.942680\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:50 INFO 140101591476032] processed a total of 1251 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17718.777894973755, \"sum\": 17718.777894973755, \"min\": 17718.777894973755}}, \"EndTime\": 1612329770.501114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329752.781453}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:50 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=70.6024573478 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:50 INFO 140101591476032] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:50 INFO 140101591476032] #quality_metric: host=algo-1, epoch=4, train loss <loss>=2.05291368961\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:50 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:59 INFO 140101591476032] Epoch[5] Batch[0] avg_epoch_loss=2.248800\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:22:59 INFO 140101591476032] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=2.24880027771\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:04 INFO 140101591476032] Epoch[5] Batch[5] avg_epoch_loss=1.786879\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:04 INFO 140101591476032] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.78687936068\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:04 INFO 140101591476032] Epoch[5] Batch [5]#011Speed: 132.07 samples/sec#011loss=1.786879\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] Epoch[5] Batch[10] avg_epoch_loss=1.765313\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=1.73943269253\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] Epoch[5] Batch [10]#011Speed: 129.37 samples/sec#011loss=1.739433\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] processed a total of 1306 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19012.434005737305, \"sum\": 19012.434005737305, \"min\": 19012.434005737305}}, \"EndTime\": 1612329789.514542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329770.501215}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=68.6913649356 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.76531269334\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:09 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_da1aa91f-143e-47f4-ac42-65c6be48a01e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.79411315917969, \"sum\": 37.79411315917969, \"min\": 37.79411315917969}}, \"EndTime\": 1612329789.553084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329789.51464}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:19 INFO 140101591476032] Epoch[6] Batch[0] avg_epoch_loss=1.635768\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.6357678175\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:23 INFO 140101591476032] Epoch[6] Batch[5] avg_epoch_loss=1.511499\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:23 INFO 140101591476032] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.51149942478\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:23 INFO 140101591476032] Epoch[6] Batch [5]#011Speed: 140.40 samples/sec#011loss=1.511499\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:27 INFO 140101591476032] processed a total of 1258 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17793.445110321045, \"sum\": 17793.445110321045, \"min\": 17793.445110321045}}, \"EndTime\": 1612329807.346705, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329789.553179}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:27 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=70.699731461 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:27 INFO 140101591476032] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:27 INFO 140101591476032] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.40306707621\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:27 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:27 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_e5974615-ee7b-4cc4-aad5-cf09f5d36dd9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.27602958679199, \"sum\": 37.27602958679199, \"min\": 37.27602958679199}}, \"EndTime\": 1612329807.384725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329807.346779}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:37 INFO 140101591476032] Epoch[7] Batch[0] avg_epoch_loss=1.146289\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:37 INFO 140101591476032] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.14628887177\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:41 INFO 140101591476032] Epoch[7] Batch[5] avg_epoch_loss=1.284908\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:41 INFO 140101591476032] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.28490767876\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:41 INFO 140101591476032] Epoch[7] Batch [5]#011Speed: 144.51 samples/sec#011loss=1.284908\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:45 INFO 140101591476032] processed a total of 1210 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17931.969165802002, \"sum\": 17931.969165802002, \"min\": 17931.969165802002}}, \"EndTime\": 1612329825.317156, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329807.38511}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:45 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=67.4767826632 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:45 INFO 140101591476032] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:45 INFO 140101591476032] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.3138620615\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:45 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:45 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_ff633414-acd1-457e-b52d-75a53eb56dfb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.05318069458008, \"sum\": 36.05318069458008, \"min\": 36.05318069458008}}, \"EndTime\": 1612329825.353981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329825.317235}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:55 INFO 140101591476032] Epoch[8] Batch[0] avg_epoch_loss=1.168309\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:55 INFO 140101591476032] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.16830921173\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:59 INFO 140101591476032] Epoch[8] Batch[5] avg_epoch_loss=1.386337\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:59 INFO 140101591476032] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.38633745909\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:23:59 INFO 140101591476032] Epoch[8] Batch [5]#011Speed: 141.57 samples/sec#011loss=1.386337\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:03 INFO 140101591476032] processed a total of 1261 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18361.504077911377, \"sum\": 18361.504077911377, \"min\": 18361.504077911377}}, \"EndTime\": 1612329843.715644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329825.354066}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:03 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=68.6753530511 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:03 INFO 140101591476032] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:03 INFO 140101591476032] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.55157756805\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:03 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:14 INFO 140101591476032] Epoch[9] Batch[0] avg_epoch_loss=1.518030\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.51803004742\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:18 INFO 140101591476032] Epoch[9] Batch[5] avg_epoch_loss=1.480119\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:18 INFO 140101591476032] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.48011869192\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:18 INFO 140101591476032] Epoch[9] Batch [5]#011Speed: 142.71 samples/sec#011loss=1.480119\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] Epoch[9] Batch[10] avg_epoch_loss=1.519822\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=1.56746618748\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] Epoch[9] Batch [10]#011Speed: 130.01 samples/sec#011loss=1.567466\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19732.465028762817, \"sum\": 19732.465028762817, \"min\": 19732.465028762817}}, \"EndTime\": 1612329863.449169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329843.71585}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=66.0833493405 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.51982209899\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:23 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:33 INFO 140101591476032] Epoch[10] Batch[0] avg_epoch_loss=1.478956\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:33 INFO 140101591476032] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.47895646095\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:37 INFO 140101591476032] Epoch[10] Batch[5] avg_epoch_loss=1.323480\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:37 INFO 140101591476032] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.32348004977\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:37 INFO 140101591476032] Epoch[10] Batch [5]#011Speed: 146.33 samples/sec#011loss=1.323480\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] Epoch[10] Batch[10] avg_epoch_loss=1.261452\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=1.18701894283\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] Epoch[10] Batch [10]#011Speed: 138.99 samples/sec#011loss=1.187019\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18627.794981002808, \"sum\": 18627.794981002808, \"min\": 18627.794981002808}}, \"EndTime\": 1612329882.077685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329863.44926}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=71.1833791494 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.26145227389\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:42 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_d844d629-6e0d-4a85-8233-abca5c7a31a7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.62984275817871, \"sum\": 37.62984275817871, \"min\": 37.62984275817871}}, \"EndTime\": 1612329882.116054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329882.077786}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:51 INFO 140101591476032] Epoch[11] Batch[0] avg_epoch_loss=1.349180\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:51 INFO 140101591476032] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.34918045998\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:56 INFO 140101591476032] Epoch[11] Batch[5] avg_epoch_loss=1.207351\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:56 INFO 140101591476032] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.20735130707\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:24:56 INFO 140101591476032] Epoch[11] Batch [5]#011Speed: 144.43 samples/sec#011loss=1.207351\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] Epoch[11] Batch[10] avg_epoch_loss=1.291088\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=1.39157176018\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] Epoch[11] Batch [10]#011Speed: 132.59 samples/sec#011loss=1.391572\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] processed a total of 1286 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19005.527019500732, \"sum\": 19005.527019500732, \"min\": 19005.527019500732}}, \"EndTime\": 1612329901.121742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329882.116139}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=67.664042523 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.29108787667\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:01 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:11 INFO 140101591476032] Epoch[12] Batch[0] avg_epoch_loss=1.518816\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:11 INFO 140101591476032] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.51881563663\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:16 INFO 140101591476032] Epoch[12] Batch[5] avg_epoch_loss=1.465257\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:16 INFO 140101591476032] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.46525687973\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:16 INFO 140101591476032] Epoch[12] Batch [5]#011Speed: 145.65 samples/sec#011loss=1.465257\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] Epoch[12] Batch[10] avg_epoch_loss=1.363928\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=1.24233345985\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] Epoch[12] Batch [10]#011Speed: 141.30 samples/sec#011loss=1.242333\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19494.116067886353, \"sum\": 19494.116067886353, \"min\": 19494.116067886353}}, \"EndTime\": 1612329920.616445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329901.121837}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=67.5070604491 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.36392805251\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:20 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:30 INFO 140101591476032] Epoch[13] Batch[0] avg_epoch_loss=0.799050\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:30 INFO 140101591476032] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=0.799050092697\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:34 INFO 140101591476032] Epoch[13] Batch[5] avg_epoch_loss=1.009279\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.0092787842\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:34 INFO 140101591476032] Epoch[13] Batch [5]#011Speed: 152.12 samples/sec#011loss=1.009279\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] Epoch[13] Batch[10] avg_epoch_loss=0.922208\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=0.817722332478\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] Epoch[13] Batch [10]#011Speed: 143.36 samples/sec#011loss=0.817722\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] processed a total of 1319 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18305.706024169922, \"sum\": 18305.706024169922, \"min\": 18305.706024169922}}, \"EndTime\": 1612329938.922755, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329920.616538}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=72.0534850855 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=13, train loss <loss>=0.922207669778\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:38 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_105a39f5-4bbf-41e6-af8a-e5b1bd88f0a4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 53.26390266418457, \"sum\": 53.26390266418457, \"min\": 53.26390266418457}}, \"EndTime\": 1612329938.976899, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329938.92285}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:48 INFO 140101591476032] Epoch[14] Batch[0] avg_epoch_loss=1.119167\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:48 INFO 140101591476032] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.11916661263\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:53 INFO 140101591476032] Epoch[14] Batch[5] avg_epoch_loss=1.108878\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:53 INFO 140101591476032] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.10887823502\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:53 INFO 140101591476032] Epoch[14] Batch [5]#011Speed: 147.58 samples/sec#011loss=1.108878\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] Epoch[14] Batch[10] avg_epoch_loss=0.752701\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=0.325289416313\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] Epoch[14] Batch [10]#011Speed: 130.81 samples/sec#011loss=0.325289\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18977.0610332489, \"sum\": 18977.0610332489, \"min\": 18977.0610332489}}, \"EndTime\": 1612329957.954124, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329938.976984}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=67.6599048133 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=14, train loss <loss>=0.752701499245\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:25:57 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_5d1d2744-ccda-4c8a-942b-1f1b6729abac-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.46294975280762, \"sum\": 37.46294975280762, \"min\": 37.46294975280762}}, \"EndTime\": 1612329957.99239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329957.954283}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:08 INFO 140101591476032] Epoch[15] Batch[0] avg_epoch_loss=1.359786\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:08 INFO 140101591476032] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.35978555679\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:12 INFO 140101591476032] Epoch[15] Batch[5] avg_epoch_loss=1.463383\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:12 INFO 140101591476032] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.46338301897\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:12 INFO 140101591476032] Epoch[15] Batch [5]#011Speed: 145.94 samples/sec#011loss=1.463383\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:16 INFO 140101591476032] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18107.866048812866, \"sum\": 18107.866048812866, \"min\": 18107.866048812866}}, \"EndTime\": 1612329976.100408, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329957.992472}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:16 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=68.9749776578 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:16 INFO 140101591476032] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:16 INFO 140101591476032] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.37428600788\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:16 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:26 INFO 140101591476032] Epoch[16] Batch[0] avg_epoch_loss=1.215754\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:26 INFO 140101591476032] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.21575367451\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:30 INFO 140101591476032] Epoch[16] Batch[5] avg_epoch_loss=1.484561\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:30 INFO 140101591476032] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=1.48456074794\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:30 INFO 140101591476032] Epoch[16] Batch [5]#011Speed: 145.17 samples/sec#011loss=1.484561\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:34 INFO 140101591476032] processed a total of 1272 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17979.92491722107, \"sum\": 17979.92491722107, \"min\": 17979.92491722107}}, \"EndTime\": 1612329994.081079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329976.10051}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:34 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=70.744917851 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:34 INFO 140101591476032] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=16, train loss <loss>=1.36436671019\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:34 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:44 INFO 140101591476032] Epoch[17] Batch[0] avg_epoch_loss=0.717203\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:44 INFO 140101591476032] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=0.717202961445\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:48 INFO 140101591476032] Epoch[17] Batch[5] avg_epoch_loss=0.881280\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:48 INFO 140101591476032] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.881280481815\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:48 INFO 140101591476032] Epoch[17] Batch [5]#011Speed: 149.26 samples/sec#011loss=0.881280\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] Epoch[17] Batch[10] avg_epoch_loss=0.846422\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=0.804590725899\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] Epoch[17] Batch [10]#011Speed: 143.31 samples/sec#011loss=0.804591\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] processed a total of 1299 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18804.424047470093, \"sum\": 18804.424047470093, \"min\": 18804.424047470093}}, \"EndTime\": 1612330012.886215, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612329994.081199}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=69.0790114456 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.846421501853\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:26:52 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:02 INFO 140101591476032] Epoch[18] Batch[0] avg_epoch_loss=0.584473\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:02 INFO 140101591476032] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.584472537041\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:07 INFO 140101591476032] Epoch[18] Batch[5] avg_epoch_loss=0.705620\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:07 INFO 140101591476032] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.705620398124\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:07 INFO 140101591476032] Epoch[18] Batch [5]#011Speed: 127.81 samples/sec#011loss=0.705620\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] Epoch[18] Batch[10] avg_epoch_loss=0.778662\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=0.866311812401\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] Epoch[18] Batch [10]#011Speed: 142.60 samples/sec#011loss=0.866312\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] processed a total of 1326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19252.856016159058, \"sum\": 19252.856016159058, \"min\": 19252.856016159058}}, \"EndTime\": 1612330032.13966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330012.886303}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=68.8723868611 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.778661950068\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:12 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:22 INFO 140101591476032] Epoch[19] Batch[0] avg_epoch_loss=1.854544\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:22 INFO 140101591476032] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=1.85454428196\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:26 INFO 140101591476032] Epoch[19] Batch[5] avg_epoch_loss=1.418513\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:26 INFO 140101591476032] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.41851280133\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:26 INFO 140101591476032] Epoch[19] Batch [5]#011Speed: 148.50 samples/sec#011loss=1.418513\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:29 INFO 140101591476032] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17769.676208496094, \"sum\": 17769.676208496094, \"min\": 17769.676208496094}}, \"EndTime\": 1612330049.910009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330032.139758}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:29 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=70.9068272547 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:29 INFO 140101591476032] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:29 INFO 140101591476032] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.29507172704\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:29 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:39 INFO 140101591476032] Epoch[20] Batch[0] avg_epoch_loss=1.297560\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:39 INFO 140101591476032] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=1.29756033421\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:44 INFO 140101591476032] Epoch[20] Batch[5] avg_epoch_loss=1.087081\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:44 INFO 140101591476032] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.08708065748\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:44 INFO 140101591476032] Epoch[20] Batch [5]#011Speed: 148.89 samples/sec#011loss=1.087081\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] Epoch[20] Batch[10] avg_epoch_loss=1.010796\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=0.919254493713\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] Epoch[20] Batch [10]#011Speed: 145.62 samples/sec#011loss=0.919254\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] processed a total of 1319 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18759.124040603638, \"sum\": 18759.124040603638, \"min\": 18759.124040603638}}, \"EndTime\": 1612330068.669763, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330049.910093}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=70.3118224251 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.01079603759\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:48 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:58 INFO 140101591476032] Epoch[21] Batch[0] avg_epoch_loss=0.875540\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:27:58 INFO 140101591476032] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.875539541245\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:03 INFO 140101591476032] Epoch[21] Batch[5] avg_epoch_loss=0.791524\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:03 INFO 140101591476032] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.791523784399\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:03 INFO 140101591476032] Epoch[21] Batch [5]#011Speed: 147.44 samples/sec#011loss=0.791524\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] Epoch[21] Batch[10] avg_epoch_loss=0.751471\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=0.703406909108\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] Epoch[21] Batch [10]#011Speed: 125.83 samples/sec#011loss=0.703407\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19531.877040863037, \"sum\": 19531.877040863037, \"min\": 19531.877040863037}}, \"EndTime\": 1612330088.202409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330068.669887}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=66.557426912 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.751470659267\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:08 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_b550aecb-065e-4033-ac9d-39fa0eca6cec-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.971092224121094, \"sum\": 36.971092224121094, \"min\": 36.971092224121094}}, \"EndTime\": 1612330088.240157, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330088.202493}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:18 INFO 140101591476032] Epoch[22] Batch[0] avg_epoch_loss=0.621770\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:18 INFO 140101591476032] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.621769726276\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:23 INFO 140101591476032] Epoch[22] Batch[5] avg_epoch_loss=0.657169\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:23 INFO 140101591476032] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.657168875138\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:23 INFO 140101591476032] Epoch[22] Batch [5]#011Speed: 149.14 samples/sec#011loss=0.657169\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] Epoch[22] Batch[10] avg_epoch_loss=0.606418\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=0.545516359806\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] Epoch[22] Batch [10]#011Speed: 147.70 samples/sec#011loss=0.545516\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19174.87597465515, \"sum\": 19174.87597465515, \"min\": 19174.87597465515}}, \"EndTime\": 1612330107.415199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330088.240245}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=68.370199155 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.606417731805\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:27 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_fad4c2a9-ae32-447a-b2a1-894f053903e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.64207458496094, \"sum\": 36.64207458496094, \"min\": 36.64207458496094}}, \"EndTime\": 1612330107.452582, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330107.415296}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:37 INFO 140101591476032] Epoch[23] Batch[0] avg_epoch_loss=0.537877\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:37 INFO 140101591476032] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.53787702322\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:41 INFO 140101591476032] Epoch[23] Batch[5] avg_epoch_loss=0.661079\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:41 INFO 140101591476032] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.661078701417\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:41 INFO 140101591476032] Epoch[23] Batch [5]#011Speed: 152.19 samples/sec#011loss=0.661079\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] Epoch[23] Batch[10] avg_epoch_loss=0.777713\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.917673945427\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] Epoch[23] Batch [10]#011Speed: 146.73 samples/sec#011loss=0.917674\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] processed a total of 1300 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18519.526958465576, \"sum\": 18519.526958465576, \"min\": 18519.526958465576}}, \"EndTime\": 1612330125.972255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330107.452668}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=70.1957219528 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.77771290324\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:45 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:56 INFO 140101591476032] Epoch[24] Batch[0] avg_epoch_loss=0.828905\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:28:56 INFO 140101591476032] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.828905045986\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:00 INFO 140101591476032] Epoch[24] Batch[5] avg_epoch_loss=0.761390\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:00 INFO 140101591476032] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.761390209198\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:00 INFO 140101591476032] Epoch[24] Batch [5]#011Speed: 152.60 samples/sec#011loss=0.761390\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] Epoch[24] Batch[10] avg_epoch_loss=0.905294\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=1.0779787004\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] Epoch[24] Batch [10]#011Speed: 128.58 samples/sec#011loss=1.077979\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19650.465965270996, \"sum\": 19650.465965270996, \"min\": 19650.465965270996}}, \"EndTime\": 1612330145.62332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330125.972338}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=65.6468107988 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.905294068835\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:05 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:16 INFO 140101591476032] Epoch[25] Batch[0] avg_epoch_loss=0.995841\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:16 INFO 140101591476032] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.995841085911\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:20 INFO 140101591476032] Epoch[25] Batch[5] avg_epoch_loss=0.773609\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:20 INFO 140101591476032] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.773609181245\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:20 INFO 140101591476032] Epoch[25] Batch [5]#011Speed: 150.81 samples/sec#011loss=0.773609\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:23 INFO 140101591476032] processed a total of 1263 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18294.412851333618, \"sum\": 18294.412851333618, \"min\": 18294.412851333618}}, \"EndTime\": 1612330163.919004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330145.62342}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:23 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=69.0369916347 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:23 INFO 140101591476032] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:23 INFO 140101591476032] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.683458808064\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:23 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:34 INFO 140101591476032] Epoch[26] Batch[0] avg_epoch_loss=0.418203\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.418203443289\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:38 INFO 140101591476032] Epoch[26] Batch[5] avg_epoch_loss=0.701485\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.701484868924\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:38 INFO 140101591476032] Epoch[26] Batch [5]#011Speed: 151.97 samples/sec#011loss=0.701485\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] Epoch[26] Batch[10] avg_epoch_loss=0.701138\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=0.700722312927\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] Epoch[26] Batch [10]#011Speed: 137.50 samples/sec#011loss=0.700722\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18985.053062438965, \"sum\": 18985.053062438965, \"min\": 18985.053062438965}}, \"EndTime\": 1612330182.904806, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330163.919085}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=67.6842906617 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.701138252562\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:42 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:53 INFO 140101591476032] Epoch[27] Batch[0] avg_epoch_loss=2.477956\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:53 INFO 140101591476032] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=2.47795581818\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:57 INFO 140101591476032] Epoch[27] Batch[5] avg_epoch_loss=1.664460\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.66445996364\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:29:57 INFO 140101591476032] Epoch[27] Batch [5]#011Speed: 146.29 samples/sec#011loss=1.664460\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:01 INFO 140101591476032] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18368.247032165527, \"sum\": 18368.247032165527, \"min\": 18368.247032165527}}, \"EndTime\": 1612330201.273923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330182.904893}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:01 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=67.7250873838 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:01 INFO 140101591476032] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:01 INFO 140101591476032] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.60679507256\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:01 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:11 INFO 140101591476032] Epoch[28] Batch[0] avg_epoch_loss=1.190425\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:11 INFO 140101591476032] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.19042479992\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:15 INFO 140101591476032] Epoch[28] Batch[5] avg_epoch_loss=1.078903\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:15 INFO 140101591476032] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.07890319824\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:15 INFO 140101591476032] Epoch[28] Batch [5]#011Speed: 148.49 samples/sec#011loss=1.078903\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:19 INFO 140101591476032] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18391.09778404236, \"sum\": 18391.09778404236, \"min\": 18391.09778404236}}, \"EndTime\": 1612330219.665862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330201.274009}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:19 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=66.5534101092 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:19 INFO 140101591476032] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.10554645061\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:19 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:29 INFO 140101591476032] Epoch[29] Batch[0] avg_epoch_loss=0.672731\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:29 INFO 140101591476032] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.672730863094\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:34 INFO 140101591476032] Epoch[29] Batch[5] avg_epoch_loss=0.722984\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.722984333833\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:34 INFO 140101591476032] Epoch[29] Batch [5]#011Speed: 146.28 samples/sec#011loss=0.722984\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] Epoch[29] Batch[10] avg_epoch_loss=0.707143\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.688132357597\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] Epoch[29] Batch [10]#011Speed: 139.03 samples/sec#011loss=0.688132\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] processed a total of 1329 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19190.906047821045, \"sum\": 19190.906047821045, \"min\": 19190.906047821045}}, \"EndTime\": 1612330238.857586, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330219.665961}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=69.2510850908 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.707142526453\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:38 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:48 INFO 140101591476032] Epoch[30] Batch[0] avg_epoch_loss=0.723239\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:48 INFO 140101591476032] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.723238706589\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:52 INFO 140101591476032] Epoch[30] Batch[5] avg_epoch_loss=0.885447\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:52 INFO 140101591476032] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.885446548462\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:52 INFO 140101591476032] Epoch[30] Batch [5]#011Speed: 141.09 samples/sec#011loss=0.885447\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] Epoch[30] Batch[10] avg_epoch_loss=0.964762\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=1.05994066\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] Epoch[30] Batch [10]#011Speed: 132.28 samples/sec#011loss=1.059941\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 18941.90287590027, \"sum\": 18941.90287590027, \"min\": 18941.90287590027}}, \"EndTime\": 1612330257.800204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330238.857668}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=69.2111270121 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.964762053706\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:30:57 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:08 INFO 140101591476032] Epoch[31] Batch[0] avg_epoch_loss=0.746946\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:08 INFO 140101591476032] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=0.746945679188\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:13 INFO 140101591476032] Epoch[31] Batch[5] avg_epoch_loss=1.483065\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:13 INFO 140101591476032] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.48306545615\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:13 INFO 140101591476032] Epoch[31] Batch [5]#011Speed: 129.96 samples/sec#011loss=1.483065\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] Epoch[31] Batch[10] avg_epoch_loss=1.432301\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=1.37138308287\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] Epoch[31] Batch [10]#011Speed: 131.41 samples/sec#011loss=1.371383\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] processed a total of 1308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20725.143909454346, \"sum\": 20725.143909454346, \"min\": 20725.143909454346}}, \"EndTime\": 1612330278.525914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330257.800298}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=63.111376301 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.43230074102\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:18 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:29 INFO 140101591476032] Epoch[32] Batch[0] avg_epoch_loss=0.866649\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:29 INFO 140101591476032] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.866648674011\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:33 INFO 140101591476032] Epoch[32] Batch[5] avg_epoch_loss=0.914012\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:33 INFO 140101591476032] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.914012114207\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:33 INFO 140101591476032] Epoch[32] Batch [5]#011Speed: 135.03 samples/sec#011loss=0.914012\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] Epoch[32] Batch[10] avg_epoch_loss=0.839119\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=0.749246275425\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] Epoch[32] Batch [10]#011Speed: 128.63 samples/sec#011loss=0.749246\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20201.34210586548, \"sum\": 20201.34210586548, \"min\": 20201.34210586548}}, \"EndTime\": 1612330298.727925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330278.526}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=64.1042174528 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.839118551124\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:38 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:49 INFO 140101591476032] Epoch[33] Batch[0] avg_epoch_loss=0.658717\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:49 INFO 140101591476032] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.658716857433\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:54 INFO 140101591476032] Epoch[33] Batch[5] avg_epoch_loss=0.636351\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:54 INFO 140101591476032] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.636350562175\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:54 INFO 140101591476032] Epoch[33] Batch [5]#011Speed: 133.43 samples/sec#011loss=0.636351\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:57 INFO 140101591476032] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19258.909940719604, \"sum\": 19258.909940719604, \"min\": 19258.909940719604}}, \"EndTime\": 1612330317.987527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330298.728016}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:57 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=64.8525903541 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:57 INFO 140101591476032] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.573729217052\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:57 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:31:58 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_d8fb2aba-b15a-47c1-9c1b-8f48eb5ce395-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 35.44306755065918, \"sum\": 35.44306755065918, \"min\": 35.44306755065918}}, \"EndTime\": 1612330318.023897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330317.987639}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:09 INFO 140101591476032] Epoch[34] Batch[0] avg_epoch_loss=0.463064\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.463063597679\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:14 INFO 140101591476032] Epoch[34] Batch[5] avg_epoch_loss=0.474269\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.474268923203\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:14 INFO 140101591476032] Epoch[34] Batch [5]#011Speed: 130.66 samples/sec#011loss=0.474269\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:17 INFO 140101591476032] processed a total of 1241 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19942.591905593872, \"sum\": 19942.591905593872, \"min\": 19942.591905593872}}, \"EndTime\": 1612330337.966646, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330318.023982}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:17 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=62.2282210813 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:17 INFO 140101591476032] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:17 INFO 140101591476032] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.524010741711\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:17 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:18 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_a80421c0-1901-483e-beb3-09058a782635-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 36.393165588378906, \"sum\": 36.393165588378906, \"min\": 36.393165588378906}}, \"EndTime\": 1612330338.003817, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330337.966735}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:28 INFO 140101591476032] Epoch[35] Batch[0] avg_epoch_loss=0.341942\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:28 INFO 140101591476032] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.341941714287\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:32 INFO 140101591476032] Epoch[35] Batch[5] avg_epoch_loss=0.648410\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:32 INFO 140101591476032] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.648410166303\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:32 INFO 140101591476032] Epoch[35] Batch [5]#011Speed: 133.53 samples/sec#011loss=0.648410\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:37 INFO 140101591476032] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19124.295949935913, \"sum\": 19124.295949935913, \"min\": 19124.295949935913}}, \"EndTime\": 1612330357.128259, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330338.00389}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:37 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=64.6292927881 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:37 INFO 140101591476032] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:37 INFO 140101591476032] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.697719046474\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:37 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:47 INFO 140101591476032] Epoch[36] Batch[0] avg_epoch_loss=0.502455\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:47 INFO 140101591476032] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.502454996109\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:52 INFO 140101591476032] Epoch[36] Batch[5] avg_epoch_loss=0.598352\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:52 INFO 140101591476032] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.598352223635\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:52 INFO 140101591476032] Epoch[36] Batch [5]#011Speed: 129.07 samples/sec#011loss=0.598352\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] Epoch[36] Batch[10] avg_epoch_loss=0.626366\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=0.659983247519\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] Epoch[36] Batch [10]#011Speed: 120.29 samples/sec#011loss=0.659983\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] processed a total of 1311 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20743.432998657227, \"sum\": 20743.432998657227, \"min\": 20743.432998657227}}, \"EndTime\": 1612330377.872346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330357.128373}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=63.2002918458 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.6263663254\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:32:57 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:09 INFO 140101591476032] Epoch[37] Batch[0] avg_epoch_loss=0.661631\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.661630511284\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:14 INFO 140101591476032] Epoch[37] Batch[5] avg_epoch_loss=0.679927\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.679927329222\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:14 INFO 140101591476032] Epoch[37] Batch [5]#011Speed: 128.55 samples/sec#011loss=0.679927\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] Epoch[37] Batch[10] avg_epoch_loss=0.642080\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=0.596662855148\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] Epoch[37] Batch [10]#011Speed: 120.92 samples/sec#011loss=0.596663\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] processed a total of 1334 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21488.707065582275, \"sum\": 21488.707065582275, \"min\": 21488.707065582275}}, \"EndTime\": 1612330399.361753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330377.872444}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=62.0786566942 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.642079841007\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:19 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:29 INFO 140101591476032] Epoch[38] Batch[0] avg_epoch_loss=0.773579\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:29 INFO 140101591476032] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.773579478264\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:34 INFO 140101591476032] Epoch[38] Batch[5] avg_epoch_loss=0.525038\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.525038237373\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:34 INFO 140101591476032] Epoch[38] Batch [5]#011Speed: 127.76 samples/sec#011loss=0.525038\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] Epoch[38] Batch[10] avg_epoch_loss=0.555699\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=0.592492711544\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] Epoch[38] Batch [10]#011Speed: 122.63 samples/sec#011loss=0.592493\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] processed a total of 1328 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20771.55900001526, \"sum\": 20771.55900001526, \"min\": 20771.55900001526}}, \"EndTime\": 1612330420.13403, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330399.361849}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=63.9329551679 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.555699361996\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:40 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:50 INFO 140101591476032] Epoch[39] Batch[0] avg_epoch_loss=0.579434\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:50 INFO 140101591476032] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.579433500767\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:55 INFO 140101591476032] Epoch[39] Batch[5] avg_epoch_loss=0.364325\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:55 INFO 140101591476032] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.364325054611\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:55 INFO 140101591476032] Epoch[39] Batch [5]#011Speed: 130.83 samples/sec#011loss=0.364325\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:59 INFO 140101591476032] processed a total of 1238 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 19391.31498336792, \"sum\": 19391.31498336792, \"min\": 19391.31498336792}}, \"EndTime\": 1612330439.526264, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330420.134169}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:59 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=63.8422835242 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:59 INFO 140101591476032] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:59 INFO 140101591476032] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.548769389465\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:33:59 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:10 INFO 140101591476032] Epoch[40] Batch[0] avg_epoch_loss=1.044559\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:10 INFO 140101591476032] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=1.04455864429\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:15 INFO 140101591476032] Epoch[40] Batch[5] avg_epoch_loss=0.894632\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:15 INFO 140101591476032] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.894632240136\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:15 INFO 140101591476032] Epoch[40] Batch [5]#011Speed: 127.00 samples/sec#011loss=0.894632\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] Epoch[40] Batch[10] avg_epoch_loss=1.032530\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=1.19800741076\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] Epoch[40] Batch [10]#011Speed: 124.11 samples/sec#011loss=1.198007\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] processed a total of 1321 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21336.940050125122, \"sum\": 21336.940050125122, \"min\": 21336.940050125122}}, \"EndTime\": 1612330460.864035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330439.52644}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=61.9110655195 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.03253004497\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:20 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:31 INFO 140101591476032] Epoch[41] Batch[0] avg_epoch_loss=0.954719\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:31 INFO 140101591476032] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.954718947411\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:36 INFO 140101591476032] Epoch[41] Batch[5] avg_epoch_loss=0.835068\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:36 INFO 140101591476032] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.835068101684\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:36 INFO 140101591476032] Epoch[41] Batch [5]#011Speed: 126.86 samples/sec#011loss=0.835068\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] Epoch[41] Batch[10] avg_epoch_loss=0.844771\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] #quality_metric: host=algo-1, epoch=41, batch=10 train loss <loss>=0.856413692236\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] Epoch[41] Batch [10]#011Speed: 111.94 samples/sec#011loss=0.856414\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21812.903881072998, \"sum\": 21812.903881072998, \"min\": 21812.903881072998}}, \"EndTime\": 1612330482.677633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330460.864109}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=59.2306745038 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.844770642844\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:42 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:52 INFO 140101591476032] Epoch[42] Batch[0] avg_epoch_loss=0.991014\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:52 INFO 140101591476032] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.991013884544\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:57 INFO 140101591476032] Epoch[42] Batch[5] avg_epoch_loss=0.781115\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.781115308404\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:34:57 INFO 140101591476032] Epoch[42] Batch [5]#011Speed: 123.31 samples/sec#011loss=0.781115\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] Epoch[42] Batch[10] avg_epoch_loss=0.515090\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=0.195859545469\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] Epoch[42] Batch [10]#011Speed: 117.69 samples/sec#011loss=0.195860\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] processed a total of 1322 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20705.10697364807, \"sum\": 20705.10697364807, \"min\": 20705.10697364807}}, \"EndTime\": 1612330503.383739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330482.677708}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=63.8485753074 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.515089961615\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:03 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_0e5c3754-78dd-4dd3-9277-9a66587689c7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.9331111907959, \"sum\": 37.9331111907959, \"min\": 37.9331111907959}}, \"EndTime\": 1612330503.422448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330503.383827}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:14 INFO 140101591476032] Epoch[43] Batch[0] avg_epoch_loss=0.477230\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.477229595184\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:19 INFO 140101591476032] Epoch[43] Batch[5] avg_epoch_loss=0.746759\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.746759384871\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:19 INFO 140101591476032] Epoch[43] Batch [5]#011Speed: 125.05 samples/sec#011loss=0.746759\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] Epoch[43] Batch[10] avg_epoch_loss=0.689658\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=0.621136367321\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] Epoch[43] Batch [10]#011Speed: 104.10 samples/sec#011loss=0.621136\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22311.47813796997, \"sum\": 22311.47813796997, \"min\": 22311.47813796997}}, \"EndTime\": 1612330525.734138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330503.42254}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=57.4140389263 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.689658013257\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:25 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:35 INFO 140101591476032] Epoch[44] Batch[0] avg_epoch_loss=1.536098\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:35 INFO 140101591476032] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=1.53609788418\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:40 INFO 140101591476032] Epoch[44] Batch[5] avg_epoch_loss=1.244984\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:40 INFO 140101591476032] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=1.24498394132\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:40 INFO 140101591476032] Epoch[44] Batch [5]#011Speed: 122.13 samples/sec#011loss=1.244984\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] Epoch[44] Batch[10] avg_epoch_loss=1.105596\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=0.938329851627\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] Epoch[44] Batch [10]#011Speed: 107.42 samples/sec#011loss=0.938330\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] processed a total of 1285 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21106.478929519653, \"sum\": 21106.478929519653, \"min\": 21106.478929519653}}, \"EndTime\": 1612330546.841279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330525.734231}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=60.8813986852 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] #quality_metric: host=algo-1, epoch=44, train loss <loss>=1.10559571873\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:46 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:57 INFO 140101591476032] Epoch[45] Batch[0] avg_epoch_loss=1.757865\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:35:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.75786530972\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:02 INFO 140101591476032] Epoch[45] Batch[5] avg_epoch_loss=1.359147\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:02 INFO 140101591476032] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=1.35914722085\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:02 INFO 140101591476032] Epoch[45] Batch [5]#011Speed: 110.41 samples/sec#011loss=1.359147\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] Epoch[45] Batch[10] avg_epoch_loss=1.285621\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=1.19739062786\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] Epoch[45] Batch [10]#011Speed: 103.22 samples/sec#011loss=1.197391\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] processed a total of 1305 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22288.917064666748, \"sum\": 22288.917064666748, \"min\": 22288.917064666748}}, \"EndTime\": 1612330569.130953, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330546.841371}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=58.5489128086 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=45, train loss <loss>=1.28562149676\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:09 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:19 INFO 140101591476032] Epoch[46] Batch[0] avg_epoch_loss=1.327481\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.327480793\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:25 INFO 140101591476032] Epoch[46] Batch[5] avg_epoch_loss=0.823123\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:25 INFO 140101591476032] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.823123236497\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:25 INFO 140101591476032] Epoch[46] Batch [5]#011Speed: 114.92 samples/sec#011loss=0.823123\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] Epoch[46] Batch[10] avg_epoch_loss=0.718718\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=0.59343174696\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] Epoch[46] Batch [10]#011Speed: 117.26 samples/sec#011loss=0.593432\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] processed a total of 1295 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21524.235010147095, \"sum\": 21524.235010147095, \"min\": 21524.235010147095}}, \"EndTime\": 1612330590.655862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330569.131049}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=60.1643706569 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.71871801398\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:30 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:41 INFO 140101591476032] Epoch[47] Batch[0] avg_epoch_loss=0.796129\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:41 INFO 140101591476032] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.796129286289\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:46 INFO 140101591476032] Epoch[47] Batch[5] avg_epoch_loss=0.664090\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:46 INFO 140101591476032] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.664090007544\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:46 INFO 140101591476032] Epoch[47] Batch [5]#011Speed: 118.31 samples/sec#011loss=0.664090\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] Epoch[47] Batch[10] avg_epoch_loss=0.622345\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=0.572251075506\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] Epoch[47] Batch [10]#011Speed: 118.47 samples/sec#011loss=0.572251\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] processed a total of 1330 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21196.127891540527, \"sum\": 21196.127891540527, \"min\": 21196.127891540527}}, \"EndTime\": 1612330611.852727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330590.65595}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=62.7469038114 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.622345038436\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:36:51 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:02 INFO 140101591476032] Epoch[48] Batch[0] avg_epoch_loss=0.541881\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:02 INFO 140101591476032] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.541880905628\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:08 INFO 140101591476032] Epoch[48] Batch[5] avg_epoch_loss=0.435638\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:08 INFO 140101591476032] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.43563794593\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:08 INFO 140101591476032] Epoch[48] Batch [5]#011Speed: 104.20 samples/sec#011loss=0.435638\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:13 INFO 140101591476032] processed a total of 1259 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21288.196086883545, \"sum\": 21288.196086883545, \"min\": 21288.196086883545}}, \"EndTime\": 1612330633.14158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330611.852819}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:13 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=59.1403223346 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:13 INFO 140101591476032] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:13 INFO 140101591476032] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.394648367167\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:13 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:13 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_ceb9092f-b492-4ddc-9970-558920a104cc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.38594055175781, \"sum\": 37.38594055175781, \"min\": 37.38594055175781}}, \"EndTime\": 1612330633.179733, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330633.141691}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:24 INFO 140101591476032] Epoch[49] Batch[0] avg_epoch_loss=0.289283\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:24 INFO 140101591476032] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.289282858372\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:29 INFO 140101591476032] Epoch[49] Batch[5] avg_epoch_loss=0.270310\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:29 INFO 140101591476032] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.270309520264\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:29 INFO 140101591476032] Epoch[49] Batch [5]#011Speed: 116.56 samples/sec#011loss=0.270310\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:34 INFO 140101591476032] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 20897.515773773193, \"sum\": 20897.515773773193, \"min\": 20897.515773773193}}, \"EndTime\": 1612330654.077395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330633.179819}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:34 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=61.1552309553 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:34 INFO 140101591476032] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.269938085973\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:34 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:34 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_bedf9e8a-67af-4a0b-9f0b-07acc6679a07-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.548065185546875, \"sum\": 37.548065185546875, \"min\": 37.548065185546875}}, \"EndTime\": 1612330654.115704, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330654.077482}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:45 INFO 140101591476032] Epoch[50] Batch[0] avg_epoch_loss=0.359417\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:45 INFO 140101591476032] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.359416753054\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:50 INFO 140101591476032] Epoch[50] Batch[5] avg_epoch_loss=0.272714\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:50 INFO 140101591476032] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.272713830074\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:50 INFO 140101591476032] Epoch[50] Batch [5]#011Speed: 114.69 samples/sec#011loss=0.272714\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:55 INFO 140101591476032] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21071.547031402588, \"sum\": 21071.547031402588, \"min\": 21071.547031402588}}, \"EndTime\": 1612330675.187406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330654.115789}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:55 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=60.175515515 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:55 INFO 140101591476032] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:55 INFO 140101591476032] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.247393071651\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:55 INFO 140101591476032] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:37:55 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/state_2b8872bf-aa8d-4866-9ec7-7461e954933b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 37.19592094421387, \"sum\": 37.19592094421387, \"min\": 37.19592094421387}}, \"EndTime\": 1612330675.225417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330675.187506}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:06 INFO 140101591476032] Epoch[51] Batch[0] avg_epoch_loss=0.496946\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:06 INFO 140101591476032] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.496945768595\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:12 INFO 140101591476032] Epoch[51] Batch[5] avg_epoch_loss=0.490678\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:12 INFO 140101591476032] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.490677858392\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:12 INFO 140101591476032] Epoch[51] Batch [5]#011Speed: 113.90 samples/sec#011loss=0.490678\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] Epoch[51] Batch[10] avg_epoch_loss=0.662732\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=0.869197386503\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] Epoch[51] Batch [10]#011Speed: 109.10 samples/sec#011loss=0.869197\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] processed a total of 1316 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23058.727025985718, \"sum\": 23058.727025985718, \"min\": 23058.727025985718}}, \"EndTime\": 1612330698.284296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330675.225503}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=57.0711923957 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.662732189352\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:18 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:29 INFO 140101591476032] Epoch[52] Batch[0] avg_epoch_loss=1.299654\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:29 INFO 140101591476032] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=1.29965412617\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:35 INFO 140101591476032] Epoch[52] Batch[5] avg_epoch_loss=1.019910\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:35 INFO 140101591476032] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=1.01990972459\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:35 INFO 140101591476032] Epoch[52] Batch [5]#011Speed: 107.66 samples/sec#011loss=1.019910\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:40 INFO 140101591476032] processed a total of 1213 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 21794.177055358887, \"sum\": 21794.177055358887, \"min\": 21794.177055358887}}, \"EndTime\": 1612330720.079163, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330698.284442}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:40 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=55.6567205992 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:40 INFO 140101591476032] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:40 INFO 140101591476032] #quality_metric: host=algo-1, epoch=52, train loss <loss>=1.39579167664\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:40 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:50 INFO 140101591476032] Epoch[53] Batch[0] avg_epoch_loss=1.973562\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:50 INFO 140101591476032] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=1.97356200218\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:57 INFO 140101591476032] Epoch[53] Batch[5] avg_epoch_loss=2.418796\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:57 INFO 140101591476032] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=2.41879649957\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:38:57 INFO 140101591476032] Epoch[53] Batch [5]#011Speed: 103.09 samples/sec#011loss=2.418796\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:02 INFO 140101591476032] processed a total of 1279 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22252.99906730652, \"sum\": 22252.99906730652, \"min\": 22252.99906730652}}, \"EndTime\": 1612330742.332849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330720.079253}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:02 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=57.4744568395 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:02 INFO 140101591476032] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:02 INFO 140101591476032] #quality_metric: host=algo-1, epoch=53, train loss <loss>=2.53591065407\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:02 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:13 INFO 140101591476032] Epoch[54] Batch[0] avg_epoch_loss=2.637397\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:13 INFO 140101591476032] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=2.63739705086\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:20 INFO 140101591476032] Epoch[54] Batch[5] avg_epoch_loss=2.337397\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:20 INFO 140101591476032] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=2.33739729722\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:20 INFO 140101591476032] Epoch[54] Batch [5]#011Speed: 102.48 samples/sec#011loss=2.337397\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:25 INFO 140101591476032] processed a total of 1224 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22768.73803138733, \"sum\": 22768.73803138733, \"min\": 22768.73803138733}}, \"EndTime\": 1612330765.102519, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330742.333141}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:25 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=53.7575491467 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:25 INFO 140101591476032] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:25 INFO 140101591476032] #quality_metric: host=algo-1, epoch=54, train loss <loss>=2.05733069181\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:25 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:35 INFO 140101591476032] Epoch[55] Batch[0] avg_epoch_loss=0.762348\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:35 INFO 140101591476032] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.762348175049\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:41 INFO 140101591476032] Epoch[55] Batch[5] avg_epoch_loss=0.806329\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:41 INFO 140101591476032] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.806328733762\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:41 INFO 140101591476032] Epoch[55] Batch [5]#011Speed: 104.34 samples/sec#011loss=0.806329\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] Epoch[55] Batch[10] avg_epoch_loss=0.711797\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.598359960318\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] Epoch[55] Batch [10]#011Speed: 98.78 samples/sec#011loss=0.598360\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] processed a total of 1303 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22999.127864837646, \"sum\": 22999.127864837646, \"min\": 22999.127864837646}}, \"EndTime\": 1612330788.102382, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330765.102598}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=56.6540308833 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.711797473106\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:48 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:58 INFO 140101591476032] Epoch[56] Batch[0] avg_epoch_loss=0.600314\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:39:58 INFO 140101591476032] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.600313842297\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:05 INFO 140101591476032] Epoch[56] Batch[5] avg_epoch_loss=0.577806\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:05 INFO 140101591476032] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.577806164821\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:05 INFO 140101591476032] Epoch[56] Batch [5]#011Speed: 89.92 samples/sec#011loss=0.577806\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:10 INFO 140101591476032] processed a total of 1278 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22644.74582672119, \"sum\": 22644.74582672119, \"min\": 22644.74582672119}}, \"EndTime\": 1612330810.747768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330788.102458}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:10 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=56.436645867 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:10 INFO 140101591476032] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:10 INFO 140101591476032] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.568386977911\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:10 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:21 INFO 140101591476032] Epoch[57] Batch[0] avg_epoch_loss=0.681419\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:21 INFO 140101591476032] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.681419074535\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:27 INFO 140101591476032] Epoch[57] Batch[5] avg_epoch_loss=0.506171\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:27 INFO 140101591476032] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.506170585752\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:27 INFO 140101591476032] Epoch[57] Batch [5]#011Speed: 100.74 samples/sec#011loss=0.506171\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:33 INFO 140101591476032] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22354.730129241943, \"sum\": 22354.730129241943, \"min\": 22354.730129241943}}, \"EndTime\": 1612330833.103102, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330810.747843}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:33 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=56.8556837378 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:33 INFO 140101591476032] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:33 INFO 140101591476032] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.59510101378\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:33 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:43 INFO 140101591476032] Epoch[58] Batch[0] avg_epoch_loss=0.758736\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:43 INFO 140101591476032] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.758736431599\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:49 INFO 140101591476032] Epoch[58] Batch[5] avg_epoch_loss=0.747149\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:49 INFO 140101591476032] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.747148913642\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:49 INFO 140101591476032] Epoch[58] Batch [5]#011Speed: 98.10 samples/sec#011loss=0.747149\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] Epoch[58] Batch[10] avg_epoch_loss=0.972532\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=1.24299094677\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] Epoch[58] Batch [10]#011Speed: 95.81 samples/sec#011loss=1.242991\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] processed a total of 1349 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23466.871976852417, \"sum\": 23466.871976852417, \"min\": 23466.871976852417}}, \"EndTime\": 1612330856.570704, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330833.103183}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=57.4847581245 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.972531655973\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:40:56 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:08 INFO 140101591476032] Epoch[59] Batch[0] avg_epoch_loss=1.278143\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:08 INFO 140101591476032] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=1.27814340591\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:14 INFO 140101591476032] Epoch[59] Batch[5] avg_epoch_loss=0.931291\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.93129132688\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:14 INFO 140101591476032] Epoch[59] Batch [5]#011Speed: 98.98 samples/sec#011loss=0.931291\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:20 INFO 140101591476032] processed a total of 1268 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23537.31393814087, \"sum\": 23537.31393814087, \"min\": 23537.31393814087}}, \"EndTime\": 1612330880.109134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330856.570854}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:20 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=53.8715915785 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:20 INFO 140101591476032] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:20 INFO 140101591476032] #quality_metric: host=algo-1, epoch=59, train loss <loss>=1.02679262459\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:20 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:31 INFO 140101591476032] Epoch[60] Batch[0] avg_epoch_loss=1.403185\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:31 INFO 140101591476032] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=1.40318453312\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:38 INFO 140101591476032] Epoch[60] Batch[5] avg_epoch_loss=1.179889\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:38 INFO 140101591476032] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=1.17988948027\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:38 INFO 140101591476032] Epoch[60] Batch [5]#011Speed: 98.01 samples/sec#011loss=1.179889\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] Epoch[60] Batch[10] avg_epoch_loss=0.954610\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=0.684274190664\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] Epoch[60] Batch [10]#011Speed: 96.27 samples/sec#011loss=0.684274\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] processed a total of 1349 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24648.119926452637, \"sum\": 24648.119926452637, \"min\": 24648.119926452637}}, \"EndTime\": 1612330904.757964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330880.109222}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=54.7300341305 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.954609803178\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:44 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:55 INFO 140101591476032] Epoch[61] Batch[0] avg_epoch_loss=0.572800\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:41:55 INFO 140101591476032] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.572799861431\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:02 INFO 140101591476032] Epoch[61] Batch[5] avg_epoch_loss=0.533502\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:02 INFO 140101591476032] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.533502077063\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:02 INFO 140101591476032] Epoch[61] Batch [5]#011Speed: 90.97 samples/sec#011loss=0.533502\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:08 INFO 140101591476032] processed a total of 1249 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23825.62494277954, \"sum\": 23825.62494277954, \"min\": 23825.62494277954}}, \"EndTime\": 1612330928.584228, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330904.75806}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:08 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=52.4222456454 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:08 INFO 140101591476032] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:08 INFO 140101591476032] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.484437516332\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:08 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:19 INFO 140101591476032] Epoch[62] Batch[0] avg_epoch_loss=0.448040\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.44803994894\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:26 INFO 140101591476032] Epoch[62] Batch[5] avg_epoch_loss=0.370348\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:26 INFO 140101591476032] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.370348498225\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:26 INFO 140101591476032] Epoch[62] Batch [5]#011Speed: 93.87 samples/sec#011loss=0.370348\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] Epoch[62] Batch[10] avg_epoch_loss=0.436257\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=0.515347260237\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] Epoch[62] Batch [10]#011Speed: 83.08 samples/sec#011loss=0.515347\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] processed a total of 1282 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25582.231044769287, \"sum\": 25582.231044769287, \"min\": 25582.231044769287}}, \"EndTime\": 1612330954.167216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330928.584321}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=50.1126220683 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.436257026412\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:34 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:43 INFO 140101591476032] Epoch[63] Batch[0] avg_epoch_loss=0.294258\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:43 INFO 140101591476032] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.294258207083\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:50 INFO 140101591476032] Epoch[63] Batch[5] avg_epoch_loss=0.413224\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:50 INFO 140101591476032] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.413223887483\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:50 INFO 140101591476032] Epoch[63] Batch [5]#011Speed: 93.66 samples/sec#011loss=0.413224\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:56 INFO 140101591476032] processed a total of 1275 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22242.491006851196, \"sum\": 22242.491006851196, \"min\": 22242.491006851196}}, \"EndTime\": 1612330976.410368, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330954.167315}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:56 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=57.3223604577 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:56 INFO 140101591476032] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:56 INFO 140101591476032] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.413957041502\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:42:56 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:07 INFO 140101591476032] Epoch[64] Batch[0] avg_epoch_loss=0.260568\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:07 INFO 140101591476032] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.26056817174\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:14 INFO 140101591476032] Epoch[64] Batch[5] avg_epoch_loss=0.253539\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:14 INFO 140101591476032] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.253539040685\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:14 INFO 140101591476032] Epoch[64] Batch [5]#011Speed: 94.68 samples/sec#011loss=0.253539\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:19 INFO 140101591476032] processed a total of 1252 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23103.440046310425, \"sum\": 23103.440046310425, \"min\": 23103.440046310425}}, \"EndTime\": 1612330999.514421, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330976.410461}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:19 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=54.190761537 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:19 INFO 140101591476032] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.271585717797\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:19 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:29 INFO 140101591476032] Epoch[65] Batch[0] avg_epoch_loss=0.400561\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:29 INFO 140101591476032] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.400560855865\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:37 INFO 140101591476032] Epoch[65] Batch[5] avg_epoch_loss=0.252541\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:37 INFO 140101591476032] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.252540854116\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:37 INFO 140101591476032] Epoch[65] Batch [5]#011Speed: 90.60 samples/sec#011loss=0.252541\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] Epoch[65] Batch[10] avg_epoch_loss=0.327698\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=0.417887103558\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] Epoch[65] Batch [10]#011Speed: 88.15 samples/sec#011loss=0.417887\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] processed a total of 1293 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24772.29905128479, \"sum\": 24772.29905128479, \"min\": 24772.29905128479}}, \"EndTime\": 1612331024.287366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612330999.514509}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=52.1951073062 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.327698240226\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:44 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:54 INFO 140101591476032] Epoch[66] Batch[0] avg_epoch_loss=0.159037\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:43:54 INFO 140101591476032] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.15903672576\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:01 INFO 140101591476032] Epoch[66] Batch[5] avg_epoch_loss=0.486092\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:01 INFO 140101591476032] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.486092465619\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:01 INFO 140101591476032] Epoch[66] Batch [5]#011Speed: 92.57 samples/sec#011loss=0.486092\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] Epoch[66] Batch[10] avg_epoch_loss=0.330040\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=0.142776840925\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] Epoch[66] Batch [10]#011Speed: 81.82 samples/sec#011loss=0.142777\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] processed a total of 1294 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 25338.901042938232, \"sum\": 25338.901042938232, \"min\": 25338.901042938232}}, \"EndTime\": 1612331049.626897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331024.287461}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=51.0674700998 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.33003990894\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:09 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:19 INFO 140101591476032] Epoch[67] Batch[0] avg_epoch_loss=0.854413\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:19 INFO 140101591476032] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.854412674904\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:27 INFO 140101591476032] Epoch[67] Batch[5] avg_epoch_loss=1.397959\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:27 INFO 140101591476032] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=1.39795920253\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:27 INFO 140101591476032] Epoch[67] Batch [5]#011Speed: 87.95 samples/sec#011loss=1.397959\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:33 INFO 140101591476032] processed a total of 1211 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 23605.107069015503, \"sum\": 23605.107069015503, \"min\": 23605.107069015503}}, \"EndTime\": 1612331073.232948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331049.626983}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:33 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=51.3021444344 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:33 INFO 140101591476032] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:33 INFO 140101591476032] #quality_metric: host=algo-1, epoch=67, train loss <loss>=1.57010965943\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:33 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:43 INFO 140101591476032] Epoch[68] Batch[0] avg_epoch_loss=1.416770\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:43 INFO 140101591476032] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=1.41676950455\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:49 INFO 140101591476032] Epoch[68] Batch[5] avg_epoch_loss=1.064714\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:49 INFO 140101591476032] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=1.06471350789\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:49 INFO 140101591476032] Epoch[68] Batch [5]#011Speed: 93.57 samples/sec#011loss=1.064714\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:55 INFO 140101591476032] processed a total of 1231 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 22620.0590133667, \"sum\": 22620.0590133667, \"min\": 22620.0590133667}}, \"EndTime\": 1612331095.85363, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331073.233045}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:55 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=54.4203923979 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:55 INFO 140101591476032] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:55 INFO 140101591476032] #quality_metric: host=algo-1, epoch=68, train loss <loss>=1.31231867671\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:44:55 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:07 INFO 140101591476032] Epoch[69] Batch[0] avg_epoch_loss=2.125555\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:07 INFO 140101591476032] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=2.12555480003\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:15 INFO 140101591476032] Epoch[69] Batch[5] avg_epoch_loss=2.385704\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:15 INFO 140101591476032] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=2.38570356369\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:15 INFO 140101591476032] Epoch[69] Batch [5]#011Speed: 84.09 samples/sec#011loss=2.385704\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] Epoch[69] Batch[10] avg_epoch_loss=2.215179\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=2.0105502367\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] Epoch[69] Batch [10]#011Speed: 89.32 samples/sec#011loss=2.010550\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] processed a total of 1340 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 26358.532190322876, \"sum\": 26358.532190322876, \"min\": 26358.532190322876}}, \"EndTime\": 1612331122.212793, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331095.853724}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=50.837156906 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] #quality_metric: host=algo-1, epoch=69, train loss <loss>=2.21517932415\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:22 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:33 INFO 140101591476032] Epoch[70] Batch[0] avg_epoch_loss=1.973605\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:33 INFO 140101591476032] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=1.97360503674\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:40 INFO 140101591476032] Epoch[70] Batch[5] avg_epoch_loss=1.221970\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:40 INFO 140101591476032] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=1.22197002172\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:40 INFO 140101591476032] Epoch[70] Batch [5]#011Speed: 85.40 samples/sec#011loss=1.221970\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 24028.096914291382, \"sum\": 24028.096914291382, \"min\": 24028.096914291382}}, \"EndTime\": 1612331146.241532, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331122.212889}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] #throughput_metric: host=algo-1, train throughput=52.8961054623 records/second\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] #quality_metric: host=algo-1, epoch=70, train loss <loss>=1.26365865469\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] loss did not improve\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] Loading parameters from best epoch (50)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 21.795988082885742, \"sum\": 21.795988082885742, \"min\": 21.795988082885742}}, \"EndTime\": 1612331146.264437, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331146.241626}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] stopping training now\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] Final loss: 0.247393071651 (occurred at epoch 50)\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] #quality_metric: host=algo-1, train final_loss <loss>=0.247393071651\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 WARNING 140101591476032] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 334.2289924621582, \"sum\": 334.2289924621582, \"min\": 334.2289924621582}}, \"EndTime\": 1612331146.599682, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331146.264553}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 453.5388946533203, \"sum\": 453.5388946533203, \"min\": 453.5388946533203}}, \"EndTime\": 1612331146.718945, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331146.599778}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 20.633935928344727, \"sum\": 20.633935928344727, \"min\": 20.633935928344727}}, \"EndTime\": 1612331146.739711, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331146.719023}\n",
      "\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[02/03/2021 05:45:46 INFO 140101591476032] No test data passed, skipping evaluation.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1464432.028055191, \"sum\": 1464432.028055191, \"min\": 1464432.028055191}, \"setuptime\": {\"count\": 1, \"max\": 10.606050491333008, \"sum\": 10.606050491333008, \"min\": 10.606050491333008}}, \"EndTime\": 1612331146.769651, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1612331146.739778}\n",
      "\u001b[0m\n",
      "\n",
      "2021-02-03 05:46:46 Uploading - Uploading generated training model\n",
      "2021-02-03 05:46:46 Completed - Training job completed\n",
      "Training seconds: 1586\n",
      "Billable seconds: 1586\n"
     ]
    }
   ],
   "source": [
    "container = get_image_uri(boto3.Session().region_name,'forecasting-deepar')\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(container,\n",
    "                      role=role,   \n",
    "                      train_instance_count=1, \n",
    "                      train_instance_type='ml.m4.xlarge', \n",
    "                      output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                      sagemaker_session=session)\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": \"500\",\n",
    "    \"time_freq\": freq,\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"context_length\": str(context_length),\n",
    "    \"num_cells\": \"100\",\n",
    "    \"num_layers\": \"4\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"0.1\",\n",
    "    \"early_stopping_patience\": \"20\"\n",
    "}\n",
    "estimator.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "estimator.fit({'train': train_location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predicting on Test Set </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(features, cutoff, days, feat_cols, target, num_samples=50, quantiles=['0.1', '0.5', '0.9']):\n",
    "    instances = []\n",
    "    symbols = features['sym'].unique()\n",
    "    \n",
    "    for idx, sym in enumerate(symbols):\n",
    "        look_back_date = cutoff - pd.Timedelta(days, 'D')\n",
    "        window = features.query(\"sym == @sym & time > @look_back_date & time <= @cutoff\")\n",
    "        \n",
    "        if window.empty:\n",
    "            continue\n",
    "        \n",
    "        json_obj = {\"start\": str(list(window['time'])[0]), \n",
    "                    \"target\": list(window[TARGET])[:-1],\n",
    "                    \"cat\":[idx], \n",
    "                    \"dynamic_feat\":[list(window[column]) for column in feat_cols]}\n",
    "        instances.append(json_obj)\n",
    "            \n",
    "#     configuration = {\"num_samples\": num_samples, \n",
    "#                      \"output_types\": [\"quantiles\"], \n",
    "#                      \"quantiles\": quantiles}\n",
    "    configuration = {\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"mean\"]}\n",
    "\n",
    "    request_data = {\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "\n",
    "    json_request = json.dumps(request_data).encode('utf-8')\n",
    "    \n",
    "    return json_request\n",
    "\n",
    "def decode_prediction(prediction, encoding='utf-8'):\n",
    "    '''Accepts a JSON prediction and returns a list of prediction data.\n",
    "    '''\n",
    "    prediction_data = json.loads(prediction.decode(encoding))\n",
    "    prediction_list = []\n",
    "    for k in range(len(prediction_data['predictions'])):\n",
    "        prediction_list.append(pd.DataFrame(data=prediction_data['predictions'][k]['mean']))\n",
    "    return prediction_list\n",
    "\n",
    "def loop_predict(features_df, cutoff, days, feat_cols, TARGET):\n",
    "    dates = list(set(features_df[features_df.time >= cutoff]['time']))\n",
    "    \n",
    "    df = pd.DataFrame([])\n",
    "    for date in dates:\n",
    "        test_features = json_predictor_input(features_df, date, context_length, feat_cols, TARGET, quantiles=['0.5'])\n",
    "        json_prediction = predictor.predict(test_features)\n",
    "        pred = [float(x.values.squeeze()) for x in decode_prediction(json_prediction)]\n",
    "        temp_df = pd.DataFrame(zip([sym for sym in features_df['sym'].unique()], pred), columns=['sym', 'pred'])\n",
    "        temp_df['time'] = date\n",
    "        \n",
    "        df = df.append(temp_df, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = loop_predict(test_scaled, TEST_START, context_length, feat_columns, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = test_scaled.copy()\n",
    "test_result = pd.merge(test_result, pred_Y, how='left', left_on=['time', 'sym'], right_on=['time', 'sym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = test_scaled[TARGET+'_mean'].reset_index(drop=True)\n",
    "pred_std = test_scaled[TARGET+'_std'].reset_index(drop=True)\n",
    "# test_result['pred'] = (test_result['pred'] * pred_std) + pred_mean\n",
    "test_result = test_result.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = 'ETH'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mape(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Compute mean absolute percentage error (MAPE)\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_mape(test_result[TARGET], test_result['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "test_result[test_result['sym'] == sym]['pred'].plot(label='Prediction', lw=1)\n",
    "test_result[test_result['sym'] == sym][TARGET].plot(label='Actual', lw=1)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Predicting on Train Set </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = loop_predict(trainval_scaled, TRAIN_START, context_length, feat_columns, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = train.copy()\n",
    "train_result = pd.merge(trainval_result, pred_Y, how='left', left_on=['time', 'sym'], right_on=['time', 'sym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean = trainval[TARGET+'_mean'].reset_index(drop=True)\n",
    "pred_std = trainval[TARGET+'_std'].reset_index(drop=True)\n",
    "train_result['pred'] = (trainval_result['pred'] * math.sqrt(scaler_train.var_[-1])) + scaler_train.mean_[-1]\n",
    "# train_result['pred'] = (train_result['pred'] * pred_std) + pred_mean\n",
    "train_result = trainval_result.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym = 'BTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "train_result[train_result['sym'] == sym]['pred'].plot(label='Prediction', lw=1)\n",
    "train_result[train_result['sym'] == sym][TARGET].plot(label='Actual', lw=1)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Trading </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mean(mean, t, new_value):\n",
    "    if t == 0:\n",
    "        return new_value\n",
    "    else:\n",
    "        return (mean * (t - 1) + new_value) / t\n",
    "\n",
    "# Function to update standard deviation based on new value\n",
    "def update_std(std, mean, new_mean, t, new_value):\n",
    "    if t == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sqrt((std ** 2 * (t - 1) + (new_value - new_mean) * (new_value - mean)) / t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute the trading strategy over the trading horizon using asset matrix\n",
    "def evaluate_strategy(result, target, n=10, initial_value=10000):\n",
    "    total_value = initial_value\n",
    "    mean_roi = 0\n",
    "    std_roi = 0\n",
    "    \n",
    "    value_history = []\n",
    "    roi_history = []\n",
    "    \n",
    "    t = 1\n",
    "\n",
    "    dates = list(set(result.index))\n",
    "    dates.sort()\n",
    "    \n",
    "    df = result.copy()\n",
    "    if target == 'price':\n",
    "        df['predicted_roi'] = (df['pred']/df['price_lag_1']) - 1\n",
    "    else:\n",
    "        df['predicted_roi'] = df['pred']\n",
    "    df.sort_values(by='predicted_roi', ascending=False, inplace=True)\n",
    "\n",
    "    for date in dates:\n",
    "        temp_df = df.query('time == @date & predicted_roi > 0')\n",
    "\n",
    "        if not temp_df.empty:\n",
    "            top_n = temp_df.nlargest(n, 'predicted_roi')\n",
    "            selected_n = len(top_n)\n",
    "            day_return = sum(top_n['roi'] * total_value / selected_n)\n",
    "            day_roi = day_return/total_value\n",
    "        else:\n",
    "            day_return = 0\n",
    "            day_roi = 0\n",
    "        \n",
    "        total_value += day_return\n",
    "        percent_returns = (total_value/initial_value - 1) * 100\n",
    "\n",
    "        prev_mean_roi = mean_roi\n",
    "        mean_roi = update_mean(prev_mean_roi, t, day_roi)\n",
    "        std_roi = update_std(std_roi, prev_mean_roi, mean_roi, t, day_roi)\n",
    "        sharpe_ratio = mean_roi/std_roi\n",
    "        \n",
    "        value_history.append(total_value)\n",
    "        roi_history.append(day_roi)\n",
    "\n",
    "        t += 1\n",
    "        \n",
    "    print('Cumulative Returns: {:.2e}%'.format(percent_returns))\n",
    "    \n",
    "    history = pd.concat([pd.DataFrame(dates), pd.DataFrame(value_history), pd.DataFrame(roi_history)], axis=1)\n",
    "    history.columns = ['time', 'total_value', 'roi']\n",
    "    history = history.set_index('time')\n",
    "    return sharpe_ratio, percent_returns, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Returns: 2.10e+06%\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_strategy(test_result, TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
